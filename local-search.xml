<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>算法复杂度</title>
    <link href="/2024/09/02/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/"/>
    <url>/2024/09/02/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/</url>
    
    <content type="html"><![CDATA[<h1 id="1-效率评估"><a href="#1-效率评估" class="headerlink" title="1 效率评估"></a>1 效率评估</h1><p>时间效率和空间效率。<br>目的是设计出又快又省的算法。</p><h1 id="2-时间复杂度"><a href="#2-时间复杂度" class="headerlink" title="2 时间复杂度"></a>2 时间复杂度</h1><p>评估各种计算操作所需的运行时间，统计代码中所有的计算操作。<br>运行时间增长趋势为线性的，线性阶，时间复杂度记为O(n)<br>如何推算？<br>忽略常数项；省略所有系数；循环嵌套时使用乘法。<br>时间复杂度由T(n)中最高阶的项来决定。<br>$O(1)&lt;O(logn)&lt;O(n)&lt;O(nlogn)&lt;O(n^2)&lt;O(2^n)&lt;O(n!)$<br>常数阶：操作量与数据大小n无关<br>线性阶：操作量相对n以线性级别增长，通常出现在单层循环。<br>平方阶：通常出现在嵌套循环中，外层循环和内层循环的时间复杂度都为$O(n)$,总体为$O(n^2)$。<br>指数阶：常出现于递归函数中，每次增长值都翻倍。<br>对数阶：常与指数阶相反，每次缩减一半。<br>线性对数阶：常出现在嵌套循环中，两层时间复杂度分别为$O(logn)$和$O(n)$。<br>阶乘阶：每次分裂减少一个。</p><h2 id="最差、最佳、平均时间复杂度"><a href="#最差、最佳、平均时间复杂度" class="headerlink" title="最差、最佳、平均时间复杂度"></a>最差、最佳、平均时间复杂度</h2><p>算法的时间效率往往不是固定的，而是与输入数据的分布有关。<br>最差时间复杂度对应函数渐近上界，大O表示，最佳对应渐近下界，用$\Omega$表示。<br>最差时间复杂度更为实用，因为它给出了一个效率安全值，平均时间复杂度可以体现算法在随机输入数据下的<br>运行效率。-将所有输入情况的时间复杂度与其发生概率相乘，然后对所有可能的输入情况求和。<br>如果输入情况是等概率的，直接取各情况时间复杂度的平均值。</p><h1 id="3-空间复杂度"><a href="#3-空间复杂度" class="headerlink" title="3 空间复杂度"></a>3 空间复杂度</h1><p>用于衡量算法占用内存空间随着数据量变大时的增长趋势。这个概念与时间<br>复杂度非常类似，只需将“运行时间”替换为“占用内存空间”。<br>输入空间：存储算法的输入数据。<br>暂存空间：存储算法在运行过程中的变量、对象、函数上下文等数据。<br>输出空间：用于存储算法的输出数据。<br>一般情况下，统计暂存空间+输出空间。<br>暂存空间可以进一步划分为三个部分。<br>‧ 暂存数据：用于保存算法运行过程中的各种常量、变量、对象等。<br>‧ 栈帧空间：用于保存调用函数的上下文数据。系统在每次调用函数时都会在栈顶部创建一个栈帧，函数<br>返回后，栈帧空间会被释放。<br>‧ 指令空间：用于保存编译后的程序指令，在实际统计中通常忽略不计。</p><h2 id="推算方法"><a href="#推算方法" class="headerlink" title="推算方法"></a>推算方法</h2><p>空间复杂度的推算方法与时间复杂度大致相同，只需将统计对象从“操作数量”转为“使用空间大小”。<br>我们通常只关注最差空间复杂度。<br>1 n&lt;10, 复杂度为O(1);n&gt;10,O(n).<br>2 以算法运行中的峰值内存为准:程序在执行最后一行之前，占用O(1);当初始化数组nums时，程序占用O(n)空间，因此最差空间复杂度为O(n).<br>常数阶：O(1),常见于数量与输入数据大小n无关的常量、变量、对象。<br>线性阶：O(n)，线性阶常见于元素数量与n成正比的数组、链表、栈、队列。<br>平方阶：平方阶常见于矩阵和图，元素数量与n成平方关系。<br>指数阶：指数阶常见于二叉树。<br>对数阶：对数阶常见于分治算法。例如归并排序，输入长度为n的数组，每轮递归将数组从中点处划分为两半，形成<br>高度为 logn的递归树，使用 𝑂(logn) 栈帧空间。</p>]]></content>
    
    
    <categories>
      
      <category>数据结构</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>粒子群算法实现简单的路径规划</title>
    <link href="/2024/09/02/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E7%9A%84%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92/"/>
    <url>/2024/09/02/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E7%9A%84%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92/</url>
    
    <content type="html"><![CDATA[<h1 id="项目地址如下"><a href="#项目地址如下" class="headerlink" title="项目地址如下"></a>项目地址如下</h1><p><a href="https://github.com/Neutrin1/PSO-for-path-planning">Neutrin1&#x2F;PSO-for-path-planning: 粒子群算法实现路径规划的一个示例 (github.com)</a></p><p>运行main.py即可。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>仿真</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>jottings-4</title>
    <link href="/2024/08/27/jottings-4/"/>
    <url>/2024/08/27/jottings-4/</url>
    
    <content type="html"><![CDATA[<p>2024-2025第一学期人设：<br>看起来司马脸+少说话+反转.<br>（不是本人）</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活</tag>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>乒乓专栏</title>
    <link href="/2024/08/25/%E4%B9%92%E4%B9%93%E4%B8%93%E6%A0%8F/"/>
    <url>/2024/08/25/%E4%B9%92%E4%B9%93%E4%B8%93%E6%A0%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="闲来无事，写一则乒乓专栏"><a href="#闲来无事，写一则乒乓专栏" class="headerlink" title="闲来无事，写一则乒乓专栏"></a>闲来无事，写一则乒乓专栏</h1><p>从2016年起，开始打球，到今年已有8年时间，其中20年 21上半年由于场地问题没怎么打</p><h2 id="1-初入乒乓"><a href="#1-初入乒乓" class="headerlink" title="1 初入乒乓"></a>1 初入乒乓</h2><p>小时候看王皓，马琳他们打球，埋下了种子，后来生根发芽。<br>刚上高中，班上有一位有童子功的同学，又加之其它班级的球友比较多，整个高中三年，我所处环境的乒乓氛围还算蛮好的。<br>当时痴迷到什么程度呢，我们学校体育馆二楼有几个乒乓球桌，那会我还能上晚自习，晚饭后那里不开灯，我们几个同学自己带台灯过去打，打到没电了才肯罢休；平常时间，第二节课下课跑操都不去，经常是称病，然后偷偷下去打乒乓球；其余的什么艺体课更不用说，都是往球台子跑。那会还不太会，都是学运动员的动作，但毕竟没练基本功，还是和现在差的很远。</p><h2 id="2-深入乒乓"><a href="#2-深入乒乓" class="headerlink" title="2 深入乒乓"></a>2 深入乒乓</h2><p>进入大学后，学院里边有个什么院队，挑人，我去了，毫无意外地选上了。但是马上退了，原因两点：<br>1 所谓的学长学姐菜得要命，架子还高，不喜欢。<br>2 我以为是军训晚训时间去，后来才知道是下训后去，与期望不符，不喜欢。<br>后来有人告诉我，其实他们还蛮想我去的，在当时的环境下，我还是那天去的人里面最强的。<br>自此，和所谓院队结下了梁子。后来也不太喜欢和他们接触，有几次比赛他们也不让我报。</p><p>后来一直在公共球室打球，一开始地方是受什么乒协管理的，蠢得很，一周只有一个下午可以非会员去打，不知道怎么这么蠢。学校里这个组织，说实在的，技术没什么，官威还挺大，呵呵。。也是满足了这群人的官瘾。</p><p>20年 21上半年 没有地方打球，只有回家了在俱乐部练练，在学校里也不打了。</p><p>21下半年，办省运会，逐渐开放了乒乓球权限，直到现在也是随时可打，不像以前那么约束了。</p><p>21年下半年到现在，球打得比较多。<br>在学校里有个学弟，和他打的最多；还有两个徒弟，教了他们很多技术，也经常训他们，很多东西都教了；其中有一个还好，有一个已经变了，谁技术好就跟着谁转，人也很浮躁，已经被我拉黑了。<br>在球馆，别人都问我是不是专业的(其实只是因为刻意的约束了自己的动作，慢慢练的比较好看)。。<br>这段时间我一直像一个公共教练，学校球馆有人不会，我都会去教一教，有些同学有礼貌会表示谢谢，我也挺开心的。上次学校打比赛，我赛前还特训了某位同学，我和她其实不认识，但是打球的时候经常看见，和她练了几个发现漏洞蛮多的，遂教了几个发球抢攻套路。令我感到诧异的是，有些球她居然完全不会处理，我说你们院队的人没教吗，她说没有，着实令我大跌眼镜。。。</p><p>在学校公共球馆频繁活动一年多后，就不怎么去了，原因有：<br>1 有些人自以为技术好，狂得很(后面也被我打败了)。<br>2 小团体的现象越来越严重，没素质占球桌问题显著。</p><p>以上两种原因，使我不得不去学校外面打球，所幸找到了一个很好的室外球场，人多，热心，技术也好，台子也不错。隔学校5KM，每天晚上骑车去打，在那里，技术又长进了很多。<br>不过那里实在太远了，老过去也累，不是个办法，后来也不去了。</p><p>现在不出去了，经常和学院的老师打球，他球路比较怪，实力也不错，变化很多，打他有挑战性。</p><h2 id="3-我的烧拍之路"><a href="#3-我的烧拍之路" class="headerlink" title="3 我的烧拍之路"></a>3 我的烧拍之路</h2><p>先后买过 银河U2，红双喜劲极5，红双喜狂飙龙5*2，红双喜506X，银河紫龙537S，蝴蝶奥恰洛夫alc，蝴蝶樊振东alc，亚萨卡yeo。打过的就有很多了，字母968，蝴蝶VISCARIA，蝴蝶林高远alc，蝴蝶金标vis，蝴蝶张继科alc，蝴蝶超级张继科alc等，都是球友的拍子，打过很多次。</p><p>附上爱拍照片：</p><p><img src="/image/%E4%B9%92%E4%B9%93%E4%B8%93%E6%A0%8F.jpg"></p><p>好了，就写到这里，此文为吐槽贴。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>乒乓球</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>jottings-3</title>
    <link href="/2024/08/22/jottings-3/"/>
    <url>/2024/08/22/jottings-3/</url>
    
    <content type="html"><![CDATA[<h1 id="最近"><a href="#最近" class="headerlink" title="最近"></a>最近</h1><p>这两天感觉状态极好，效率很高，似很容易进入心流状态。<br>早起早睡落实得很到位，每天的健身也没有中断。<br>练了二十多天了，明显感觉脸部又瘦了点，腹部肌肉也呼之欲出，胸肌也练出来了点。<br>深感欣慰，这是为数不多的能有即时反馈的事情了，庆幸坚持了下来。<br>其实21年练过三个月，不过断断续续的，只打下了一点微弱的基础。。。</p><h1 id="回忆碎片"><a href="#回忆碎片" class="headerlink" title="回忆碎片"></a>回忆碎片</h1><p>从19年刚进大学，似乎就有意的选择一条孤独的道路。<br>刚开始在学校某个大社团人缘比较好、和学长学姐关系也很不错；后来，刻意的在某一天晚上删除大部分人，走向独孤。。。 或许是所谓“物极必反”吧。<br>一瞬间就像变了个人，现在想来，也挺有意思。<br>后来也刻意地去回避一些热闹的场合，除了本专业同学们保持着良好的关系，也有几位关系很好的朋友，感觉够了，其余的也不折腾了。</p><p>大二后，游戏打的也少了，热爱的英雄联盟打到钻三就没打了。<br>大三，基本都在图书馆度过，通信专业的东西又难，老师说的又听不懂；<br>只能下课去图书馆恶补，通常是网课看到晚上10点回去，喊关系好的室友打几把英雄联盟然后睡大觉。<br>最自在的时候就是在冬天的图书馆看窗外的雪了；静谧，又没有压力，好像只需要做题推公式。</p><p>到了考研的时候，其实压力不大，因为之前就有学习的习惯。这段时光很美好，也是啥都不想就做题，到晚上就去练球了，也没啥负担。<br>一个人早出晚归，后来玩了一个暑假。开学了继续学，这会有个室友要考公，所以有了个搭子，大家不太喜欢他，我觉得其实还好，只是他年龄大点沟通方式有区别。好景不长，也就是一起学了一个多月，后来还是单打独斗，也挺好的。<br>入秋了就搬宿舍学了，改成中午起床，凌晨睡觉。效率出奇的高。<br>晚上干到三点，去隔壁看别人打会战地，扯会蛋就睡觉。<br>中途也劝过身边的好几个人朋友一起考，不过都没坚持下来；现在看来，或许他们的选择是好的。到考前，大家都阳了，难受得很，我一度以为我废了；没想到，分数高得惊人，英语更是奇高。不过还是留下了后遗症，之前一度注意力很难长时间集中，现在倒也好多了。经历这段时间后，也明白了“孤独”并不是一个贬义词，人家每天抱团，搞所谓学习方法的霸凌，组团宣传自己的学习方法，后来那群人只有一个分数比我高点，他们的学习时间都比我长得多。</p><p>后来一直保持着“孤独”的形象示人，不愿合群、不想合群；给自己自由，感觉也不错。<br>现在是，到一个新环境，就和新的人保持好的关系，出了这个环境就不联系；下次又到这个环境；又继续笑呵呵的。省去了其余时间尴尬的聊天，只专注于大家共同的领域与话题，感觉挺好的。</p><p>我爱孤独，我爱掌握我全部时间的感觉，我爱不被别人干扰决策的感觉。不过有只猫陪着就更好了🐈</p><p>就写这么多吧，这是今天晚上坐在位置上犯困，写一点记录提提神，可能有点凌乱，嘴下留情。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活</tag>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL-7.1 DQN系列总结</title>
    <link href="/2024/08/22/RL-7-1/"/>
    <url>/2024/08/22/RL-7-1/</url>
    
    <content type="html"><![CDATA[<h1 id="1-DQN"><a href="#1-DQN" class="headerlink" title="1 DQN"></a>1 DQN</h1><p>DQN用神经网络来近似Q-learning算法中的Q值。<br>两个关键技巧;<br><strong>经验回放</strong>:在传统 Q-learning 中，更新 Q 值是基于当前状态、动作、奖励和下一个状态的转移，这些转移可能是相关的。这种相关性可能导致 Q 网络训练过程不稳定。经验回放通过在训练时随机抽取存储在缓冲区中的过往经验样本来打破这种相关性，从而使训练过程更加稳定。<br><strong>目标网络</strong>：在 DQN 中，有两个神经网络，一个是用于估计当前 Q 值的主网络（Q 网络），另一个是用于计算目标 Q 值的目标网络。目标网络的参数与主网络相同，但它们每隔一定步数才更新一次，这样可以避免在更新过程中目标值发生剧烈变化，从而提升训练的稳定性。<br><strong>更新公式</strong><br>$Q(s,a)\leftarrow Q(s,a)+\alpha\left[r+\gamma\max_{a^{\prime}}Q_{\mathrm{target}}(s^{\prime},a^{\prime})-Q(s,a)\right]$<br>其中：</p><ul><li>$Q(s,a)$是当前 Q 网络的预测值。</li><li>$Q_{\text{target}}(s’, a’)$ 是目标网络的 Q 值。</li><li>$r$是即时奖励。</li><li>$\gamma$是折扣因子。</li></ul><h1 id="2-Double-DQN"><a href="#2-Double-DQN" class="headerlink" title="2 Double DQN"></a>2 Double DQN</h1><p>DQN 在估计目标 Q 值时，使用的是相同的 Q 网络来选择和评估动作，这样会导致 Q 值高估的问题。为了减轻这个问题，Double DQN 被提出。<br>Double DQN 通过将动作选择与动作评估分离来解决这个高估问题：</p><ul><li>动作选择仍然使用当前的 Q 网络。</li><li>动作评估则使用目标网络。<br>这样做的好处是，在估计目标 Q 值时不会因为动作选择与动作评估使用同一个网络而导致高估。<br><strong>算法流程</strong></li></ul><ol><li><strong>初始化：</strong> 与 DQN 类似，初始化 Q 网络、目标网络以及经验回放缓冲区。</li><li><strong>采集经验：</strong> 在环境中执行动作，并将经验存储到缓冲区中。</li><li><strong>经验回放：</strong> 从缓冲区中随机抽取一个小批量的经验样本。</li><li><strong>更新网络：</strong> 使用当前 Q 网络选择动作，用目标网络评估该动作的 Q 值，并计算损失更新 Q 网络的参数。</li><li><strong>更新目标网络：</strong> 每隔固定步数，将 Q 网络的参数复制到目标网络。</li><li><strong>重复以上步骤，直到训练完成。</strong></li></ol><p><strong>更新公式</strong><br>$Q(s,a)\leftarrow Q(s,a)+\alpha\left[r+\gamma Q_{\mathrm{target}}(s^{\prime},\arg\max_{a^{\prime}}Q(s^{\prime},a^{\prime}))-Q\right.$<br>其中：<br>$\arg\max_{a^\prime}Q(s^\prime,a^\prime)$是由当前网络Q选择的动作<br>$Q_{\mathrm{target}}(s^{\prime},\arg\max_{a^{\prime}}Q(s^{\prime},a^{\prime}))$是目标网络评估的Q值<br><strong>目标值的计算</strong><br>$\text{target}&#x3D;r+\gamma Q(s’,\arg\max_{a’}Q(s’,a’;\theta);\theta^-)$</p><h1 id="3-Dueling-DQN"><a href="#3-Dueling-DQN" class="headerlink" title="3 Dueling DQN"></a>3 Dueling DQN</h1><p>Dueling DQN 是对 DQN 的进一步改进，旨在更有效地学习每个状态的重要性。Dueling DQN 将 Q 值分解为两个部分：状态价值函数 V(s) 和优势函数 A(s,a)。这种分解方法使得在某些状态下，即使所有动作的 Q 值相近，也能够较好地评估该状态的价值。<br><strong>Q值</strong><br>$Q(s,a;\theta,\alpha,\beta)&#x3D;V(s;\theta,\beta)+\left(A(s,a;\theta,\alpha)-\frac1{|\mathcal{A}|}\sum_{a^{\prime}}A(s,a^{\prime};\theta,\alpha)\right)$<br>其中：</p><ul><li>V(s)是状态值函数，用于表示当前状态的价值。</li><li>A(s,a)是优势函数，用于表示在状态s下，采取动作a相对于平均水平的优势。</li></ul><p>Dueling DQN 的网络结构如下：</p><ul><li>第一部分是一个共享网络，用于提取状态特征。</li><li>第二部分分为两个分支，一个用于计算 V(s)，另一个用于计算 A(s,a)</li></ul><h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h1><ul><li><strong>DQN</strong>: 基本的 Q-learning 与神经网络结合，通过经验回放和目标网络稳定训练。</li><li><strong>Double DQN</strong>: 解决 DQN 中 Q 值过高估计的问题，分离动作选择和动作值估计。</li><li><strong>Dueling DQN</strong>: 分解 Q 值为状态价值和优势函数，更有效地评估状态价值。</li></ul>]]></content>
    
    
    <categories>
      
      <category>深度强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>仿真</tag>
      
      <tag>强化学习</tag>
      
      <tag>深度强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL-7 DQN改型</title>
    <link href="/2024/08/22/RL-7/"/>
    <url>/2024/08/22/RL-7/</url>
    
    <content type="html"><![CDATA[<p>#强化学习 #深度强化学习 </p><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p>在 DQN 之后，学术界涌现出了非常多的改进算法。本章将介绍其中两个非常著名的算法：Double DQN 和 Dueling DQN，这两个算法的实现非常简单，只需要在 DQN 的基础上稍加修改，它们能在一定程度上改善 DQN 的效果。如果读者想要了解更多、更详细的 DQN 改进方法，可以阅读Rainbow 模型的论文及其引用文献。</p><h1 id="2-Double-DQN"><a href="#2-Double-DQN" class="headerlink" title="2 Double DQN"></a>2 Double DQN</h1><p>普通的DQN算法通常会导致对Q值的过高估算，传统DQN算法的TD误差目标为：<br>$r+\gamma\max_{a^{\prime}}Q_{\omega^-}\left(s^{\prime},a^{\prime}\right)$<br>其中$\max_{a^{\prime}}Q_{\omega^-}\left(s^{\prime},a^{\prime}\right)$由目标网络计算得出，可改写：<br>$Q_{\omega^-}\left(s^\prime,\arg\max_{a^\prime}Q_{\omega^-}\left(s^\prime,a^\prime\right)\right)$<br>max操作可分为两部分：首先选取状态$s^{\prime}$下的最优动作$a^*&#x3D;\arg\max_{a^\prime}Q_{\omega^-}\left(s^\prime,a^\prime\right)$，接着计算该动作对应的价值$Q_{\omega^-}\left(s^{\prime},a^{<em>}\right)$。<br>当这两部分采用同一套Q网络进行计算时，每次得到的都是神经网络当前估算的所有动作价值中的最大值。但神经网络估算的Q本身会产生某些正&#x2F;负误差，在DQN的更新方式下神经网络会将正向误差积累。如，我们考虑一个特殊情形：在状态$s^{\prime}$下所有动作的值Q均为0，即$Q(s^{\prime},a_i)&#x3D;0,\forall i$,此时正确的更新目标应该为r+0&#x3D;r，但是由于神经网络拟合误差的存在，总有某个动作Q&gt;0,从而导致更新目标出现过高估计。Q(s,a)也会被过高估计。这样下去，误差会逐步积累，使过高估计的问题越来越严重。<br>解决方法：Doubel DQN算法提出利用两个独立训练的神经网估算$\max_{a’}Q_</em>(s’,a’)$，将原有的$\max_{a^{\prime}}Q_{\omega^{-}}\left(s^{\prime},a^{\prime}\right)$改为$Q_{\omega^-}\left(s^{\prime},\arg\max_{a^{\prime}}Q_\omega\left(s^{\prime},a^{\prime}\right)\right)$，即利用一套神经网络$Q_{\omega}$的输出选取价值最大的动作，使用该动作的价值时，用另一套神经网络$Q_{\omega}^{-}$计算该动作的价值，由于另一套神经网络的存在，这个动作最终使用的Q值不会存在很大的过高估计问题。</p><p>传统的DQN算法中，本来就有两套Q函数的神经网络，训练网络和目标网络。不过$\max_{a^{\prime}}Q_{\omega^{-}}\left(s^{\prime},a^{\prime}\right)$的计算只用到了其中的目标网络，我们可以直接将训练网络作为Double DQN算法中的第一套神经网络，第二套神经网络用目标网络；其中，训练网络的参数为$\omega$目标网络的参数为$\omega^{-}$<br>Double DQN的优化目标：<br>$r+\gamma Q_{\omega^-}\left(s^{\prime},\underset{a^{\prime}}{\operatorname*{\arg\max}}Q_{\omega}\left(s^{\prime},a^{\prime}\right)\right)$</p><h1 id="3-实现Double-DQN"><a href="#3-实现Double-DQN" class="headerlink" title="3 实现Double DQN"></a>3 实现Double DQN</h1><p>显然，DQN与Double DQN的差别只是在于计算状态$s^{\prime}$下Q值如何时选取动作。</p><ul><li>DQN 的优化目标可以写为$r+\gamma Q_{\omega^-}\left(s^{\prime},\arg\max_{a^{\prime}}Q_{\omega^-}\left(s^{\prime},a^{\prime}\right)\right)$，动作的选取依靠目标网络$Q_{\omega^-}$；</li><li>Double DQN 的优化目标为$r+\gamma Q_{\omega^-}\left(s^{\prime},\arg\max_{a^{\prime}}Q_{\omega}\left(s^{\prime},a^{\prime}\right)\right)$，动作的选取依靠训练网络$Q_{\omega}$。<br>所以 Double DQN 的代码实现可以直接在 DQN 的基础上进行，无须做过多修改。<br>环境进行修改，采用倒立摆环境。<br>[Pendulum-v0](<a href="https://github.com/openai/gym/wiki/Pendulum-v0">Pendulum v0 · openai&#x2F;gym Wiki (github.com)</a>)（环境已改名为v1，代码中已修改）</li></ul><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><br><span class="hljs-comment"># -*- encoding: utf-8 -*-</span><br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string"></span><br><span class="hljs-string">@File    :   DQN_改.ipynb</span><br><span class="hljs-string"></span><br><span class="hljs-string">@Time    :   2024/08/22 15:03:03</span><br><span class="hljs-string"></span><br><span class="hljs-string">@Author  :   Neutrin</span><br><span class="hljs-string"></span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>  <br><br><span class="hljs-comment"># here put the import lib</span><br><br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">import</span> gym<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">import</span> rl_utils<br><br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br>  <br>  <br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Qnet</span>(torch.nn.Module):<br><br>    <span class="hljs-string">&#x27;&#x27;&#x27; 只有一层隐藏层的Q网络 &#x27;&#x27;&#x27;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, state_dim, hidden_dim, action_dim</span>):<br><br>        <span class="hljs-built_in">super</span>(Qnet, <span class="hljs-variable language_">self</span>).__init__()<br><br>        <span class="hljs-variable language_">self</span>.fc1 = torch.nn.Linear(state_dim, hidden_dim)<br><br>        <span class="hljs-variable language_">self</span>.fc2 = torch.nn.Linear(hidden_dim, action_dim)<br><br>  <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br><br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.fc2(x)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DQN</span>:<br><br>    <span class="hljs-string">&#x27;&#x27;&#x27; DQN算法,包括Double DQN &#x27;&#x27;&#x27;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 state_dim,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 hidden_dim,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 action_dim,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 learning_rate,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 gamma,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 epsilon,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 target_update,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 device,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 dqn_type=<span class="hljs-string">&#x27;VanillaDQN&#x27;</span></span>):<br><br>        <span class="hljs-variable language_">self</span>.action_dim = action_dim<br><br>        <span class="hljs-variable language_">self</span>.q_net = Qnet(state_dim, hidden_dim, <span class="hljs-variable language_">self</span>.action_dim).to(device)<br><br>        <span class="hljs-variable language_">self</span>.target_q_net = Qnet(state_dim, hidden_dim,<br><br>                                 <span class="hljs-variable language_">self</span>.action_dim).to(device)<br><br>        <span class="hljs-variable language_">self</span>.optimizer = torch.optim.Adam(<span class="hljs-variable language_">self</span>.q_net.parameters(),<br><br>                                          lr=learning_rate)<br><br>        <span class="hljs-variable language_">self</span>.gamma = gamma<br><br>        <span class="hljs-variable language_">self</span>.epsilon = epsilon<br><br>        <span class="hljs-variable language_">self</span>.target_update = target_update<br><br>        <span class="hljs-variable language_">self</span>.count = <span class="hljs-number">0</span><br><br>        <span class="hljs-variable language_">self</span>.dqn_type = dqn_type<br><br>        <span class="hljs-variable language_">self</span>.device = device<br><br>  <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">take_action</span>(<span class="hljs-params">self, state</span>):<br><br>        <span class="hljs-keyword">if</span> np.random.random() &lt; <span class="hljs-variable language_">self</span>.epsilon:<br><br>            action = np.random.randint(<span class="hljs-variable language_">self</span>.action_dim)<br><br>        <span class="hljs-keyword">else</span>:<br><br>            state = torch.tensor([state], dtype=torch.<span class="hljs-built_in">float</span>).to(<span class="hljs-variable language_">self</span>.device)<br><br>            action = <span class="hljs-variable language_">self</span>.q_net(state).argmax().item()<br><br>        <span class="hljs-keyword">return</span> action<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">max_q_value</span>(<span class="hljs-params">self, state</span>):<br><br>        state = torch.tensor([state], dtype=torch.<span class="hljs-built_in">float</span>).to(<span class="hljs-variable language_">self</span>.device)<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.q_net(state).<span class="hljs-built_in">max</span>().item()<br><br>  <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, transition_dict</span>):<br><br>        states = torch.tensor(transition_dict[<span class="hljs-string">&#x27;states&#x27;</span>],<br><br>                              dtype=torch.<span class="hljs-built_in">float</span>).to(<span class="hljs-variable language_">self</span>.device)<br><br>        actions = torch.tensor(transition_dict[<span class="hljs-string">&#x27;actions&#x27;</span>]).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(<br><br>            <span class="hljs-variable language_">self</span>.device)<br><br>        rewards = torch.tensor(transition_dict[<span class="hljs-string">&#x27;rewards&#x27;</span>],<br><br>                               dtype=torch.<span class="hljs-built_in">float</span>).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(<span class="hljs-variable language_">self</span>.device)<br><br>        next_states = torch.tensor(transition_dict[<span class="hljs-string">&#x27;next_states&#x27;</span>],<br><br>                                   dtype=torch.<span class="hljs-built_in">float</span>).to(<span class="hljs-variable language_">self</span>.device)<br><br>        dones = torch.tensor(transition_dict[<span class="hljs-string">&#x27;dones&#x27;</span>],<br><br>                             dtype=torch.<span class="hljs-built_in">float</span>).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(<span class="hljs-variable language_">self</span>.device)<br><br>  <br><br>        q_values = <span class="hljs-variable language_">self</span>.q_net(states).gather(<span class="hljs-number">1</span>, actions)  <span class="hljs-comment"># Q值</span><br><br>        <span class="hljs-comment"># 下个状态的最大Q值</span><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.dqn_type == <span class="hljs-string">&#x27;DoubleDQN&#x27;</span>: <span class="hljs-comment"># DQN与Double DQN的区别</span><br><br>            max_action = <span class="hljs-variable language_">self</span>.q_net(next_states).<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>            max_next_q_values = <span class="hljs-variable language_">self</span>.target_q_net(next_states).gather(<span class="hljs-number">1</span>, max_action)<br><br>        <span class="hljs-keyword">else</span>: <span class="hljs-comment"># DQN的情况</span><br><br>            max_next_q_values = <span class="hljs-variable language_">self</span>.target_q_net(next_states).<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>        q_targets = rewards + <span class="hljs-variable language_">self</span>.gamma * max_next_q_values * (<span class="hljs-number">1</span> - dones)  <span class="hljs-comment"># TD误差目标</span><br><br>        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))  <span class="hljs-comment"># 均方误差损失函数</span><br><br>        <span class="hljs-variable language_">self</span>.optimizer.zero_grad()  <span class="hljs-comment"># PyTorch中默认梯度会累积,这里需要显式将梯度置为0</span><br><br>        dqn_loss.backward()  <span class="hljs-comment"># 反向传播更新参数</span><br><br>        <span class="hljs-variable language_">self</span>.optimizer.step()<br><br>  <br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.count % <span class="hljs-variable language_">self</span>.target_update == <span class="hljs-number">0</span>:<br><br>            <span class="hljs-variable language_">self</span>.target_q_net.load_state_dict(<br><br>                <span class="hljs-variable language_">self</span>.q_net.state_dict())  <span class="hljs-comment"># 更新目标网络</span><br><br>        <span class="hljs-variable language_">self</span>.count += <span class="hljs-number">1</span><br><br>lr = <span class="hljs-number">1e-2</span><br><br>num_episodes = <span class="hljs-number">150</span><br><br>hidden_dim = <span class="hljs-number">128</span><br><br>gamma = <span class="hljs-number">0.98</span><br><br>epsilon = <span class="hljs-number">0.01</span><br><br>target_update = <span class="hljs-number">50</span><br><br>buffer_size = <span class="hljs-number">5000</span><br><br>minimal_size = <span class="hljs-number">1000</span><br><br>batch_size = <span class="hljs-number">32</span><br><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<br><br>    <span class="hljs-string">&quot;cpu&quot;</span>)<br><br>  <br><br>env_name = <span class="hljs-string">&#x27;Pendulum-v1&#x27;</span><br><br>env = gym.make(env_name)<br><br>state_dim = env.observation_space.shape[<span class="hljs-number">0</span>]<br><br>action_dim = <span class="hljs-number">11</span>  <span class="hljs-comment"># 将连续动作分成11个离散动作</span><br><br>  <br>  <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dis_to_con</span>(<span class="hljs-params">discrete_action, env, action_dim</span>):  <span class="hljs-comment"># 离散动作转回连续的函数</span><br><br>    action_lowbound = env.action_space.low[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 连续动作的最小值</span><br><br>    action_upbound = env.action_space.high[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 连续动作的最大值</span><br><br>    <span class="hljs-keyword">return</span> action_lowbound + (discrete_action /<br><br>                              (action_dim - <span class="hljs-number">1</span>)) * (action_upbound -<br><br>                                                   action_lowbound)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_DQN</span>(<span class="hljs-params">agent, env, num_episodes, replay_buffer, minimal_size,</span><br><span class="hljs-params"></span><br><span class="hljs-params">              batch_size</span>):<br><br>    return_list = []<br><br>    max_q_value_list = []<br><br>    max_q_value = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br><br>        <span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-built_in">int</span>(num_episodes / <span class="hljs-number">10</span>),<br><br>                  desc=<span class="hljs-string">&#x27;Iteration %d&#x27;</span> % i) <span class="hljs-keyword">as</span> pbar:<br><br>            <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(num_episodes / <span class="hljs-number">10</span>)):<br><br>                episode_return = <span class="hljs-number">0</span><br><br>                state = env.reset()<br><br>                done = <span class="hljs-literal">False</span><br><br>                <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br><br>                    <span class="hljs-comment"># env.render()</span><br><br>                    action = agent.take_action(state)<br><br>                    max_q_value = agent.max_q_value(<br><br>                        state) * <span class="hljs-number">0.005</span> + max_q_value * <span class="hljs-number">0.995</span>  <span class="hljs-comment"># 平滑处理</span><br><br>                    max_q_value_list.append(max_q_value)  <span class="hljs-comment"># 保存每个状态的最大Q值</span><br><br>                    action_continuous = dis_to_con(action, env,<br><br>                                                   agent.action_dim)<br><br>                    next_state, reward, done, _ = env.step([action_continuous])<br><br>                    replay_buffer.add(state, action, reward, next_state, done)<br><br>                    state = next_state<br><br>                    episode_return += reward<br><br>                    <span class="hljs-keyword">if</span> replay_buffer.size() &gt; minimal_size:<br><br>                        b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(<br><br>                            batch_size)<br><br>                        transition_dict = &#123;<br><br>                            <span class="hljs-string">&#x27;states&#x27;</span>: b_s,<br><br>                            <span class="hljs-string">&#x27;actions&#x27;</span>: b_a,<br><br>                            <span class="hljs-string">&#x27;next_states&#x27;</span>: b_ns,<br><br>                            <span class="hljs-string">&#x27;rewards&#x27;</span>: b_r,<br><br>                            <span class="hljs-string">&#x27;dones&#x27;</span>: b_d<br><br>                        &#125;<br><br>                        agent.update(transition_dict)<br><br>                return_list.append(episode_return)<br><br>                <span class="hljs-keyword">if</span> (i_episode + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br><br>                    pbar.set_postfix(&#123;<br><br>                        <span class="hljs-string">&#x27;episode&#x27;</span>:<br><br>                        <span class="hljs-string">&#x27;%d&#x27;</span> % (num_episodes / <span class="hljs-number">10</span> * i + i_episode + <span class="hljs-number">1</span>),<br><br>                        <span class="hljs-string">&#x27;return&#x27;</span>:<br><br>                        <span class="hljs-string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="hljs-number">10</span>:])<br><br>                    &#125;)<br><br>                pbar.update(<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">return</span> return_list, max_q_value_list<br><br><span class="hljs-comment"># env.close()</span><br><br>random.seed(<span class="hljs-number">0</span>)<br><br>np.random.seed(<span class="hljs-number">0</span>)<br><br>env.seed(<span class="hljs-number">0</span>)<br><br>torch.manual_seed(<span class="hljs-number">0</span>)<br><br>replay_buffer = rl_utils.ReplayBuffer(buffer_size)<br><br>agent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon,<br><br>            target_update, device)<br><br>return_list, max_q_value_list = train_DQN(agent, env, num_episodes,<br><br>                                          replay_buffer, minimal_size,<br><br>                                          batch_size)<br><br>  <br><br>episodes_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(return_list)))<br><br>mv_return = rl_utils.moving_average(return_list, <span class="hljs-number">5</span>)<br><br>plt.plot(episodes_list, mv_return)<br><br>plt.xlabel(<span class="hljs-string">&#x27;Episodes&#x27;</span>)<br><br>plt.ylabel(<span class="hljs-string">&#x27;Returns&#x27;</span>)<br><br>plt.title(<span class="hljs-string">&#x27;DQN on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(env_name))<br><br>plt.show()<br><br>  <br><br>frames_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(max_q_value_list)))<br><br>plt.plot(frames_list, max_q_value_list)<br><br>plt.axhline(<span class="hljs-number">0</span>, c=<span class="hljs-string">&#x27;orange&#x27;</span>, ls=<span class="hljs-string">&#x27;--&#x27;</span>)<br><br>plt.axhline(<span class="hljs-number">10</span>, c=<span class="hljs-string">&#x27;red&#x27;</span>, ls=<span class="hljs-string">&#x27;--&#x27;</span>)<br><br>plt.xlabel(<span class="hljs-string">&#x27;Frames&#x27;</span>)<br><br>plt.ylabel(<span class="hljs-string">&#x27;Q value&#x27;</span>)<br><br>plt.title(<span class="hljs-string">&#x27;DQN on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(env_name))<br><br>plt.show()                            <br><br>random.seed(<span class="hljs-number">0</span>)<br><br>np.random.seed(<span class="hljs-number">0</span>)<br><br>env.seed(<span class="hljs-number">0</span>)<br><br>torch.manual_seed(<span class="hljs-number">0</span>)<br><br>replay_buffer = rl_utils.ReplayBuffer(buffer_size)<br><br>agent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon,<br><br>            target_update, device, <span class="hljs-string">&#x27;DoubleDQN&#x27;</span>)<br><br>return_list, max_q_value_list = train_DQN(agent, env, num_episodes,<br><br>                                          replay_buffer, minimal_size,<br><br>                                          batch_size)<br><br>  <br><br>episodes_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(return_list)))<br><br>mv_return = rl_utils.moving_average(return_list, <span class="hljs-number">5</span>)<br><br>plt.plot(episodes_list, mv_return)<br><br>plt.xlabel(<span class="hljs-string">&#x27;Episodes&#x27;</span>)<br><br>plt.ylabel(<span class="hljs-string">&#x27;Returns&#x27;</span>)<br><br>plt.title(<span class="hljs-string">&#x27;Double DQN on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(env_name))<br><br>plt.show()<br><br>  <br><br>frames_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(max_q_value_list)))<br><br>plt.plot(frames_list, max_q_value_list)<br><br>plt.axhline(<span class="hljs-number">0</span>, c=<span class="hljs-string">&#x27;orange&#x27;</span>, ls=<span class="hljs-string">&#x27;--&#x27;</span>)<br><br>plt.axhline(<span class="hljs-number">10</span>, c=<span class="hljs-string">&#x27;red&#x27;</span>, ls=<span class="hljs-string">&#x27;--&#x27;</span>)<br><br>plt.xlabel(<span class="hljs-string">&#x27;Frames&#x27;</span>)<br><br>plt.ylabel(<span class="hljs-string">&#x27;Q value&#x27;</span>)<br><br>plt.title(<span class="hljs-string">&#x27;Double DQN on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(env_name))<br><br>plt.show()<br></code></pre></td></tr></table></figure><p>如图所示，DoubleDQN相比于DQN比较少出现Q值大于0的情况。<br><img src="/image/RL-71.png"></p><p><img src="/image/RL-72.png"></p><h1 id="4-Dueling-DQN"><a href="#4-Dueling-DQN" class="headerlink" title="4 Dueling DQN"></a>4 Dueling DQN</h1><p>Dueling DQN 是 DQN 另一种的改进算法，它在传统 DQN 的基础上只进行了微小的改动，但却能大幅提升 DQN 的表现。在强化学习中，我们将状态动作价值函数Q减去状态价值函数V的结果定义为优势函数A。即：$A(s,a)&#x3D;Q(s,a)-V(s)$<br>在同一个状态下，所有动作的优势值之和为 0，因为所有动作的动作价值的期望就是这个状态的状态价值。据此，在 Dueling DQN 中，Q 网络被建模为：<br>$Q_{\eta,\alpha,\beta}(s,a)&#x3D;V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)$<br>其中，$V_{\eta,\alpha}(s)$为状态价值函数，而$A_{\eta,\beta}(s,a)$则为该状态下采取不同动作的优势函数，表示采取不同动作的差异性；$\eta$是状态价值函数和优势函数共享的网络参数，一般用在神经网络中，用来提取特征的前几层；而$\alpha$和$\beta$分别为状态价值函数和优势函数的参数。在这样的模型下，我们不再让神经网络直接输出Q值，而是训练神经网络的最后几层的两个分支，分别输出状态价值函数和优势函数，再求和得到Q值。Dueling DQN 的网络结构如下图所示。<br><img src="/image/RL-73.png"><br>将状态价值函数和优势函数分别建模的好处在于：某些情境下智能体只会关注状态的价值，而并不关心不同动作导致的差异，此时将二者分开建模能够使智能体更好地处理与动作关联较小的状态。<br>在下图所示的驾驶车辆游戏中，智能体注意力集中的部位被显示为橙色，当智能体前面没有车时，车辆自身动作并没有太大差异，此时智能体更关注状态价值，而当智能体前面有车时（智能体需要超车），智能体开始关注不同动作优势值的差异。<br><img src="/image/RL-74.png"><br>对于公式，存在V值和A值建模不唯一的问题，Dueling DQN强制最优动作的优势函数实际输出为0，即：<br>$Q_{\eta,\alpha,\beta}(s,a)&#x3D;V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)-\max_{a’}A_{\eta,\beta}\left(s,a’\right)$<br>此时$V(s)&#x3D;\max_aQ(s,a)$保证了V值建模的唯一性。在实现过程中，我们还可以用平均代替最大化操作，即：<br>$Q_{\eta,\alpha,\beta}(s,a)&#x3D;V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)-\frac{1}{|\mathcal{A}|}\sum_{a’}A_{\eta,\beta}\left(s,a’\right)$<br>此时$V(s)&#x3D;\frac{1}{|\mathcal{A}|}\sum_{a^{\prime}}Q(s,a^{\prime})。$</p><h1 id="5-实现Dueling-DNQ"><a href="#5-实现Dueling-DNQ" class="headerlink" title="5 实现Dueling DNQ"></a>5 实现Dueling DNQ</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">VAnet</span>(torch.nn.Module):<br>    <span class="hljs-string">&#x27;&#x27;&#x27; 只有一层隐藏层的A网络和V网络 &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, state_dim, hidden_dim, action_dim</span>):<br>        <span class="hljs-built_in">super</span>(VAnet, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.fc1 = torch.nn.Linear(state_dim, hidden_dim)  <span class="hljs-comment"># 共享网络部分</span><br>        <span class="hljs-variable language_">self</span>.fc_A = torch.nn.Linear(hidden_dim, action_dim)<br>        <span class="hljs-variable language_">self</span>.fc_V = torch.nn.Linear(hidden_dim, <span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        A = <span class="hljs-variable language_">self</span>.fc_A(F.relu(<span class="hljs-variable language_">self</span>.fc1(x)))<br>        V = <span class="hljs-variable language_">self</span>.fc_V(F.relu(<span class="hljs-variable language_">self</span>.fc1(x)))<br>        Q = V + A - A.mean(<span class="hljs-number">1</span>).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># Q值由V值和A值计算得到</span><br>        <span class="hljs-keyword">return</span> Q<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DQN</span>:<br>    <span class="hljs-string">&#x27;&#x27;&#x27; DQN算法,包括Double DQN和Dueling DQN &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params">                 state_dim,</span><br><span class="hljs-params">                 hidden_dim,</span><br><span class="hljs-params">                 action_dim,</span><br><span class="hljs-params">                 learning_rate,</span><br><span class="hljs-params">                 gamma,</span><br><span class="hljs-params">                 epsilon,</span><br><span class="hljs-params">                 target_update,</span><br><span class="hljs-params">                 device,</span><br><span class="hljs-params">                 dqn_type=<span class="hljs-string">&#x27;VanillaDQN&#x27;</span></span>):<br>        <span class="hljs-variable language_">self</span>.action_dim = action_dim<br>        <span class="hljs-keyword">if</span> dqn_type == <span class="hljs-string">&#x27;DuelingDQN&#x27;</span>:  <span class="hljs-comment"># Dueling DQN采取不一样的网络框架</span><br>            <span class="hljs-variable language_">self</span>.q_net = VAnet(state_dim, hidden_dim,<br>                               <span class="hljs-variable language_">self</span>.action_dim).to(device)<br>            <span class="hljs-variable language_">self</span>.target_q_net = VAnet(state_dim, hidden_dim,<br>                                      <span class="hljs-variable language_">self</span>.action_dim).to(device)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.q_net = Qnet(state_dim, hidden_dim,<br>                              <span class="hljs-variable language_">self</span>.action_dim).to(device)<br>            <span class="hljs-variable language_">self</span>.target_q_net = Qnet(state_dim, hidden_dim,<br>                                     <span class="hljs-variable language_">self</span>.action_dim).to(device)<br>        <span class="hljs-variable language_">self</span>.optimizer = torch.optim.Adam(<span class="hljs-variable language_">self</span>.q_net.parameters(),<br>                                          lr=learning_rate)<br>        <span class="hljs-variable language_">self</span>.gamma = gamma<br>        <span class="hljs-variable language_">self</span>.epsilon = epsilon<br>        <span class="hljs-variable language_">self</span>.target_update = target_update<br>        <span class="hljs-variable language_">self</span>.count = <span class="hljs-number">0</span><br>        <span class="hljs-variable language_">self</span>.dqn_type = dqn_type<br>        <span class="hljs-variable language_">self</span>.device = device<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">take_action</span>(<span class="hljs-params">self, state</span>):<br>        <span class="hljs-keyword">if</span> np.random.random() &lt; <span class="hljs-variable language_">self</span>.epsilon:<br>            action = np.random.randint(<span class="hljs-variable language_">self</span>.action_dim)<br>        <span class="hljs-keyword">else</span>:<br>            state = torch.tensor([state], dtype=torch.<span class="hljs-built_in">float</span>).to(<span class="hljs-variable language_">self</span>.device)<br>            action = <span class="hljs-variable language_">self</span>.q_net(state).argmax().item()<br>        <span class="hljs-keyword">return</span> action<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">max_q_value</span>(<span class="hljs-params">self, state</span>):<br>        state = torch.tensor([state], dtype=torch.<span class="hljs-built_in">float</span>).to(<span class="hljs-variable language_">self</span>.device)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.q_net(state).<span class="hljs-built_in">max</span>().item()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, transition_dict</span>):<br>        states = torch.tensor(transition_dict[<span class="hljs-string">&#x27;states&#x27;</span>],<br>                              dtype=torch.<span class="hljs-built_in">float</span>).to(<span class="hljs-variable language_">self</span>.device)<br>        actions = torch.tensor(transition_dict[<span class="hljs-string">&#x27;actions&#x27;</span>]).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(<br>            <span class="hljs-variable language_">self</span>.device)<br>        rewards = torch.tensor(transition_dict[<span class="hljs-string">&#x27;rewards&#x27;</span>],<br>                               dtype=torch.<span class="hljs-built_in">float</span>).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(<span class="hljs-variable language_">self</span>.device)<br>        next_states = torch.tensor(transition_dict[<span class="hljs-string">&#x27;next_states&#x27;</span>],<br>                                   dtype=torch.<span class="hljs-built_in">float</span>).to(<span class="hljs-variable language_">self</span>.device)<br>        dones = torch.tensor(transition_dict[<span class="hljs-string">&#x27;dones&#x27;</span>],<br>                             dtype=torch.<span class="hljs-built_in">float</span>).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(<span class="hljs-variable language_">self</span>.device)<br><br>        q_values = <span class="hljs-variable language_">self</span>.q_net(states).gather(<span class="hljs-number">1</span>, actions)<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.dqn_type == <span class="hljs-string">&#x27;DoubleDQN&#x27;</span>:<br>            max_action = <span class="hljs-variable language_">self</span>.q_net(next_states).<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>            max_next_q_values = <span class="hljs-variable language_">self</span>.target_q_net(next_states).gather(<br>                <span class="hljs-number">1</span>, max_action)<br>        <span class="hljs-keyword">else</span>:<br>            max_next_q_values = <span class="hljs-variable language_">self</span>.target_q_net(next_states).<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].view(<br>                -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        q_targets = rewards + <span class="hljs-variable language_">self</span>.gamma * max_next_q_values * (<span class="hljs-number">1</span> - dones)<br>        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))<br>        <span class="hljs-variable language_">self</span>.optimizer.zero_grad()<br>        dqn_loss.backward()<br>        <span class="hljs-variable language_">self</span>.optimizer.step()<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.count % <span class="hljs-variable language_">self</span>.target_update == <span class="hljs-number">0</span>:<br>            <span class="hljs-variable language_">self</span>.target_q_net.load_state_dict(<span class="hljs-variable language_">self</span>.q_net.state_dict())<br>        <span class="hljs-variable language_">self</span>.count += <span class="hljs-number">1</span><br><br><br>random.seed(<span class="hljs-number">0</span>)<br>np.random.seed(<span class="hljs-number">0</span>)<br>env.seed(<span class="hljs-number">0</span>)<br>torch.manual_seed(<span class="hljs-number">0</span>)<br>replay_buffer = rl_utils.ReplayBuffer(buffer_size)<br>agent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon,<br>            target_update, device, <span class="hljs-string">&#x27;DuelingDQN&#x27;</span>)<br>return_list, max_q_value_list = train_DQN(agent, env, num_episodes,<br>                                          replay_buffer, minimal_size,<br>                                          batch_size)<br><br>episodes_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(return_list)))<br>mv_return = rl_utils.moving_average(return_list, <span class="hljs-number">5</span>)<br>plt.plot(episodes_list, mv_return)<br>plt.xlabel(<span class="hljs-string">&#x27;Episodes&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Returns&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Dueling DQN on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(env_name))<br>plt.show()<br><br>frames_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(max_q_value_list)))<br>plt.plot(frames_list, max_q_value_list)<br>plt.axhline(<span class="hljs-number">0</span>, c=<span class="hljs-string">&#x27;orange&#x27;</span>, ls=<span class="hljs-string">&#x27;--&#x27;</span>)<br>plt.axhline(<span class="hljs-number">10</span>, c=<span class="hljs-string">&#x27;red&#x27;</span>, ls=<span class="hljs-string">&#x27;--&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Frames&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Q value&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Dueling DQN on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(env_name))<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/image/RL-75.png"></p><h1 id="6-总结"><a href="#6-总结" class="headerlink" title="6 总结"></a>6 总结</h1><p>在传统的 DQN 基础上，有两种非常容易实现的变式——Double DQN 和 Dueling DQN，Double DQN 解决了 DQN 中对值的Q过高估计，而 Dueling DQN 能够很好地学习到不同动作的差异性，在动作空间较大的环境下非常有效。从 Double DQN 和 Dueling DQN 的方法原理中，我们也能感受到深度强化学习的研究是在关注深度学习和强化学习有效结合：一是在深度学习的模块的基础上，强化学习方法如何更加有效地工作，并避免深度模型学习行为带来的一些问题，例如使用 Double DQN 解决Q值过高估计的问题；二是在强化学习的场景下，深度学习模型如何有效学习到有用的模式，例如设计 Dueling DQN 网络架构来高效地学习状态价值函数以及动作优势函数。</p>]]></content>
    
    
    <categories>
      
      <category>深度强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>仿真</tag>
      
      <tag>强化学习</tag>
      
      <tag>深度强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL-6 DQN</title>
    <link href="/2024/08/22/RL-6/"/>
    <url>/2024/08/22/RL-6/</url>
    
    <content type="html"><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p>在前面讲解的 Q-learning 算法中，我们以矩阵的方式建立了一张存储每个状态下所有动作值的表格。表格中的每一个动作价值Q(s,a)表示在状态s下选择动作a然后继续遵循某一策略预期能够得到的期望回报。然而，这种用表格存储动作价值的做法只在环境的状态和动作都是离散的，并且空间都比较小的情况下适用，我们之前进行代码实战的几个环境都是如此（如悬崖漫步）。当状态或者动作数量非常大的时候，这种做法就不适用了。例如，当状态是一张 RGB 图像时，假设图像大小是210x160x3，此时一共有$256^{210<em>60</em>3}$种状态，在计算机中存储这个数量级的值表格是不现实的。更甚者，当状态或者动作连续的时候，就有无限个状态动作对，我们更加无法使用这种表格形式来记录各个状态动作对的Q值。</p><p>对于这种情况，我们需要用函数拟合的方法来估Q值，即将这个复杂的值表格视作数据，使用一个参数化的函数$Q_{\theta}$来拟合这些数据。很显然，这种函数拟合的方法存在一定的精度损失，因此被称为近似方法。我们今天要介绍的 DQN 算法便可以用来解决连续状态下离散动作的问题。</p><h1 id="2-CartPole环境"><a href="#2-CartPole环境" class="headerlink" title="2 CartPole环境"></a>2 CartPole环境</h1><p>详情见：<a href="https://github.com/openai/gym/wiki/CartPole-v0"># CartPole v0</a>(在gym库升级后疑似变成了V1，本文沿用旧的gym库)<br>在车杆环境中，有一辆小车，智能体的任务是通过左右移动保持车上的杆竖直，若杆的倾斜度数过大，或者车子离初始位置左右的偏离程度过大，或者坚持时间到达 200 帧，则游戏结束。</p><h1 id="3-DQN"><a href="#3-DQN" class="headerlink" title="3 DQN"></a>3 DQN</h1><p>现在我们想在类似车杆的环境中得到动作价值函数Q(s,a)，由于状态每一维度的值都是连续的，无法使用表格记录，因此一个常见的解决方法便是使用<strong>函数拟合</strong>（function approximation）的思想。由于神经网络具有强大的表达能力，因此我们可以用一个神经网络来表示函数Q。若动作是连续（无限）的，神经网络的输入是状态s和动作a，然后输出一个标量，表示在状态s下采取动作a能获得的价值。若动作是离散（有限）的，除了可以采取动作连续情况下的做法，我们还可以只将状态s输入到神经网络中，使其同时输出每一个动作的Q值。通常 DQN（以及 Q-learning）只能处理动作离散的情况，因为在函数Q的更新过程中有$max_a$这一操作。假设神经网络用来拟合函数w的参数是 ，即每一个状态s下所有可能动作a的Q值我们都能表示为$Q_{\omega}(s,a)$。我们将用于拟合函数Q函数的神经网络称为<strong>Q 网络</strong>，如下图所示。<br><img src="/image/RL-61.png"><br>Q-learning的更新规则：<br>$Q(s,a)\leftarrow Q(s,a)+\alpha\left[r+\gamma\max_{a’\in\mathcal{A}}Q(s’,a’)-Q(s,a)\right]$<br>上述公式用时序差分学习目标$r+\gamma\max_{a^{\prime}\in\mathcal{A}}Q(s^{\prime},a^{\prime})$来增量式更新Q(s,a)，也就是说要使得Q(s,a)和TD目标$r+\gamma\max_{a^{\prime}\in\mathcal{A}}Q(s^{\prime},a^{\prime})$靠近。于是，对于一组数据${(s_i,a_i,r_i,s_i^{\prime})}$，可以自然地将Q网络的损失函数构造为均方误差的形式：<br>$\omega^*&#x3D;\arg\min_\omega\frac{1}{2N}\sum_{i&#x3D;1}^N\left[Q_\omega\left(s_i,a_i\right)-\left(r_i+\gamma\max_{a’}Q_\omega\left(s’_i,a’\right)\right)\right]^2$<br>至此，我们就可以将Q-learing扩展到神经网络形式——深度Q网络算法。由于 DQN 是离线策略算法，因此我们在收集数据的时候可以使用一个$\epsilon$-贪婪策略来平衡探索与利用，将收集到的数据存储起来，在后续的训练中使用。DQN 中还有两个非常重要的模块——<strong>经验回放</strong>和<strong>目标网络</strong>，它们能够帮助 DQN 取得稳定、出色的性能。</p><h2 id="3-1经验回放"><a href="#3-1经验回放" class="headerlink" title="3.1经验回放"></a>3.1经验回放</h2><p>在一般的有监督学习中，假设训练数据是独立同分布的，我们每次训练神经网络的时候从训练数据中随机采样一个或若干个数据来进行梯度下降，随着学习的不断进行，每一个训练数据会被使用多次。在原来的 Q-learning 算法中，每一个数据只会用来更新一次Q值。为了更好地将 Q-learning 和深度神经网络结合，DQN 算法采用了<strong>经验回放</strong>（experience replay）方法，具体做法为维护一个<strong>回放缓冲区</strong>，将每次从环境中采样得到的四元组数据（状态、动作、奖励、下一状态）存储到回放缓冲区中，训练 Q 网络的时候再从回放缓冲区中随机采样若干数据来进行训练。这么做可以起到以下两个作用。<br>（1）使样本满足独立假设。在 MDP 中交互采样得到的数据本身不满足独立假设，因为这一时刻的状态和上一时刻的状态有关。非独立同分布的数据对训练神经网络有很大的影响，会使神经网络拟合到最近训练的数据上。采用经验回放可以打破样本之间的相关性，让其满足独立假设。<br>（2）提高样本效率。每一个样本可以被使用多次，十分适合深度神经网络的梯度学习。</p><h2 id="3-2目标网络"><a href="#3-2目标网络" class="headerlink" title="3.2目标网络"></a>3.2目标网络</h2><p>DQN 算法最终更新的目标是让$Q_{\omega}(s,a)$逼近$r+\gamma\max_{a^{\prime}\in\mathcal{A}}Q(s^{\prime},a^{\prime})$，由于 TD 误差目标本身就包含神经网络的输出，因此在更新网络参数的同时目标也在不断地改变，这非常容易造成神经网络训练的不稳定性。为了解决这一问题，DQN 便使用了<strong>目标网络</strong>（target network）的思想：既然训练过程中 Q 网络的不断更新会导致目标不断发生改变，不如暂时先将 TD 目标中的 Q 网络固定住。为了实现这一思想，我们需要利用两套 Q 网络。</p><p>(1)原来的训练网络$Q_{\omega}(s,a)$，用于计算原来的损失函数$\frac12[Q_\omega\left(s,a\right)-\left(r+\gamma\max_{a^{\prime}}Q_{\omega^-}\left(s^{\prime},a^{\prime}\right)\right)]^2$中的$Q_{\omega}(s,a)$项，并使用正常梯度下降方法来进行更新。<br>(2) 目标网络$Q_{\omega^-}(s,a)$，用于计算原先损失函数$\frac12[Q_\omega\left(s,a\right)-\left(r+\gamma\max_{a^{\prime}}Q_{\omega^-}\left(s^{\prime},a^{\prime}\right)\right)]^2$中的$(r+\gamma\max_{a^{\prime}}Q_{\omega^{-}}(s^{\prime},a^{\prime}))]$项，其中$w^{-}$表示为目标网络中的参数。如果两套网络的参数随时保持一致，则仍为原先不够稳定的算法，为了让更新目标更稳定，目标网络并不会每一步都更新。目标网络使用训练网络的一套较旧的参数，训练网络$Q_{\omega}(s,a)$在训练的每一步都会更新，而目标网络的参数每隔C步才会与训练网络同步一次，即$\omega^{-}\leftarrow\omega$这样使得目标函数相对于训练网络更加稳定。<br><img src="/image/RL-62.png"></p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><br><span class="hljs-comment"># -*- encoding: utf-8 -*-</span><br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string"></span><br><span class="hljs-string">@File    :   DQN.ipynb</span><br><span class="hljs-string"></span><br><span class="hljs-string">@Time    :   2024/08/22 09:59:18</span><br><span class="hljs-string"></span><br><span class="hljs-string">@Author  :   Neutrin</span><br><span class="hljs-string"></span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>  <br><br><span class="hljs-comment"># here put the import lib</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">import</span> gym<br><br><span class="hljs-keyword">import</span> collections<br><br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">import</span> rl_utils<br><br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ReplayBuffer</span>:<br><br>    <span class="hljs-comment">#经验回放池</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, capacity</span>):<br><br>        <span class="hljs-variable language_">self</span>.buffer = collections.deque(maxlen=capacity) <span class="hljs-comment">#双向队列</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add</span>(<span class="hljs-params">self, state, action, reward, next_state, done</span>): <span class="hljs-comment">#把数据加入到buffer中</span><br><br>        <span class="hljs-variable language_">self</span>.buffer.append((state, action, reward, next_state, done))  <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sample</span>(<span class="hljs-params">self, batch_size</span>):   <span class="hljs-comment">#从buffer中随机采样batch_size个数据</span><br><br>        transitions = random.sample(<span class="hljs-variable language_">self</span>.buffer, batch_size)<br><br>        state, action, reward, next_state, done = <span class="hljs-built_in">zip</span>(*transitions)<br><br>        <span class="hljs-keyword">return</span> np.array(state), action, reward, np.array(next_state), done<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">size</span>(<span class="hljs-params">self</span>):<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.buffer)<br> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Qnet</span>(torch.nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,state_dim,hide_dim,action_dim</span>):<br><br>        <span class="hljs-built_in">super</span>(Qnet,<span class="hljs-variable language_">self</span>).__init__()  <br><br>        <span class="hljs-variable language_">self</span>.fc1 = torch.nn.Linear(state_dim,hide_dim)  <span class="hljs-comment">#输入层</span><br><br>        <span class="hljs-variable language_">self</span>.fc2 = torch.nn.Linear(hide_dim,action_dim) <span class="hljs-comment">#输出层</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>): <span class="hljs-comment">#前向传播</span><br><br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))  <span class="hljs-comment">#激活函数</span><br><br>        x = <span class="hljs-variable language_">self</span>.fc2(x)     <span class="hljs-comment">#输出  </span><br><br>        <span class="hljs-keyword">return</span> x   <br> <span class="hljs-keyword">class</span> <span class="hljs-title class_">DQN</span>:<br><br>    <span class="hljs-string">&#x27;&#x27;&#x27; DQN算法 &#x27;&#x27;&#x27;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, state_dim, hidden_dim, action_dim, learning_rate, gamma,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 epsilon, target_update, device</span>):<br><br>        <span class="hljs-variable language_">self</span>.action_dim = action_dim<br><br>        <span class="hljs-variable language_">self</span>.q_net = Qnet(state_dim, hidden_dim,<br><br>                          <span class="hljs-variable language_">self</span>.action_dim).to(device)  <span class="hljs-comment"># Q网络</span><br><br>        <span class="hljs-comment"># 目标网络</span><br><br>        <span class="hljs-variable language_">self</span>.target_q_net = Qnet(state_dim, hidden_dim,<br><br>                                 <span class="hljs-variable language_">self</span>.action_dim).to(device)<br><br>        <span class="hljs-comment"># 使用Adam优化器</span><br><br>        <span class="hljs-variable language_">self</span>.optimizer = torch.optim.Adam(<span class="hljs-variable language_">self</span>.q_net.parameters(),<br><br>                                          lr=learning_rate)<br><br>        <span class="hljs-variable language_">self</span>.gamma = gamma  <span class="hljs-comment"># 折扣因子</span><br><br>        <span class="hljs-variable language_">self</span>.epsilon = epsilon  <span class="hljs-comment"># epsilon-贪婪策略</span><br><br>        <span class="hljs-variable language_">self</span>.target_update = target_update  <span class="hljs-comment"># 目标网络更新频率</span><br><br>        <span class="hljs-variable language_">self</span>.count = <span class="hljs-number">0</span>  <span class="hljs-comment"># 计数器,记录更新次数</span><br><br>        <span class="hljs-variable language_">self</span>.device = device<br><br>  <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">take_action</span>(<span class="hljs-params">self, state</span>):  <span class="hljs-comment"># epsilon-贪婪策略采取动作</span><br><br>        <span class="hljs-keyword">if</span> np.random.random() &lt; <span class="hljs-variable language_">self</span>.epsilon:<br><br>            action = np.random.randint(<span class="hljs-variable language_">self</span>.action_dim)<br><br>        <span class="hljs-keyword">else</span>:<br><br>            state = torch.tensor([state], dtype=torch.<span class="hljs-built_in">float</span>).to(<span class="hljs-variable language_">self</span>.device)<br><br>            action = <span class="hljs-variable language_">self</span>.q_net(state).argmax().item()<br><br>        <span class="hljs-keyword">return</span> action<br><br>  <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, transition_dict</span>):<br><br>        states = torch.tensor(transition_dict[<span class="hljs-string">&#x27;states&#x27;</span>],<br><br>                              dtype=torch.<span class="hljs-built_in">float</span>).to(<span class="hljs-variable language_">self</span>.device)<br><br>        actions = torch.tensor(transition_dict[<span class="hljs-string">&#x27;actions&#x27;</span>]).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(<br><br>            <span class="hljs-variable language_">self</span>.device)<br><br>        rewards = torch.tensor(transition_dict[<span class="hljs-string">&#x27;rewards&#x27;</span>],<br><br>                               dtype=torch.<span class="hljs-built_in">float</span>).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(<span class="hljs-variable language_">self</span>.device)<br><br>        next_states = torch.tensor(transition_dict[<span class="hljs-string">&#x27;next_states&#x27;</span>],<br><br>                                   dtype=torch.<span class="hljs-built_in">float</span>).to(<span class="hljs-variable language_">self</span>.device)<br><br>        dones = torch.tensor(transition_dict[<span class="hljs-string">&#x27;dones&#x27;</span>],<br><br>                             dtype=torch.<span class="hljs-built_in">float</span>).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(<span class="hljs-variable language_">self</span>.device)<br><br>  <br><br>        q_values = <span class="hljs-variable language_">self</span>.q_net(states).gather(<span class="hljs-number">1</span>, actions)  <span class="hljs-comment"># Q值</span><br><br>        <span class="hljs-comment"># 下个状态的最大Q值</span><br><br>        max_next_q_values = <span class="hljs-variable language_">self</span>.target_q_net(next_states).<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].view(<br><br>            -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>        q_targets = rewards + <span class="hljs-variable language_">self</span>.gamma * max_next_q_values * (<span class="hljs-number">1</span> - dones<br><br>                                                                )  <span class="hljs-comment"># TD误差目标</span><br><br>        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))  <span class="hljs-comment"># 均方误差损失函数</span><br><br>        <span class="hljs-variable language_">self</span>.optimizer.zero_grad()  <span class="hljs-comment"># PyTorch中默认梯度会累积,这里需要显式将梯度置为0</span><br><br>        dqn_loss.backward()  <span class="hljs-comment"># 反向传播更新参数</span><br><br>        <span class="hljs-variable language_">self</span>.optimizer.step()<br><br>  <br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.count % <span class="hljs-variable language_">self</span>.target_update == <span class="hljs-number">0</span>:<br><br>            <span class="hljs-variable language_">self</span>.target_q_net.load_state_dict(<br><br>                <span class="hljs-variable language_">self</span>.q_net.state_dict())  <span class="hljs-comment"># 更新目标网络</span><br><br>        <span class="hljs-variable language_">self</span>.count += <span class="hljs-number">1</span><br>lr = <span class="hljs-number">0.002</span><br><br>num_episodes = <span class="hljs-number">300</span><br><br>batch_size = <span class="hljs-number">128</span><br><br>gamma = <span class="hljs-number">0.97</span><br><br>epsilon = <span class="hljs-number">0.1</span><br><br>target_update = <span class="hljs-number">10</span><br><br>buffer_size = <span class="hljs-number">10000</span><br><br>minimal_size = <span class="hljs-number">500</span><br><br>device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<br><br>hidden_dim = <span class="hljs-number">128</span><br><br>  <br><br>env_name = <span class="hljs-string">&#x27;CartPole-v0&#x27;</span><br><br>env = gym.make(<span class="hljs-string">&#x27;CartPole-v0&#x27;</span>)<br><br>env = gym.make(env_name)<br><br>random.seed(<span class="hljs-number">0</span>) <span class="hljs-comment">#设置随机种子</span><br><br>np.random.seed(<span class="hljs-number">0</span>) <span class="hljs-comment">#设置随机种子</span><br><br>env.seed(<span class="hljs-number">0</span>) <span class="hljs-comment">#设置随机种子</span><br><br>torch.manual_seed(<span class="hljs-number">0</span>)<br><br>replay_buffer = ReplayBuffer(buffer_size) <span class="hljs-comment">#经验回放池  </span><br><br>state_dim = env.observation_space.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment">#状态维度</span><br><br>action_dim = env.action_space.n  <span class="hljs-comment">#动作维度  </span><br><br>agent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon,<br><br>            target_update, device)  <span class="hljs-comment">#DQN算法</span><br><br>  <br><br>return_list = []<br><br>  <br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br><br>    <span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-built_in">int</span>(num_episodes / <span class="hljs-number">10</span>), desc=<span class="hljs-string">&#x27;Iteration %d&#x27;</span> % i) <span class="hljs-keyword">as</span> pbar:<br><br>        <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(num_episodes / <span class="hljs-number">10</span>)):<br><br>            episode_return = <span class="hljs-number">0</span><br><br>            state = env.reset()<br><br>            done = <span class="hljs-literal">False</span><br><br>            <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br><br>                env.render()<br><br>                action = agent.take_action(state)<br><br>                next_state, reward, done, _ = env.step(action)<br><br>                replay_buffer.add(state, action, reward, next_state, done)<br><br>                state = next_state<br><br>                episode_return += reward<br><br>                <span class="hljs-comment"># 当buffer数据的数量超过一定值后,才进行Q网络训练</span><br><br>                <span class="hljs-keyword">if</span> replay_buffer.size() &gt; minimal_size:<br><br>                    b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)<br><br>                    transition_dict = &#123;<br><br>                        <span class="hljs-string">&#x27;states&#x27;</span>: b_s,<br><br>                        <span class="hljs-string">&#x27;actions&#x27;</span>: b_a,<br><br>                        <span class="hljs-string">&#x27;next_states&#x27;</span>: b_ns,<br><br>                        <span class="hljs-string">&#x27;rewards&#x27;</span>: b_r,<br><br>                        <span class="hljs-string">&#x27;dones&#x27;</span>: b_d<br><br>                    &#125;<br><br>                    agent.update(transition_dict)<br><br>            return_list.append(episode_return)<br><br>            <span class="hljs-keyword">if</span> (i_episode + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br><br>                pbar.set_postfix(&#123;<br><br>                    <span class="hljs-string">&#x27;episode&#x27;</span>:<br><br>                    <span class="hljs-string">&#x27;%d&#x27;</span> % (num_episodes / <span class="hljs-number">10</span> * i + i_episode + <span class="hljs-number">1</span>),<br><br>                    <span class="hljs-string">&#x27;return&#x27;</span>:<br><br>                    <span class="hljs-string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="hljs-number">10</span>:])<br><br>                &#125;)<br><br>            pbar.update(<span class="hljs-number">1</span>)<br><br>env.close()            <br><br>episodes_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(return_list)))<br><br>plt.plot(episodes_list, return_list)<br><br>plt.xlabel(<span class="hljs-string">&#x27;Episodes&#x27;</span>)<br><br>plt.ylabel(<span class="hljs-string">&#x27;Returns&#x27;</span>)<br><br>plt.title(<span class="hljs-string">&#x27;DQN on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(env_name))<br><br>plt.show()<br><br>  <br><br>mv_return = rl_utils.moving_average(return_list, <span class="hljs-number">9</span>)<br><br>plt.plot(episodes_list, mv_return)<br><br>plt.xlabel(<span class="hljs-string">&#x27;Episodes&#x27;</span>)<br><br>plt.ylabel(<span class="hljs-string">&#x27;Returns&#x27;</span>)<br><br>plt.title(<span class="hljs-string">&#x27;DQN on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(env_name))<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/image/RL-63.png"></p><h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h1><p>细看代码实现。</p>]]></content>
    
    
    <categories>
      
      <category>深度强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>仿真</tag>
      
      <tag>强化学习</tag>
      
      <tag>深度强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL-5 Dyan-Q</title>
    <link href="/2024/08/21/RL-5/"/>
    <url>/2024/08/21/RL-5/</url>
    
    <content type="html"><![CDATA[<p>#强化学习 </p><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p>在强化学习中，“模型”通常指与智能体交互的环境模型，即对环境的状态转移概率和奖励函数进行建模。根据是否具有环境模型，强化学习算法分为两种：<strong>基于模型的强化学习</strong>（model-based reinforcement learning）和<strong>无模型的强化学习</strong>（model-free reinforcement learning）。无模型的强化学习根据智能体与环境交互采样到的数据直接进行策略提升或者价值估计，第 5 章讨论的两种时序差分算法，即 Sarsa 和 Q-learning 算法，便是两种无模型的强化学习方法，本书在后续章节中将要介绍的方法也大多是无模型的强化学习算法。在基于模型的强化学习中，模型可以是事先知道的，也可以是根据智能体与环境交互采样到的数据学习得到的，然后用这个模型帮助策略提升或者价值估计。第 4 章讨论的两种动态规划算法，即策略迭代和价值迭代，则是基于模型的强化学习方法，在这两种算法中环境模型是事先已知的。本章即将介绍的 Dyna-Q 算法也是非常基础的基于模型的强化学习算法，不过它的环境模型是通过采样数据估计得到的。</p><p>强化学习算法有两个重要的评价指标：一个是算法收敛后的策略在初始状态下的期望回报，另一个是样本复杂度，即算法达到收敛结果需要在真实环境中采样的样本数量。基于模型的强化学习算法由于具有一个环境模型，智能体可以额外和环境模型进行交互，对真实环境中样本的需求量往往就会减少，因此通常会比无模型的强化学习算法具有更低的样本复杂度。但是，环境模型可能并不准确，不能完全代替真实环境，因此基于模型的强化学习算法收敛后其策略的期望回报可能不如无模型的强化学习算法。</p><h1 id="2-Dyan-Q"><a href="#2-Dyan-Q" class="headerlink" title="2 Dyan-Q"></a>2 Dyan-Q</h1><p>Dyna-Q 算法是一个经典的基于模型的强化学习算法。如下图所示，Dyna-Q 使用一种叫做 Q-planning 的方法来基于模型生成一些模拟数据，然后用模拟数据和真实数据一起改进策略。Q-planning 每次选取一个曾经访问过的状态s，采取一个曾经在该状态下执行过的动作a，通过模型得到转移后的状态s’以及奖励a，并根据这个模拟数据$(s,a,r,s’)$，用 Q-learning 的更新方式来更新动作价值函数。<br><img src="/image/RL-51.png"><br><img src="/image/RL-52.png"><br>可以看到，在每次与环境进行交互执行一次 Q-learning 之后，Dyna-Q 会做次 Q-planning。其中 Q-planning 的次数N是一个事先可以选择的超参数，当其为 0 时就是普通的 Q-learning。值得注意的是，上述 Dyna-Q 算法是执行在一个离散并且确定的环境中，所以当看到一条经验数据$(s,a,r,s’)$时，可以直接对模型做出更新，即$M(s,a)\leftarrow r,s^{\prime}$。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">import</span> time<br><br>  <br>  <br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CliffWalkingEnv</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, ncol, nrow</span>):<br><br>        <span class="hljs-variable language_">self</span>.nrow = nrow<br><br>        <span class="hljs-variable language_">self</span>.ncol = ncol<br><br>        <span class="hljs-variable language_">self</span>.x = <span class="hljs-number">0</span>  <span class="hljs-comment"># 记录当前智能体位置的横坐标</span><br><br>        <span class="hljs-variable language_">self</span>.y = <span class="hljs-variable language_">self</span>.nrow - <span class="hljs-number">1</span>  <span class="hljs-comment"># 记录当前智能体位置的纵坐标</span><br><br>  <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self, action</span>):  <span class="hljs-comment"># 外部调用这个函数来改变当前位置</span><br><br>        <span class="hljs-comment"># 4种动作, change[0]:上, change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)</span><br><br>        <span class="hljs-comment"># 定义在左上角</span><br><br>        change = [[<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]]<br><br>        <span class="hljs-variable language_">self</span>.x = <span class="hljs-built_in">min</span>(<span class="hljs-variable language_">self</span>.ncol - <span class="hljs-number">1</span>, <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, <span class="hljs-variable language_">self</span>.x + change[action][<span class="hljs-number">0</span>]))  <span class="hljs-comment"># 限制智能体在地图内</span><br><br>        <span class="hljs-variable language_">self</span>.y = <span class="hljs-built_in">min</span>(<span class="hljs-variable language_">self</span>.nrow - <span class="hljs-number">1</span>, <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, <span class="hljs-variable language_">self</span>.y + change[action][<span class="hljs-number">1</span>])) <span class="hljs-comment"># 限制智能体在地图内</span><br><br>        next_state = <span class="hljs-variable language_">self</span>.y * <span class="hljs-variable language_">self</span>.ncol + <span class="hljs-variable language_">self</span>.x  <span class="hljs-comment"># 返回下一个状态</span><br><br>        reward = -<span class="hljs-number">1</span><br><br>        done = <span class="hljs-literal">False</span> <span class="hljs-comment"># 任务是否结束</span><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.y == <span class="hljs-variable language_">self</span>.nrow - <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.x &gt; <span class="hljs-number">0</span>:  <span class="hljs-comment"># 下一个位置在悬崖或者目标</span><br><br>            done = <span class="hljs-literal">True</span> <span class="hljs-comment"># 任务结束</span><br><br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.x != <span class="hljs-variable language_">self</span>.ncol - <span class="hljs-number">1</span>: <span class="hljs-comment"># 如果下一个位置是悬崖</span><br><br>                reward = -<span class="hljs-number">100</span>  <span class="hljs-comment"># 悬崖处的惩罚</span><br><br>        <span class="hljs-keyword">return</span> next_state, reward, done<br><br>  <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self</span>):  <span class="hljs-comment"># 回归初始状态,起点在左上角</span><br><br>        <span class="hljs-variable language_">self</span>.x = <span class="hljs-number">0</span><br><br>        <span class="hljs-variable language_">self</span>.y = <span class="hljs-variable language_">self</span>.nrow - <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.y * <span class="hljs-variable language_">self</span>.ncol + <span class="hljs-variable language_">self</span>.x<br><br>  <br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DynaQ</span>:<br><br>    <span class="hljs-string">&quot;&quot;&quot; Dyna-Q算法 &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 ncol,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 nrow,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 epsilon,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 alpha,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 gamma,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 n_planning,</span><br><span class="hljs-params"></span><br><span class="hljs-params">                 n_action=<span class="hljs-number">4</span></span>):<br><br>        <span class="hljs-variable language_">self</span>.Q_table = np.zeros([nrow * ncol, n_action])  <span class="hljs-comment"># 初始化Q(s,a)表格</span><br><br>        <span class="hljs-variable language_">self</span>.n_action = n_action  <span class="hljs-comment"># 动作个数</span><br><br>        <span class="hljs-variable language_">self</span>.alpha = alpha  <span class="hljs-comment"># 学习率</span><br><br>        <span class="hljs-variable language_">self</span>.gamma = gamma  <span class="hljs-comment"># 折扣因子</span><br><br>        <span class="hljs-variable language_">self</span>.epsilon = epsilon  <span class="hljs-comment"># epsilon-贪婪策略中的参数</span><br><br>  <br><br>        <span class="hljs-variable language_">self</span>.n_planning = n_planning  <span class="hljs-comment">#执行Q-planning的次数, 对应1次Q-learning</span><br><br>        <span class="hljs-variable language_">self</span>.model = <span class="hljs-built_in">dict</span>()  <span class="hljs-comment"># 环境模型</span><br><br>  <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">take_action</span>(<span class="hljs-params">self, state</span>):  <span class="hljs-comment"># 选取下一步的操作</span><br><br>        <span class="hljs-keyword">if</span> np.random.random() &lt; <span class="hljs-variable language_">self</span>.epsilon:  <span class="hljs-comment"># epsilon-贪婪策略</span><br><br>            action = np.random.randint(<span class="hljs-variable language_">self</span>.n_action)<br><br>        <span class="hljs-keyword">else</span>:<br><br>            action = np.argmax(<span class="hljs-variable language_">self</span>.Q_table[state])<br><br>        <span class="hljs-keyword">return</span> action<br><br>  <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">q_learning</span>(<span class="hljs-params">self, s0, a0, r, s1</span>): <span class="hljs-comment"># Q-learning算法</span><br><br>        td_error = r + <span class="hljs-variable language_">self</span>.gamma * <span class="hljs-variable language_">self</span>.Q_table[s1].<span class="hljs-built_in">max</span>(<br><br>        ) - <span class="hljs-variable language_">self</span>.Q_table[s0, a0]<br><br>        <span class="hljs-variable language_">self</span>.Q_table[s0, a0] += <span class="hljs-variable language_">self</span>.alpha * td_error<br><br>  <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, s0, a0, r, s1</span>): <span class="hljs-comment"># 更新Q表格</span><br><br>        <span class="hljs-variable language_">self</span>.q_learning(s0, a0, r, s1)<br><br>        <span class="hljs-variable language_">self</span>.model[(s0, a0)] = r, s1  <span class="hljs-comment"># 将数据添加到模型中</span><br><br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.n_planning):  <span class="hljs-comment"># Q-planning循环</span><br><br>            <span class="hljs-comment"># 随机选择曾经遇到过的状态动作对</span><br><br>            (s, a), (r, s_) = random.choice(<span class="hljs-built_in">list</span>(<span class="hljs-variable language_">self</span>.model.items()))<br><br>            <span class="hljs-variable language_">self</span>.q_learning(s, a, r, s_)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">DynaQ_CliffWalking</span>(<span class="hljs-params">n_planning</span>):<br><br>    ncol = <span class="hljs-number">12</span><br><br>    nrow = <span class="hljs-number">4</span><br><br>    env = CliffWalkingEnv(ncol, nrow)<br><br>    epsilon = <span class="hljs-number">0.01</span><br><br>    alpha = <span class="hljs-number">0.1</span><br><br>    gamma = <span class="hljs-number">0.9</span><br><br>    agent = DynaQ(ncol, nrow, epsilon, alpha, gamma, n_planning)<br><br>    num_episodes = <span class="hljs-number">300</span>  <span class="hljs-comment"># 智能体在环境中运行多少条序列</span><br><br>  <br><br>    return_list = []  <span class="hljs-comment"># 记录每一条序列的回报</span><br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):  <span class="hljs-comment"># 显示10个进度条</span><br><br>        <span class="hljs-comment"># tqdm的进度条功能</span><br><br>        <span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-built_in">int</span>(num_episodes / <span class="hljs-number">10</span>),<br><br>                  desc=<span class="hljs-string">&#x27;Iteration %d&#x27;</span> % i) <span class="hljs-keyword">as</span> pbar:<br><br>            <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(num_episodes / <span class="hljs-number">10</span>)):  <span class="hljs-comment"># 每个进度条的序列数</span><br><br>                episode_return = <span class="hljs-number">0</span><br><br>                state = env.reset()<br><br>                done = <span class="hljs-literal">False</span><br><br>                <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br><br>                    action = agent.take_action(state)<br><br>                    next_state, reward, done = env.step(action)<br><br>                    episode_return += reward  <span class="hljs-comment"># 这里回报的计算不进行折扣因子衰减</span><br><br>                    agent.update(state, action, reward, next_state)<br><br>                    state = next_state<br><br>                return_list.append(episode_return)<br><br>                <span class="hljs-keyword">if</span> (i_episode + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:  <span class="hljs-comment"># 每10条序列打印一下这10条序列的平均回报</span><br><br>                    pbar.set_postfix(&#123;<br><br>                        <span class="hljs-string">&#x27;episode&#x27;</span>:<br><br>                        <span class="hljs-string">&#x27;%d&#x27;</span> % (num_episodes / <span class="hljs-number">10</span> * i + i_episode + <span class="hljs-number">1</span>),<br><br>                        <span class="hljs-string">&#x27;return&#x27;</span>:<br><br>                        <span class="hljs-string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="hljs-number">10</span>:])<br><br>                    &#125;)<br><br>                pbar.update(<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">return</span> return_list<br><br>  <br><br>np.random.seed(<span class="hljs-number">0</span>)<br><br>random.seed(<span class="hljs-number">0</span>)<br><br>n_planning_list = [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">20</span>]<br><br><span class="hljs-keyword">for</span> n_planning <span class="hljs-keyword">in</span> n_planning_list:<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Q-planning步数为：%d&#x27;</span> % n_planning)<br><br>    time.sleep(<span class="hljs-number">0.5</span>)<br><br>    return_list = DynaQ_CliffWalking(n_planning)<br><br>    episodes_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(return_list)))<br><br>    plt.plot(episodes_list,<br><br>             return_list,<br><br>             label=<span class="hljs-built_in">str</span>(n_planning) + <span class="hljs-string">&#x27; planning steps&#x27;</span>)<br><br>plt.legend()<br><br>plt.xlabel(<span class="hljs-string">&#x27;Episodes&#x27;</span>)<br><br>plt.ylabel(<span class="hljs-string">&#x27;Returns&#x27;</span>)<br><br>plt.title(<span class="hljs-string">&#x27;Dyna-Q on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-string">&#x27;Cliff Walking&#x27;</span>))<br><br>plt.show()<br></code></pre></td></tr></table></figure><h1 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h1><p>本章讲解了一个经典的基于模型的强化学习算法 Dyna-Q，并且通过调整在悬崖漫步环境下的 Q-planning 步数，直观地展示了 Q-planning 步数对于收敛速度的影响。我们发现基于模型的强化学习算法 Dyna-Q 在以上环境中获得了很好的效果，但这些环境比较简单，模型可以直接通过经验数据得到。如果环境比较复杂，状态是连续的，或者状态转移是随机的而不是决定性的，如何学习一个比较准确的模型就变成非常重大的挑战，这直接影响到基于模型的强化学习算法能否应用于这些环境并获得比无模型的强化学习更好的效果。</p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>仿真</tag>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL-4 时序差分算法</title>
    <link href="/2024/08/20/RL-4/"/>
    <url>/2024/08/20/RL-4/</url>
    
    <content type="html"><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p>上一篇介绍的动态规划算法要求马尔可夫决策过程是已知的，即要求与智能体交互的环境是完全已知的（例如迷宫或者给定规则的网格世界）。在此条件下，智能体其实并不需要和环境真正交互来采样数据，直接用动态规划算法就可以解出最优价值或策略。这就好比对于有监督学习任务，如果直接显式给出了数据的分布公式，那么也可以通过在期望层面上直接最小化模型的泛化误差来更新模型参数，并不需要采样任何数据点。但这在大部分场景下并不现实，机器学习的主要方法都是在数据分布未知的情况下针对具体的数据点来对模型做出更新的。对于大部分强化学习现实场景（例如电子游戏或者一些复杂物理环境），其马尔可夫决策过程的状态转移概率是无法写出来的，也就无法直接进行动态规划。在这种情况下，智能体只能和环境进行交互，通过采样到的数据来学习，这类学习方法统称为<strong>无模型的强化学习</strong>（model-free reinforcement learning）。<br>不同于动态规划算法，无模型的强化学习算法不需要事先知道环境的奖励函数和状态转移函数，而是直接使用和环境交互的过程中采样到的数据来学习，这使得它可以被应用到一些简单的实际场景中。本章将要讲解无模型的强化学习中的两大经典算法：Sarsa 和 Q-learning，它们都是基于<strong>时序差分</strong>（temporal difference，TD）的强化学习算法。同时，本章还会引入一组概念：在线策略学习和离线策略学习。通常来说，在线策略学习要求使用在当前策略下采样得到的样本进行学习，一旦策略被更新，当前的样本就被放弃了，就好像在水龙头下用自来水洗手；而离线策略学习使用经验回放池将之前采样得到的样本收集起来再次利用，就好像使用脸盆接水后洗手。因此，离线策略学习往往能够更好地利用历史数据，并具有更小的样本复杂度（算法达到收敛结果需要在环境中采样的样本数量），这使其被更广泛地应用。</p><h1 id="2-时序差分方法"><a href="#2-时序差分方法" class="headerlink" title="2 时序差分方法"></a>2 时序差分方法</h1><p>时序差分是一种用来估计一个策略的价值函数的方法，它结合了蒙特卡洛和动态规划算法的思想。时序差分方法和蒙特卡洛的相似之处在于可以从样本数据中学习，不需要事先知道环境；和动态规划的相似之处在于根据贝尔曼方程的思想，利用后续状态的价值估计来更新当前状态的价值估计。回顾一下蒙特卡洛方法对价值函数的增量更新方式：<br>$V(s_t)\leftarrow V(s_t)+\alpha[G_t-V(s_t)]$<br>$\alpha$表示更新的步长，可以取常数，更新方式不再像蒙特卡洛方法严格地取期望。蒙特卡洛方法必须要等整个序列结束之后才能计算得到这一次的回报$G_t$，而时序差分方法只需要当前步结束即可进行计算。具体来说，时序差分算法用当前获得的奖励加上下一个状态的价值估计来作为在当前状态会获得的回报，即：<br>$V(s_t)\leftarrow V(s_t)+\alpha[r_t+\gamma V(s_{t+1})-V(s_t)]$<br>$R_t+\gamma V(s_{t+1})-V(s_t)$通常被称为时序差分误差，时序差分算法将其与步长的乘积作为状态价值的更新量。可以替代$G_t$的原因是：<br>$$\begin{aligned}<br>V_{\pi}(s)&amp; &#x3D;\mathbb{E}<em>\pi[G_t|S_t&#x3D;s] \<br>&amp;&#x3D;\mathbb{E}<em>\pi[\sum</em>{k&#x3D;0}^\infty\gamma^kR</em>{t+k}|S_t&#x3D;s] \<br>&amp;&#x3D;\mathbb{E}<em>\pi[R_t+\gamma\sum</em>{k&#x3D;0}^\infty\gamma^kR_{t+k+1}|S_t&#x3D;s] \<br>&amp;&#x3D;\mathbb{E}<em>\pi[R_t+\gamma V_\pi(S</em>{t+1})|S_t&#x3D;s]<br>\end{aligned}$$<br>因此蒙特卡洛方法将上式第一行作为更新的目标，而时序差分算法将上式最后一行作为更新的目标。于是，在用策略和环境交互时，每采样一步，我们就可以用时序差分算法来更新状态价值估计。时序差分算法用到了$V(s_{t+1})$的估计值，可以证明它最终收敛到策略$\pi$的价值函数，我们在此不对此进行展开说明。</p><h1 id="3-Sarsa算法"><a href="#3-Sarsa算法" class="headerlink" title="3 Sarsa算法"></a>3 Sarsa算法</h1><p>既然我们可以用时序差分方法来估计价值函数，那一个很自然的问题是，我们能否用类似策略迭代的方法来进行强化学习。策略评估已经可以通过时序差分算法实现，那么在不知道奖励函数和状态转移函数的情况下该怎么进行策略提升呢？答案是时可以直接用时序差分算法来估计动作价值函数Q：<br>$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]$<br>然后我们用贪婪算法来选取在某个状态下动作价值最大的那个动作。这样似乎已经形成了一个完整的强化学习算法：用贪婪算法根据动作价值选取动作来和环境交互，再根据得到的数据用时序差分算法更新动作价值估计。<br>需要考虑的问题：<br>1 样本量过大，但是可以忽略，因为策略提升可以在策略评估未完全进行的情况下部署。<br>2 在策略提升中一直根据贪婪算法得到一个确定性策略，可能导致某些状态动作对永远没有出现，导致无法对其动作价值进行估计。<br>如何解决？不用简单的贪婪算法，就像前文所说的，使用$\epsilon$贪婪策略：<br>$$\pi(a|s)&#x3D;\begin{cases}\epsilon&#x2F;|\mathcal{A}|+1-\epsilon&amp;\textit{如果}a&#x3D;\arg\max_{a^{\prime}}Q(s,a^{\prime})\\epsilon&#x2F;|\mathcal{A}|&amp;\textit{其他动作}\end{cases}$$<br>现在，我们就可以得到一个实际的基于时序差分方法的强化学习算法。这个算法被称为 Sarsa，它的动作价值更新用到了s,a,r,以及下一个状态s和下一个动作a。<br><img src="/image/RL-41.png"><br>继续在悬崖漫步环境下部署。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs pf">ncol = <span class="hljs-number">12</span><br>nrow = <span class="hljs-number">4</span><br>env = CliffWalkingEnv(ncol, nrow)<br>np.<span class="hljs-keyword">random</span>.seed(<span class="hljs-number">0</span>)<br>epsilon = <span class="hljs-number">0.1</span><br>alpha = <span class="hljs-number">0.1</span><br>gamma = <span class="hljs-number">0.9</span><br>agent = Sarsa(ncol, nrow, epsilon, alpha, gamma)<br>num_episodes = <span class="hljs-number">500</span>  <span class="hljs-comment"># 智能体在环境中运行的序列的数量</span><br><br>return_list = []  <span class="hljs-comment"># 记录每一条序列的回报</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):  <span class="hljs-comment"># 显示10个进度条</span><br>    <span class="hljs-comment"># tqdm的进度条功能</span><br>    with tqdm(total=int(num_episodes / <span class="hljs-number">10</span>), desc=&#x27;Iteration %d&#x27; % i) as pbar:<br>        <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> range(int(num_episodes / <span class="hljs-number">10</span>)):  <span class="hljs-comment"># 每个进度条的序列数</span><br>            episode_return = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">state</span> = env.reset()<br>            action = agent.take_action(<span class="hljs-keyword">state</span>)<br>            done = False<br>            while not done:<br>                next_state, reward, done = env.step(action)<br>                next_action = agent.take_action(next_state)<br>                episode_return += reward  <span class="hljs-comment"># 这里回报的计算不进行折扣因子衰减</span><br>                agent.update(<span class="hljs-keyword">state</span>, action, reward, next_state, next_action)<br>                <span class="hljs-keyword">state</span> = next_state<br>                action = next_action<br>            return_list.append(episode_return)<br>            if (i_episode + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:  <span class="hljs-comment"># 每10条序列打印一下这10条序列的平均回报</span><br>                pbar.set_postfix(&#123;<br>                    &#x27;episode&#x27;:<br>                    &#x27;%d&#x27; % (num_episodes / <span class="hljs-number">10</span> * i + i_episode + <span class="hljs-number">1</span>),<br>                    &#x27;return&#x27;:<br>                    &#x27;%.<span class="hljs-number">3</span>f&#x27; % np.mean(return_list[-<span class="hljs-number">10</span>:])<br>                &#125;)<br>            pbar.update(<span class="hljs-number">1</span>)<br><br>episodes_list = list(range(len(return_list)))<br>plt.plot(episodes_list, return_list)<br>plt.xlabel(&#x27;Episodes&#x27;)<br>plt.ylabel(&#x27;Returns&#x27;)<br>plt.title(&#x27;Sarsa <span class="hljs-keyword">on</span> &#123;&#125;&#x27;.format(&#x27;Cliff Walking&#x27;))<br>plt.show()<br></code></pre></td></tr></table></figure><p>我们发现，随着训练的进行，Sarsa 算法获得的回报越来越高。在进行 500 条序列的学习后，可以获得 −20 左右的回报，此时已经非常接近最优策略了。然后我们看一下 Sarsa 算法得到的策略在各个状态下会使智能体采取什么样的动作。</p><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def print_agent(agent, env, action_meaning, disaster=[], end=[]):<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(env.nrow):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(env.ncol):<br>            <span class="hljs-keyword">if</span> (i * env.ncol + j) <span class="hljs-keyword">in</span> disaster:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;****&#x27;</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">&#x27; &#x27;</span>)<br>            elif (i * env.ncol + j) <span class="hljs-keyword">in</span> end:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;EEEE&#x27;</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">&#x27; &#x27;</span>)<br>            <span class="hljs-keyword">else</span>:<br>                a = agent.best_action(i * env.ncol + j)<br>                pi_str = <span class="hljs-string">&#x27;&#x27;</span><br>                <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(len(action_meaning)):<br>                    pi_str += action_meaning[k] <span class="hljs-keyword">if</span> a[k] &gt; 0 <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;o&#x27;</span><br>                <span class="hljs-built_in">print</span>(pi_str, <span class="hljs-attribute">end</span>=<span class="hljs-string">&#x27; &#x27;</span>)<br>        <span class="hljs-built_in">print</span>()<br><br><br>action_meaning = [<span class="hljs-string">&#x27;^&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>, <span class="hljs-string">&#x27;&lt;&#x27;</span>, <span class="hljs-string">&#x27;&gt;&#x27;</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Sarsa算法最终收敛得到的策略为：&#x27;</span>)<br>print_agent(agent, env, action_meaning, list(range(37, 47)), [47])<br></code></pre></td></tr></table></figure><h1 id="4-多步Sarsa算法"><a href="#4-多步Sarsa算法" class="headerlink" title="4 多步Sarsa算法"></a>4 多步Sarsa算法</h1><p>蒙特卡洛方法利用当前状态之后每一步的奖励而不使用任何价值估计，时序差分算法只利用一步奖励和下一个状态的价值估计。那它们之间的区别是什么呢？总的来说，蒙特卡洛方法是<strong>无偏</strong>（unbiased）的，但是具有比较大的方差，因为每一步的状态转移都有不确定性，而每一步状态采取的动作所得到的不一样的奖励最终都会加起来，这会极大影响最终的价值估计；时序差分算法具有非常小的方差，因为只关注了一步状态转移，用到了一步的奖励，但是它是有偏的，因为用到了下一个状态的价值估计而不是其真实的价值。那有没有什么方法可以结合二者的优势呢？答案是<strong>多步时序差分</strong>！多步时序差分的意思是使用步的奖励，然后使用之后状态的价值估计。用公式表示，<strong>将</strong><br>$G_t&#x3D;r_t+\gamma Q(s_{t+1},a_{t+1})$<br>换成<br>$G_t&#x3D;r_t+\gamma r_{t+1}+\cdots+\gamma^nQ(s_{t+n},a_{t+n})$<br>于是，相应存在一种多步 Sarsa 算法，它把 Sarsa 算法中的动作价值函数的更新公式<br>$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]$<br>替换成<br>$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma r_{t+1}+\cdots+\gamma^nQ(s_{t+n},a_{t+n})-Q(s_t,a_t)]$</p><h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">nstep_Sarsa</span>:<br>    <span class="hljs-string">&quot;&quot;&quot; n步Sarsa算法 &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n, ncol, nrow, epsilon, alpha, gamma, n_action=<span class="hljs-number">4</span></span>):<br>        <span class="hljs-variable language_">self</span>.Q_table = np.zeros([nrow * ncol, n_action])<br>        <span class="hljs-variable language_">self</span>.n_action = n_action<br>        <span class="hljs-variable language_">self</span>.alpha = alpha<br>        <span class="hljs-variable language_">self</span>.gamma = gamma<br>        <span class="hljs-variable language_">self</span>.epsilon = epsilon<br>        <span class="hljs-variable language_">self</span>.n = n  <span class="hljs-comment"># 采用n步Sarsa算法</span><br>        <span class="hljs-variable language_">self</span>.state_list = []  <span class="hljs-comment"># 保存之前的状态</span><br>        <span class="hljs-variable language_">self</span>.action_list = []  <span class="hljs-comment"># 保存之前的动作</span><br>        <span class="hljs-variable language_">self</span>.reward_list = []  <span class="hljs-comment"># 保存之前的奖励</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">take_action</span>(<span class="hljs-params">self, state</span>):<br>        <span class="hljs-keyword">if</span> np.random.random() &lt; <span class="hljs-variable language_">self</span>.epsilon:<br>            action = np.random.randint(<span class="hljs-variable language_">self</span>.n_action)<br>        <span class="hljs-keyword">else</span>:<br>            action = np.argmax(<span class="hljs-variable language_">self</span>.Q_table[state])<br>        <span class="hljs-keyword">return</span> action<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">best_action</span>(<span class="hljs-params">self, state</span>):  <span class="hljs-comment"># 用于打印策略</span><br>        Q_max = np.<span class="hljs-built_in">max</span>(<span class="hljs-variable language_">self</span>.Q_table[state])<br>        a = [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.n_action)]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.n_action):<br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.Q_table[state, i] == Q_max:<br>                a[i] = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> a<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, s0, a0, r, s1, a1, done</span>):<br>        <span class="hljs-variable language_">self</span>.state_list.append(s0)<br>        <span class="hljs-variable language_">self</span>.action_list.append(a0)<br>        <span class="hljs-variable language_">self</span>.reward_list.append(r)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.state_list) == <span class="hljs-variable language_">self</span>.n:  <span class="hljs-comment"># 若保存的数据可以进行n步更新</span><br>            G = <span class="hljs-variable language_">self</span>.Q_table[s1, a1]  <span class="hljs-comment"># 得到Q(s_&#123;t+n&#125;, a_&#123;t+n&#125;)</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.n)):<br>                G = <span class="hljs-variable language_">self</span>.gamma * G + <span class="hljs-variable language_">self</span>.reward_list[i]  <span class="hljs-comment"># 不断向前计算每一步的回报</span><br>                <span class="hljs-comment"># 如果到达终止状态,最后几步虽然长度不够n步,也将其进行更新</span><br>                <span class="hljs-keyword">if</span> done <span class="hljs-keyword">and</span> i &gt; <span class="hljs-number">0</span>:<br>                    s = <span class="hljs-variable language_">self</span>.state_list[i]<br>                    a = <span class="hljs-variable language_">self</span>.action_list[i]<br>                    <span class="hljs-variable language_">self</span>.Q_table[s, a] += <span class="hljs-variable language_">self</span>.alpha * (G - <span class="hljs-variable language_">self</span>.Q_table[s, a])<br>            s = <span class="hljs-variable language_">self</span>.state_list.pop(<span class="hljs-number">0</span>)  <span class="hljs-comment"># 将需要更新的状态动作从列表中删除,下次不必更新</span><br>            a = <span class="hljs-variable language_">self</span>.action_list.pop(<span class="hljs-number">0</span>)<br>            <span class="hljs-variable language_">self</span>.reward_list.pop(<span class="hljs-number">0</span>)<br>            <span class="hljs-comment"># n步Sarsa的主要更新步骤</span><br>            <span class="hljs-variable language_">self</span>.Q_table[s, a] += <span class="hljs-variable language_">self</span>.alpha * (G - <span class="hljs-variable language_">self</span>.Q_table[s, a])<br>        <span class="hljs-keyword">if</span> done:  <span class="hljs-comment"># 如果到达终止状态,即将开始下一条序列,则将列表全清空</span><br>            <span class="hljs-variable language_">self</span>.state_list = []<br>            <span class="hljs-variable language_">self</span>.action_list = []<br>            <span class="hljs-variable language_">self</span>.reward_list = []<br>np.random.seed(<span class="hljs-number">0</span>)<br>n_step = <span class="hljs-number">5</span>  <span class="hljs-comment"># 5步Sarsa算法</span><br>alpha = <span class="hljs-number">0.1</span><br>epsilon = <span class="hljs-number">0.1</span><br>gamma = <span class="hljs-number">0.9</span><br>agent = nstep_Sarsa(n_step, ncol, nrow, epsilon, alpha, gamma)<br>num_episodes = <span class="hljs-number">500</span>  <span class="hljs-comment"># 智能体在环境中运行的序列的数量</span><br><br>return_list = []  <span class="hljs-comment"># 记录每一条序列的回报</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):  <span class="hljs-comment"># 显示10个进度条</span><br>    <span class="hljs-comment">#tqdm的进度条功能</span><br>    <span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-built_in">int</span>(num_episodes / <span class="hljs-number">10</span>), desc=<span class="hljs-string">&#x27;Iteration %d&#x27;</span> % i) <span class="hljs-keyword">as</span> pbar:<br>        <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(num_episodes / <span class="hljs-number">10</span>)):  <span class="hljs-comment"># 每个进度条的序列数</span><br>            episode_return = <span class="hljs-number">0</span><br>            state = env.reset()<br>            action = agent.take_action(state)<br>            done = <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br>                next_state, reward, done = env.step(action)<br>                next_action = agent.take_action(next_state)<br>                episode_return += reward  <span class="hljs-comment"># 这里回报的计算不进行折扣因子衰减</span><br>                agent.update(state, action, reward, next_state, next_action,<br>                             done)<br>                state = next_state<br>                action = next_action<br>            return_list.append(episode_return)<br>            <span class="hljs-keyword">if</span> (i_episode + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:  <span class="hljs-comment"># 每10条序列打印一下这10条序列的平均回报</span><br>                pbar.set_postfix(&#123;<br>                    <span class="hljs-string">&#x27;episode&#x27;</span>:<br>                    <span class="hljs-string">&#x27;%d&#x27;</span> % (num_episodes / <span class="hljs-number">10</span> * i + i_episode + <span class="hljs-number">1</span>),<br>                    <span class="hljs-string">&#x27;return&#x27;</span>:<br>                    <span class="hljs-string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="hljs-number">10</span>:])<br>                &#125;)<br>            pbar.update(<span class="hljs-number">1</span>)<br><br>episodes_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(return_list)))<br>plt.plot(episodes_list, return_list)<br>plt.xlabel(<span class="hljs-string">&#x27;Episodes&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Returns&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;5-step Sarsa on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-string">&#x27;Cliff Walking&#x27;</span>))<br>plt.show()<br></code></pre></td></tr></table></figure><h1 id="5-Q-learning算法"><a href="#5-Q-learning算法" class="headerlink" title="5 Q-learning算法"></a>5 Q-learning算法</h1><p>除了 Sarsa，还有一种非常著名的基于时序差分算法的强化学习算法——Q-learning。Q-learning 和 Sarsa 的最大区别在于 Q-learning 的时序差分更新方式为<br>$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[R_t+\gamma\max_aQ(s_{t+1},a)-Q(s_t,a_t)]$<br>算法流程：<br><img src="/image/RL-42.png"><br>我们可以用价值迭代的思想来理解 Q-learning，即 Q-learning 是直接在估计$Q^*$，因为动作价值函数的贝尔曼最优方程是:<br>$Q^*(s,a)&#x3D;r(s,a)+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}|s,a)\max_{a^{\prime}}Q^*(s^{\prime},a^{\prime})$<br>需要强调的是，Q-learning 的更新并非必须使用当前贪心策略采样得到的数据，给定任意的$(s,a,r,s’)$<br>都可以直接根据更新公式来更新Q。提部Sarsa用的是当前策略在$s’$的动作，是在线策略算法，Q-learning是离线策略算法。</p><h2 id="5-1在线策略算法与离线策略算法"><a href="#5-1在线策略算法与离线策略算法" class="headerlink" title="5.1在线策略算法与离线策略算法"></a>5.1在线策略算法与离线策略算法</h2><p>我们称采样数据的策略为<strong>行为策略</strong>（behavior policy），称用这些数据来更新的策略为<strong>目标策略</strong>（target policy）。在线策略（on-policy）算法表示行为策略和目标策略是同一个策略；而离线策略（off-policy）算法表示行为策略和目标策略不是同一个策略。Sarsa 是典型的在线策略算法，而 Q-learning 是典型的离线策略算法。判断二者类别的一个重要手段是看计算时序差分的价值目标的数据是否来自当前的策略，如下图所示。具体而言：</p><ul><li>对于 Sarsa，它的更新公式必须使用来自当前策略采样得到的五元组$(s,a,r,s’,a’)$，因此它是在线策略学习方法；</li><li>对于 Q-learning，它的更新公式使用的是四元组$(s,a,r,s’)$来更新当前状态动作对的价值$Q(s,a)$，数据中的s和a是给定的条件，r和s’皆由环境采样得到，该四元组并不需要一定是当前策略采样得到的数据，也可以来自行为策略，因此它是离线策略算法。<br><img src="/image/RL-43.png"></li></ul><h3 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">QLearning</span>:<br>    <span class="hljs-string">&quot;&quot;&quot; Q-learning算法 &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, ncol, nrow, epsilon, alpha, gamma, n_action=<span class="hljs-number">4</span></span>):<br>        <span class="hljs-variable language_">self</span>.Q_table = np.zeros([nrow * ncol, n_action])  <span class="hljs-comment"># 初始化Q(s,a)表格</span><br>        <span class="hljs-variable language_">self</span>.n_action = n_action  <span class="hljs-comment"># 动作个数</span><br>        <span class="hljs-variable language_">self</span>.alpha = alpha  <span class="hljs-comment"># 学习率</span><br>        <span class="hljs-variable language_">self</span>.gamma = gamma  <span class="hljs-comment"># 折扣因子</span><br>        <span class="hljs-variable language_">self</span>.epsilon = epsilon  <span class="hljs-comment"># epsilon-贪婪策略中的参数</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">take_action</span>(<span class="hljs-params">self, state</span>):  <span class="hljs-comment">#选取下一步的操作</span><br>        <span class="hljs-keyword">if</span> np.random.random() &lt; <span class="hljs-variable language_">self</span>.epsilon:<br>            action = np.random.randint(<span class="hljs-variable language_">self</span>.n_action)<br>        <span class="hljs-keyword">else</span>:<br>            action = np.argmax(<span class="hljs-variable language_">self</span>.Q_table[state])<br>        <span class="hljs-keyword">return</span> action<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">best_action</span>(<span class="hljs-params">self, state</span>):  <span class="hljs-comment"># 用于打印策略</span><br>        Q_max = np.<span class="hljs-built_in">max</span>(<span class="hljs-variable language_">self</span>.Q_table[state])<br>        a = [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.n_action)]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.n_action):<br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.Q_table[state, i] == Q_max:<br>                a[i] = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> a<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, s0, a0, r, s1</span>):<br>        td_error = r + <span class="hljs-variable language_">self</span>.gamma * <span class="hljs-variable language_">self</span>.Q_table[s1].<span class="hljs-built_in">max</span>(<br>        ) - <span class="hljs-variable language_">self</span>.Q_table[s0, a0]<br>        <span class="hljs-variable language_">self</span>.Q_table[s0, a0] += <span class="hljs-variable language_">self</span>.alpha * td_error<br>        np.random.seed(<span class="hljs-number">0</span>)<br>epsilon = <span class="hljs-number">0.1</span><br>alpha = <span class="hljs-number">0.1</span><br>gamma = <span class="hljs-number">0.9</span><br>agent = QLearning(ncol, nrow, epsilon, alpha, gamma)<br>num_episodes = <span class="hljs-number">500</span>  <span class="hljs-comment"># 智能体在环境中运行的序列的数量</span><br><br>return_list = []  <span class="hljs-comment"># 记录每一条序列的回报</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):  <span class="hljs-comment"># 显示10个进度条</span><br>    <span class="hljs-comment"># tqdm的进度条功能</span><br>    <span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-built_in">int</span>(num_episodes / <span class="hljs-number">10</span>), desc=<span class="hljs-string">&#x27;Iteration %d&#x27;</span> % i) <span class="hljs-keyword">as</span> pbar:<br>        <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(num_episodes / <span class="hljs-number">10</span>)):  <span class="hljs-comment"># 每个进度条的序列数</span><br>            episode_return = <span class="hljs-number">0</span><br>            state = env.reset()<br>            done = <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br>                action = agent.take_action(state)<br>                next_state, reward, done = env.step(action)<br>                episode_return += reward  <span class="hljs-comment"># 这里回报的计算不进行折扣因子衰减</span><br>                agent.update(state, action, reward, next_state)<br>                state = next_state<br>            return_list.append(episode_return)<br>            <span class="hljs-keyword">if</span> (i_episode + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:  <span class="hljs-comment"># 每10条序列打印一下这10条序列的平均回报</span><br>                pbar.set_postfix(&#123;<br>                    <span class="hljs-string">&#x27;episode&#x27;</span>:<br>                    <span class="hljs-string">&#x27;%d&#x27;</span> % (num_episodes / <span class="hljs-number">10</span> * i + i_episode + <span class="hljs-number">1</span>),<br>                    <span class="hljs-string">&#x27;return&#x27;</span>:<br>                    <span class="hljs-string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="hljs-number">10</span>:])<br>                &#125;)<br>            pbar.update(<span class="hljs-number">1</span>)<br><br>episodes_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(return_list)))<br>plt.plot(episodes_list, return_list)<br>plt.xlabel(<span class="hljs-string">&#x27;Episodes&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Returns&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Q-learning on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-string">&#x27;Cliff Walking&#x27;</span>))<br>plt.show()<br><br>action_meaning = [<span class="hljs-string">&#x27;^&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>, <span class="hljs-string">&#x27;&lt;&#x27;</span>, <span class="hljs-string">&#x27;&gt;&#x27;</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Q-learning算法最终收敛得到的策略为：&#x27;</span>)<br>print_agent(agent, env, action_meaning, <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">37</span>, <span class="hljs-number">47</span>)), [<span class="hljs-number">47</span>])<br></code></pre></td></tr></table></figure><p>需要注意的是，打印出来的回报是行为策略在环境中交互得到的，而不是 Q-learning 算法在学习的目标策略的真实回报。我们把目标策略的行为打印出来后，发现其更偏向于走在悬崖边上，这与 Sarsa 算法得到的比较保守的策略相比是更优的。 但是仔细观察 Sarsa 和 Q-learning 在训练过程中的回报曲线图，我们可以发现，在一个序列中 Sarsa 获得的期望回报是高于 Q-learning 的。这是因为在训练过程中智能体采取基于当前$Q(s,a)$函数的$\epsilon$-贪婪策略来平衡探索与利用，Q-learning 算法由于沿着悬崖边走，会以一定概率探索“掉入悬崖”这一动作，而 Sarsa 相对保守的路线使智能体几乎不可能掉入悬崖。</p><h1 id="6-总结"><a href="#6-总结" class="headerlink" title="6 总结"></a>6 总结</h1><p>本章介绍了无模型的强化学习中的一种非常重要的算法——时序差分算法。时序差分算法的核心思想是用对未来动作选择的价值估计来更新对当前动作选择的价值估计，这是强化学习中的核心思想之一。本章重点讨论了 Sarsa 和 Q-learning 这两个最具有代表性的时序差分算法。当环境是有限状态集合和有限动作集合时，这两个算法非常好用，可以根据任务是否允许在线策略学习来决定使用哪一个算法。 值得注意的是，尽管离线策略学习可以让智能体基于经验回放池中的样本来学习，但需要保证智能体在学习的过程中可以不断和环境进行交互，将采样得到的最新的经验样本加入经验回放池中，从而使经验回放池中有一定数量的样本和当前智能体策略对应的数据分布保持很近的距离。如果不允许智能体在学习过程中和环境进行持续交互，而是完全基于一个给定的样本集来直接训练一个策略，这样的学习范式被称为<strong>离线强化学习</strong>（offline reinforcement learning）。</p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>仿真</tag>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>jottings-1</title>
    <link href="/2024/08/20/jottings-1/"/>
    <url>/2024/08/20/jottings-1/</url>
    
    <content type="html"><![CDATA[<h1 id="本文写于2024-8-7-23-11"><a href="#本文写于2024-8-7-23-11" class="headerlink" title="本文写于2024.8.7 23:11"></a>本文写于2024.8.7 23:11</h1><p>22岁的我比17岁刚上大学的我变了好多。<br> 长高了几厘米，聪明了一点点，也长大了一点点。<br> 本来是想学新闻的，喜欢文字工作。但莫名其妙选到的了通信工程专业，一开始也不太会但是慢慢的也学进去了，后来又继续读相关专业的硕士研究生。<br>慢慢地也明白了，做什么都是一样的，有目标就有动力，有动力就很充实，充实了就不会乱想。<br>但是对文字的热爱依旧没有消失，在日渐冷漠的社会环境下里，文字依旧能温暖人心，带来力量。<br>祝看完这段文字的你保持平和，开心，明天的你会更好。<br>附上一张小猫团团的美照</p><p><img src="/image/jottings-13.jpg"></p><p>PS：自从千千，十三走后每天都很难受；小福，圆圆丢了；团团回去后更难受了；他们都和我有过一段时间的共同生活经历，就像我的孩子一样，现在慢慢的，也不像以前那样把猫完完全全看成动物，和人生活在一起，它们就是我的小家的一份子。<br>不管它们是不是还活着，我要记住它们。正如《寻梦环游记》中说的，真正的死亡是再没有一个人记得你。<br>我将它们放在这里，我永远记得它们。<br>谨以此文纪念 “2023&#x2F;09&#x2F;01—2024&#x2F;03&#x2F;07” </p><p>附上一张千千十三小福和一张福福圆圆团团的合照</p><p><img src="/image/jottings-11.jpg"></p><p><img src="/image/jottings-12.jpg"></p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活</tag>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL-3 动态规划算法</title>
    <link href="/2024/08/19/RL-3/"/>
    <url>/2024/08/19/RL-3/</url>
    
    <content type="html"><![CDATA[<p>#强化学习  </p><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p><strong>动态规划</strong>（dynamic programming）是程序设计算法中非常重要的内容，能够高效解决一些经典问题，例如背包问题和最短路径规划。动态规划的基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到目标问题的解。动态规划会保存已解决的子问题的答案，在求解目标问题的过程中，需要这些子问题答案时就可以直接利用，避免重复计算。本章介绍如何用动态规划的思想来求解在马尔可夫决策过程中的最优策略。<br>基于动态规划的强化学习算法主要有两种：一是<strong>策略迭代</strong>（policy iteration），二是<strong>价值迭代</strong>（value iteration）。其中，策略迭代由两部分组成：<strong>策略评估</strong>（policy evaluation）和<strong>策略提升</strong>（policy improvement）。具体来说，策略迭代中的策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数，这是一个动态规划的过程；而价值迭代直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。</p><p>不同于的蒙特卡洛方法和时序差分算法，基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。在这样一个白盒环境中，不需要通过智能体和环境的大量交互来学习，可以直接用动态规划求解状态价值函数。但是，现实中的白盒环境很少，这也是动态规划算法的局限之处，我们无法将其运用到很多实际场景中。另外，策略迭代和价值迭代通常只适用于有限马尔可夫决策过程，即状态空间和动作空间是离散且有限的。</p><h1 id="2-示例：悬崖漫步环境"><a href="#2-示例：悬崖漫步环境" class="headerlink" title="2 示例：悬崖漫步环境"></a>2 示例：悬崖漫步环境</h1><p>本节使用策略迭代和价值迭代来求解<strong>悬崖漫步</strong>（Cliff Walking）这个环境中的最优策略。接下来先简单介绍一下该环境。</p><p>悬崖漫步是一个非常经典的强化学习环境，它要求一个智能体从起点出发，避开悬崖行走，最终到达目标位置。如图 4-1 所示，有一个 4×12 的网格世界，每一个网格表示一个状态。智能体的起点是左下角的状态，目标是右下角的状态，智能体在每一个状态都可以采取 4 种动作：上、下、左、右。如果智能体采取动作后触碰到边界墙壁则状态不发生改变，否则就会相应到达下一个状态。环境中有一段悬崖，智能体掉入悬崖或到达目标状态都会结束动作并回到起点，也就是说掉入悬崖或者达到目标状态是终止状态。智能体每走一步的奖励是 −1，掉入悬崖的奖励是 −100。<br><img src="/image/RL-31.png"></p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CliffWalkingEnv</span>:<br>    <span class="hljs-string">&quot;&quot;&quot; 悬崖漫步环境&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, ncol=<span class="hljs-number">12</span>, nrow=<span class="hljs-number">4</span></span>):<br>        <span class="hljs-variable language_">self</span>.ncol = ncol  <span class="hljs-comment"># 定义网格世界的列</span><br>        <span class="hljs-variable language_">self</span>.nrow = nrow  <span class="hljs-comment"># 定义网格世界的行</span><br>        <span class="hljs-comment"># 转移矩阵P[state][action] = [(p, next_state, reward, done)]包含下一个状态和奖励</span><br>        <span class="hljs-variable language_">self</span>.P = <span class="hljs-variable language_">self</span>.createP()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">createP</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 初始化</span><br>        P = [[[] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.nrow * <span class="hljs-variable language_">self</span>.ncol)]<br>        <span class="hljs-comment"># 4种动作, change[0]:上,change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)</span><br>        <span class="hljs-comment"># 定义在左上角</span><br>        change = [[<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.nrow):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.ncol):<br>                <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                    <span class="hljs-comment"># 位置在悬崖或者目标状态,因为无法继续交互,任何动作奖励都为0</span><br>                    <span class="hljs-keyword">if</span> i == <span class="hljs-variable language_">self</span>.nrow - <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> j &gt; <span class="hljs-number">0</span>:<br>                        P[i * <span class="hljs-variable language_">self</span>.ncol + j][a] = [(<span class="hljs-number">1</span>, i * <span class="hljs-variable language_">self</span>.ncol + j, <span class="hljs-number">0</span>,<br>                                                    <span class="hljs-literal">True</span>)]<br>                        <span class="hljs-keyword">continue</span><br>                    <span class="hljs-comment"># 其他位置</span><br>                    next_x = <span class="hljs-built_in">min</span>(<span class="hljs-variable language_">self</span>.ncol - <span class="hljs-number">1</span>, <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, j + change[a][<span class="hljs-number">0</span>]))<br>                    next_y = <span class="hljs-built_in">min</span>(<span class="hljs-variable language_">self</span>.nrow - <span class="hljs-number">1</span>, <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, i + change[a][<span class="hljs-number">1</span>]))<br>                    next_state = next_y * <span class="hljs-variable language_">self</span>.ncol + next_x<br>                    reward = -<span class="hljs-number">1</span><br>                    done = <span class="hljs-literal">False</span><br>                    <span class="hljs-comment"># 下一个位置在悬崖或者终点</span><br>                    <span class="hljs-keyword">if</span> next_y == <span class="hljs-variable language_">self</span>.nrow - <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> next_x &gt; <span class="hljs-number">0</span>:<br>                        done = <span class="hljs-literal">True</span><br>                        <span class="hljs-keyword">if</span> next_x != <span class="hljs-variable language_">self</span>.ncol - <span class="hljs-number">1</span>:  <span class="hljs-comment"># 下一个位置在悬崖</span><br>                            reward = -<span class="hljs-number">100</span><br>                    P[i * <span class="hljs-variable language_">self</span>.ncol + j][a] = [(<span class="hljs-number">1</span>, next_state, reward, done)]<br>        <span class="hljs-keyword">return</span> P<br></code></pre></td></tr></table></figure><h1 id="3-策略迭代算法"><a href="#3-策略迭代算法" class="headerlink" title="3 策略迭代算法"></a>3 策略迭代算法</h1><p>策略迭代是策略评估和策略提升不断循环交替，直至最后得到最优策略的过程。本节分别对这两个过程进行详细介绍。</p><h2 id="3-1策略评估"><a href="#3-1策略评估" class="headerlink" title="3.1策略评估"></a>3.1策略评估</h2><p>策略评估这一过程用来计算一个策略的状态价值函数。回顾一下之前学习的贝尔曼期望方程：<br>$V^\pi(s)&#x3D;\sum_{a\in A}\pi(a|s)\left(r(s,a)+\gamma\sum_{s^{\prime}\in S}p(s^{\prime}|s,a)V^\pi(s^{\prime})\right)$<br>其中，$\pi(a|s)$是策略$\pi$在状态s下采取动作的概率。可以看到，当知道奖励函数和状态转移函数时，我们可以根据下一个状态的价值来计算当前状态的价值。因此，根据动态规划的思想，可以把计算下一个可能状态的价值当成一个子问题，把计算当前状态的价值看作当前问题。在得知子问题的解后，就可以求解当前问题。更一般的，考虑所有的状态，就变成了用上一轮的状态价值函数来计算当前这一轮的状态价值函数，即<br>$V^{k+1}(s)&#x3D;\sum_{a\in A}\pi(a|s)\left(r(s,a)+\gamma\sum_{s’\in S}P(s’|s,a)V^k(s’)\right)$</p><h2 id="3-2策略提升"><a href="#3-2策略提升" class="headerlink" title="3.2策略提升"></a>3.2策略提升</h2><p>使用策略评估计算得到当前策略的状态价值函数之后，我们可以据此来改进该策略。假设此时对于策略$\pi$，我们已经知道其价值$V^{\pi}$，也就是知道了在策略$\pi$下从每一个状态s出发最终得到的期望回报。我们要如何改变策略来获得在状态s下更高的期望回报呢？假设智能体在状态s下采取动作a，之后的动作依旧遵循策略$\pi$，此时得到的期望回报其实就是动作价值$Q^{\pi}(s,a)$。如果我们有$Q^{\pi}(s,a)&gt;$V^\pi(s)，则说明在状态s下采取动作a会比原来的策略$\pi(a|s)$得到更高的期望回报。</p><p>我们发现构造的贪心策略满足策略提升定理的条件，所以策略能够比策略更好或者至少与其一样好。这个根据贪心法选取动作从而得到新的策略的过程称为策略提升。当策略提升之后得到的策略和之前的策略一样时，说明策略迭代达到了收敛，此时$\pi^{\prime}$和$\pi$就是最优策略。</p><p>$\begin{aligned}<br>V^{\pi}(s)&amp; \leq Q^\pi(s,\pi’(s)) \<br>&amp;&#x3D;\mathbb{E}<em>{\pi’}[R_t+\gamma V^{\pi}(S</em>{t+1})|S_t&#x3D;s] \<br>&amp;\leq\mathbb{E}<em>{\pi^{\prime}}[R_t+\gamma Q^\pi(S</em>{t+1},\pi^{\prime}(S_{t+1}))|S_t&#x3D;s] \<br>&amp;&#x3D;\mathbb{E}<em>{\pi^{\prime}}[R_t+\gamma R</em>{t+1}+\gamma^2V^\pi(S_{t+2})|S_t&#x3D;s] \<br>&amp;\leq\mathbb{E}<em>{\pi^{\prime}}[R_t+\gamma R</em>{t+1}+\gamma^2R_{t+2}+\gamma^3V^\pi(S_{t+3})|S_t&#x3D;s] \<br>&amp;: \<br>&amp;\leq\mathbb{E}<em>{\pi^{\prime}}[R_t+\gamma R</em>{t+1}+\gamma^2R_{t+2}+\gamma^3R_{t+3}+\cdots|S_t&#x3D;s] \<br>&amp;&#x3D;V^{\pi^{\prime}}(s)<br>\end{aligned}$</p><p>可以看到，推导过程中的每一个时间步都用到局部动作价值优势</p><h2 id="3-3策略迭代算法"><a href="#3-3策略迭代算法" class="headerlink" title="3.3策略迭代算法"></a>3.3策略迭代算法</h2><p>总体来说，策略迭代算法的过程如下：对当前的策略进行策略评估，得到其状态价值函数，然后根据该状态价值函数进行策略提升以得到一个更好的新策略，接着继续评估新策略、提升策略……直至最后收敛到最优策略<br>$$\pi^0\overset{\textit{策略评估}}{\operatorname*{\longrightarrow}}V^{\pi^0}\overset{\textit{策略提升}}{\operatorname*{\longrightarrow}}\pi^1\overset{\textit{策略评估}}{\operatorname*{\longrightarrow}}V^{\pi^1}\overset{\textit{策略提升}}{\operatorname*{\longrightarrow}}\pi^2\overset{\textit{策略评估}}{\operatorname*{\longrightarrow}}\ldots\overset{\textit{策略提升}}{\operatorname*{\longrightarrow}}\pi^*$$<br>结合策略评估和策略提升，我们得到以下策略迭代算法：<br><img src="/image/RL-32.png"></p><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PolicyIteration</span>:<br>    <span class="hljs-string">&quot;&quot;&quot; 策略迭代算法 &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, env, theta, gamma</span>):<br>        <span class="hljs-variable language_">self</span>.env = env<br>        <span class="hljs-variable language_">self</span>.v = [<span class="hljs-number">0</span>] * <span class="hljs-variable language_">self</span>.env.ncol * <span class="hljs-variable language_">self</span>.env.nrow  <span class="hljs-comment"># 初始化价值为0</span><br>        <span class="hljs-variable language_">self</span>.pi = [[<span class="hljs-number">0.25</span>, <span class="hljs-number">0.25</span>, <span class="hljs-number">0.25</span>, <span class="hljs-number">0.25</span>]<br>                   <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.env.ncol * <span class="hljs-variable language_">self</span>.env.nrow)]  <span class="hljs-comment"># 初始化为均匀随机策略</span><br>        <span class="hljs-variable language_">self</span>.theta = theta  <span class="hljs-comment"># 策略评估收敛阈值</span><br>        <span class="hljs-variable language_">self</span>.gamma = gamma  <span class="hljs-comment"># 折扣因子</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">policy_evaluation</span>(<span class="hljs-params">self</span>):  <span class="hljs-comment"># 策略评估</span><br>        cnt = <span class="hljs-number">1</span>  <span class="hljs-comment"># 计数器</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-number">1</span>:<br>            max_diff = <span class="hljs-number">0</span><br>            new_v = [<span class="hljs-number">0</span>] * <span class="hljs-variable language_">self</span>.env.ncol * <span class="hljs-variable language_">self</span>.env.nrow<br>            <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.env.ncol * <span class="hljs-variable language_">self</span>.env.nrow):<br>                qsa_list = []  <span class="hljs-comment"># 开始计算状态s下的所有Q(s,a)价值</span><br>                <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                    qsa = <span class="hljs-number">0</span><br>                    <span class="hljs-keyword">for</span> res <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.env.P[s][a]:<br>                        p, next_state, r, done = res<br>                        qsa += p * (r + <span class="hljs-variable language_">self</span>.gamma * <span class="hljs-variable language_">self</span>.v[next_state] * (<span class="hljs-number">1</span> - done))<br>                        <span class="hljs-comment"># 本章环境比较特殊,奖励和下一个状态有关,所以需要和状态转移概率相乘</span><br>                    qsa_list.append(<span class="hljs-variable language_">self</span>.pi[s][a] * qsa)<br>                new_v[s] = <span class="hljs-built_in">sum</span>(qsa_list)  <span class="hljs-comment"># 状态价值函数和动作价值函数之间的关系</span><br>                max_diff = <span class="hljs-built_in">max</span>(max_diff, <span class="hljs-built_in">abs</span>(new_v[s] - <span class="hljs-variable language_">self</span>.v[s]))<br>            <span class="hljs-variable language_">self</span>.v = new_v<br>            <span class="hljs-keyword">if</span> max_diff &lt; <span class="hljs-variable language_">self</span>.theta: <span class="hljs-keyword">break</span>  <span class="hljs-comment"># 满足收敛条件,退出评估迭代</span><br>            cnt += <span class="hljs-number">1</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;策略评估进行%d轮后完成&quot;</span> % cnt)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">policy_improvement</span>(<span class="hljs-params">self</span>):  <span class="hljs-comment"># 策略提升</span><br>        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.env.nrow * <span class="hljs-variable language_">self</span>.env.ncol):<br>            qsa_list = []<br>            <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                qsa = <span class="hljs-number">0</span><br>                <span class="hljs-keyword">for</span> res <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.env.P[s][a]:<br>                    p, next_state, r, done = res<br>                    qsa += p * (r + <span class="hljs-variable language_">self</span>.gamma * <span class="hljs-variable language_">self</span>.v[next_state] * (<span class="hljs-number">1</span> - done))<br>                qsa_list.append(qsa)<br>            maxq = <span class="hljs-built_in">max</span>(qsa_list)<br>            cntq = qsa_list.count(maxq)  <span class="hljs-comment"># 计算有几个动作得到了最大的Q值</span><br>            <span class="hljs-comment"># 让这些动作均分概率</span><br>            <span class="hljs-variable language_">self</span>.pi[s] = [<span class="hljs-number">1</span> / cntq <span class="hljs-keyword">if</span> q == maxq <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> q <span class="hljs-keyword">in</span> qsa_list]<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;策略提升完成&quot;</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.pi<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">policy_iteration</span>(<span class="hljs-params">self</span>):  <span class="hljs-comment"># 策略迭代</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-number">1</span>:<br>            <span class="hljs-variable language_">self</span>.policy_evaluation()<br>            old_pi = copy.deepcopy(<span class="hljs-variable language_">self</span>.pi)  <span class="hljs-comment"># 将列表进行深拷贝,方便接下来进行比较</span><br>            new_pi = <span class="hljs-variable language_">self</span>.policy_improvement()<br>            <span class="hljs-keyword">if</span> old_pi == new_pi: <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure><p>现在我们已经写好了环境代码和策略迭代代码。为了更好地展现最终的策略，接下来增加一个打印策略的函数，用于打印当前策略在每个状态下的价值以及智能体会采取的动作。对于打印出来的动作，我们用<code>^o&lt;o</code>表示等概率采取向左和向上两种动作，<code>ooo&gt;</code>表示在当前状态只采取向右动作。</p><h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def print_agent(agent, action_meaning, disaster=[], end=[]):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;状态价值：&quot;</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(agent.env.nrow):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(agent.env.ncol):<br>            # 为了输出美观,保持输出6个字符<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;%6.6s&#x27;</span> % (<span class="hljs-string">&#x27;%.3f&#x27;</span> % agent.v[i * agent.env.ncol + j]), <span class="hljs-attribute">end</span>=<span class="hljs-string">&#x27; &#x27;</span>)<br>        <span class="hljs-built_in">print</span>()<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;策略：&quot;</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(agent.env.nrow):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(agent.env.ncol):<br>            # 一些特殊的状态,例如悬崖漫步中的悬崖<br>            <span class="hljs-keyword">if</span> (i * agent.env.ncol + j) <span class="hljs-keyword">in</span> disaster:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;****&#x27;</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">&#x27; &#x27;</span>)<br>            elif (i * agent.env.ncol + j) <span class="hljs-keyword">in</span> end:  # 目标状态<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;EEEE&#x27;</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">&#x27; &#x27;</span>)<br>            <span class="hljs-keyword">else</span>:<br>                a = agent.pi[i * agent.env.ncol + j]<br>                pi_str = <span class="hljs-string">&#x27;&#x27;</span><br>                <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(len(action_meaning)):<br>                    pi_str += action_meaning[k] <span class="hljs-keyword">if</span> a[k] &gt; 0 <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;o&#x27;</span><br>                <span class="hljs-built_in">print</span>(pi_str, <span class="hljs-attribute">end</span>=<span class="hljs-string">&#x27; &#x27;</span>)<br>        <span class="hljs-built_in">print</span>()<br><br><br>env = CliffWalkingEnv()<br>action_meaning = [<span class="hljs-string">&#x27;^&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>, <span class="hljs-string">&#x27;&lt;&#x27;</span>, <span class="hljs-string">&#x27;&gt;&#x27;</span>]<br>theta = 0.001<br>gamma = 0.9<br>agent = PolicyIteration(env, theta, gamma)<br>agent.policy_iteration()<br>print_agent(agent, action_meaning, list(range(37, 47)), [47])<br></code></pre></td></tr></table></figure><p><img src="/image/RL-33.png"><br>经过 5 次策略评估和策略提升的循环迭代，策略收敛了，此时将获得的策略打印出来。用贝尔曼最优方程去检验其中每一个状态的价值，可以发现最终输出的策略的确是最优策略。</p><h2 id="3-4价值迭代算法"><a href="#3-4价值迭代算法" class="headerlink" title="3.4价值迭代算法"></a>3.4价值迭代算法</h2><p>从上面的代码运行结果中我们能发现，策略迭代中的策略评估需要进行很多轮才能收敛得到某一策略的状态函数，这需要很大的计算量，尤其是在状态和动作空间比较大的情况下。我们是否必须要完全等到策略评估完成后再进行策略提升呢？试想一下，可能出现这样的情况：虽然状态价值函数还没有收敛，但是不论接下来怎么更新状态价值，策略提升得到的都是同一个策略。如果只在策略评估中进行一轮价值更新，然后直接根据更新后的价值进行策略提升，这样是否可以呢？答案是肯定的，这其实就是本节将要讲解的价值迭代算法，它可以被认为是一种策略评估只进行了一轮更新的策略迭代算法。需要注意的是，价值迭代中不存在显式的策略，我们只维护一个状态价值函数。<br>确切来说，价值迭代可以看成一种动态规划过程，它利用的是贝尔曼最优方程：<br>$V^{k+1}(s)&#x3D;\max_{a\in\mathcal{A}}{r(s,a)+\gamma\sum_{s’\in\mathcal{S}}P(s’|s,a)V^k(s’)}$<br><img src="/image/RL-34.png"></p><h3 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ValueIteration</span>:<br>    <span class="hljs-string">&quot;&quot;&quot; 价值迭代算法 &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, env, theta, gamma</span>):<br>        <span class="hljs-variable language_">self</span>.env = env<br>        <span class="hljs-variable language_">self</span>.v = [<span class="hljs-number">0</span>] * <span class="hljs-variable language_">self</span>.env.ncol * <span class="hljs-variable language_">self</span>.env.nrow  <span class="hljs-comment"># 初始化价值为0</span><br>        <span class="hljs-variable language_">self</span>.theta = theta  <span class="hljs-comment"># 价值收敛阈值</span><br>        <span class="hljs-variable language_">self</span>.gamma = gamma<br>        <span class="hljs-comment"># 价值迭代结束后得到的策略</span><br>        <span class="hljs-variable language_">self</span>.pi = [<span class="hljs-literal">None</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.env.ncol * <span class="hljs-variable language_">self</span>.env.nrow)]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">value_iteration</span>(<span class="hljs-params">self</span>):<br>        cnt = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-number">1</span>:<br>            max_diff = <span class="hljs-number">0</span><br>            new_v = [<span class="hljs-number">0</span>] * <span class="hljs-variable language_">self</span>.env.ncol * <span class="hljs-variable language_">self</span>.env.nrow<br>            <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.env.ncol * <span class="hljs-variable language_">self</span>.env.nrow):<br>                qsa_list = []  <span class="hljs-comment"># 开始计算状态s下的所有Q(s,a)价值</span><br>                <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                    qsa = <span class="hljs-number">0</span><br>                    <span class="hljs-keyword">for</span> res <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.env.P[s][a]:<br>                        p, next_state, r, done = res<br>                        qsa += p * (r + <span class="hljs-variable language_">self</span>.gamma * <span class="hljs-variable language_">self</span>.v[next_state] * (<span class="hljs-number">1</span> - done))<br>                    qsa_list.append(qsa)  <span class="hljs-comment"># 这一行和下一行代码是价值迭代和策略迭代的主要区别</span><br>                new_v[s] = <span class="hljs-built_in">max</span>(qsa_list)<br>                max_diff = <span class="hljs-built_in">max</span>(max_diff, <span class="hljs-built_in">abs</span>(new_v[s] - <span class="hljs-variable language_">self</span>.v[s]))<br>            <span class="hljs-variable language_">self</span>.v = new_v<br>            <span class="hljs-keyword">if</span> max_diff &lt; <span class="hljs-variable language_">self</span>.theta: <span class="hljs-keyword">break</span>  <span class="hljs-comment"># 满足收敛条件,退出评估迭代</span><br>            cnt += <span class="hljs-number">1</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;价值迭代一共进行%d轮&quot;</span> % cnt)<br>        <span class="hljs-variable language_">self</span>.get_policy()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_policy</span>(<span class="hljs-params">self</span>):  <span class="hljs-comment"># 根据价值函数导出一个贪婪策略</span><br>        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.env.nrow * <span class="hljs-variable language_">self</span>.env.ncol):<br>            qsa_list = []<br>            <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                qsa = <span class="hljs-number">0</span><br>                <span class="hljs-keyword">for</span> res <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.env.P[s][a]:<br>                    p, next_state, r, done = res<br>                    qsa += p * (r + <span class="hljs-variable language_">self</span>.gamma * <span class="hljs-variable language_">self</span>.v[next_state] * (<span class="hljs-number">1</span> - done))<br>                qsa_list.append(qsa)<br>            maxq = <span class="hljs-built_in">max</span>(qsa_list)<br>            cntq = qsa_list.count(maxq)  <span class="hljs-comment"># 计算有几个动作得到了最大的Q值</span><br>            <span class="hljs-comment"># 让这些动作均分概率</span><br>            <span class="hljs-variable language_">self</span>.pi[s] = [<span class="hljs-number">1</span> / cntq <span class="hljs-keyword">if</span> q == maxq <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> q <span class="hljs-keyword">in</span> qsa_list]<br><br><br>env = CliffWalkingEnv()<br>action_meaning = [<span class="hljs-string">&#x27;^&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>, <span class="hljs-string">&#x27;&lt;&#x27;</span>, <span class="hljs-string">&#x27;&gt;&#x27;</span>]<br>theta = <span class="hljs-number">0.001</span><br>gamma = <span class="hljs-number">0.9</span><br>agent = ValueIteration(env, theta, gamma)<br>agent.value_iteration()<br>print_agent(agent, action_meaning, <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">37</span>, <span class="hljs-number">47</span>)), [<span class="hljs-number">47</span>])<br></code></pre></td></tr></table></figure><h1 id="4-冰湖环境"><a href="#4-冰湖环境" class="headerlink" title="4 冰湖环境"></a>4 冰湖环境</h1><p>除了悬崖漫步环境，本章还准备了另一个环境——<strong>冰湖</strong>（Frozen Lake）。冰湖环境的状态空间和动作空间是有限的，我们在该环境中也尝试一下策略迭代算法和价值迭代算法，以便更好地理解这两个算法。<br><img src="/image/RL-35.png"></p><h3 id="代码-基于gymnasium"><a href="#代码-基于gymnasium" class="headerlink" title="代码(基于gymnasium)"></a>代码(基于gymnasium)</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">import gymnasium <span class="hljs-keyword">as</span> gym<br><br>env = gym.make(<span class="hljs-string">&quot;FrozenLake-v1&quot;</span>)  <span class="hljs-comment"># 创建环境</span><br>env = env.unwrapped  <span class="hljs-comment"># 解封装才能访问状态转移矩阵P</span><br>env.render()  <span class="hljs-comment"># 环境渲染,通常是弹窗显示或打印出可视化的环境</span><br>holes = <span class="hljs-built_in">set</span>()<br><span class="hljs-keyword">ends</span> = <span class="hljs-built_in">set</span>()<br><span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> env.P:<br>    <span class="hljs-keyword">for</span> <span class="hljs-keyword">a</span> <span class="hljs-keyword">in</span> env.P[s]:<br>        <span class="hljs-keyword">for</span> s_ <span class="hljs-keyword">in</span> env.P[s][<span class="hljs-keyword">a</span>]:<br>            <span class="hljs-keyword">if</span> s_[<span class="hljs-number">2</span>] == <span class="hljs-number">1.0</span>:  <span class="hljs-comment"># 获得奖励为1,代表是目标</span><br>                <span class="hljs-keyword">ends</span>.<span class="hljs-built_in">add</span>(s_[<span class="hljs-number">1</span>])<br>            <span class="hljs-keyword">if</span> s_[<span class="hljs-number">3</span>] == True:<br>                holes.<span class="hljs-built_in">add</span>(s_[<span class="hljs-number">1</span>])<br>holes = holes - <span class="hljs-keyword">ends</span><br>print(<span class="hljs-string">&quot;冰洞的索引:&quot;</span>, holes)<br>print(<span class="hljs-string">&quot;目标的索引:&quot;</span>, <span class="hljs-keyword">ends</span>)<br><br><span class="hljs-keyword">for</span> <span class="hljs-keyword">a</span> <span class="hljs-keyword">in</span> env.P[<span class="hljs-number">14</span>]:  <span class="hljs-comment"># 查看目标左边一格的状态转移信息</span><br>    print(env.P[<span class="hljs-number">14</span>][<span class="hljs-keyword">a</span>])<br><span class="hljs-comment"># 这个动作意义是Gym库针对冰湖环境事先规定好的</span><br>action_meaning = [<span class="hljs-string">&#x27;&lt;&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>, <span class="hljs-string">&#x27;&gt;&#x27;</span>, <span class="hljs-string">&#x27;^&#x27;</span>]<br>theta = <span class="hljs-number">1e-5</span><br>gamma = <span class="hljs-number">0.9</span><br>agent = PolicyIteration(env, theta, gamma)<br>agent.policy_iteration()<br>print_agent(agent, action_meaning, [<span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>], [<span class="hljs-number">15</span>])<br>action_meaning = [<span class="hljs-string">&#x27;&lt;&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>, <span class="hljs-string">&#x27;&gt;&#x27;</span>, <span class="hljs-string">&#x27;^&#x27;</span>]<br>theta = <span class="hljs-number">1e-5</span><br>gamma = <span class="hljs-number">0.9</span><br>agent = ValueIteration(env, theta, gamma)<br>agent.value_iteration()<br>print_agent(agent, action_meaning, [<span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>], [<span class="hljs-number">15</span>])<br></code></pre></td></tr></table></figure><p>这个最优策略很看上去比较反直觉，其原因是这是一个智能体会随机滑向其他状态的冰冻湖面。例如，在目标左边一格的状态，采取向右的动作时，它有可能会滑到目标左上角的位置，从该位置再次到达目标会更加困难，所以此时采取向下的动作是更为保险的，并且有一定概率能够滑到目标。我们再来尝试一下价值迭代算法。</p><h1 id="6-总结"><a href="#6-总结" class="headerlink" title="6 总结"></a>6 总结</h1><p>本章讲解了强化学习中两个经典的动态规划算法：策略迭代算法和价值迭代算法，它们都能用于求解最优价值和最优策略。动态规划的主要思想是利用贝尔曼方程对所有状态进行更新。需要注意的是，在利用贝尔曼方程进行状态更新时，我们会用到马尔可夫决策过程中的奖励函数和状态转移函数。如果智能体无法事先得知奖励函数和状态转移函数，就只能通过和环境进行交互来采样（状态-动作-奖励-下一状态）这样的数据，我们将在之后的章节中讲解如何求解这种情况下的最优策略。</p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>仿真</tag>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL-2.1 关于马尔可夫决策过程的补充说明</title>
    <link href="/2024/08/19/RL-2-1/"/>
    <url>/2024/08/19/RL-2-1/</url>
    
    <content type="html"><![CDATA[<p>#强化学习 </p><h1 id="1-前情提要"><a href="#1-前情提要" class="headerlink" title="1 前情提要"></a>1 前情提要</h1><p>本章是强化学习过程的重要概念，马尔可夫决策过程包含了状态信息以及状态之间的转移机制，强化学习解决问题的本质就是把问题抽象为一个马尔可夫决策过程。</p><h1 id="2-重点内容"><a href="#2-重点内容" class="headerlink" title="2 重点内容"></a>2 重点内容</h1><h2 id="2-1马尔可夫性质"><a href="#2-1马尔可夫性质" class="headerlink" title="2.1马尔可夫性质"></a>2.1马尔可夫性质</h2><p>指当且仅当某时刻的状态只取决于上一时刻的状态。<br>看似是下一个状态只取决于当前状态；<br>事实上，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。<br>t+1时刻只与t有关，由于马尔可夫性的链式关系，t时刻包含了t-1时刻状态的信息，以此类推。</p><h2 id="2-2马尔可夫过程"><a href="#2-2马尔可夫过程" class="headerlink" title="2.2马尔可夫过程"></a>2.2马尔可夫过程</h2><p>元组&lt;S,P&gt;可以描述一个马尔可夫过程，S是有限数量的状态集合，P是状态转移矩阵。<br>P包含所有状态，$n*n$的矩阵，$P_{i,j}$表示了从状态$s_i$转移到状态$s_j$的概率，$P(s^{\prime}|s)$称状态转移函数，转出概率和必须为1。<br>根据状态转移矩阵生成一个状态序列的过程叫做采样。</p><h2 id="2-3马尔可夫奖励过程"><a href="#2-3马尔可夫奖励过程" class="headerlink" title="2.3马尔可夫奖励过程"></a>2.3马尔可夫奖励过程</h2><p>在马尔可夫过程的元组上，加入$r,\gamma$;构成$&lt;S,P,r,\gamma&gt;$<br>新加入的分别是奖励函数和折扣因子。<br>r是指转移到状态s可以获得奖励的期望r(s)。<br>$\gamma$是折扣因子，远期利益具有不确定性，因此对远期利益打折扣。<br>在一个马尔可夫奖励过程中，从第t时刻的状态$S_t$开始，所有奖励的衰减之和称为$G_t$:<br>$G_t&#x3D;R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\cdots&#x3D;\sum_{k&#x3D;0}^\infty\gamma^kR_{t+k}$<br>回报是基于未来时间步的回报,所以要先算未来步数的回报。<br>价值函数：一个状态的期望回报称为这个状态的价值，所有状态的价值组成了价值函数。<br>$$\begin{aligned}<br>V(s)&amp; &#x3D;\mathbb{E}[G_t|S_t&#x3D;s] \<br>&amp;&#x3D;\mathbb{E}[R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\ldots|S_t&#x3D;s] \<br>&amp;&#x3D;\mathbb{E}[R_t+\gamma(R_{t+1}+\gamma R_{t+2}+\ldots)|S_t&#x3D;s] \<br>&amp;&#x3D;\mathbb{E}[R_t+\gamma G_{t+1}|S_t&#x3D;s] \<br>&amp;&#x3D;\mathbb{E}[R_t+\gamma V(S_{t+1})|S_t&#x3D;s]<br>\end{aligned}$$<br>价值函数包含：当前的奖励+折扣因子与下一时刻价值函数的积。<br>$V(s)&#x3D;r(s)+\gamma\sum_{s’\in S}p(s’|s)V(s’)$<br>其中，$s^{\prime}$是下一时刻的状态，上面就是所谓贝尔曼方程。<br>写成矩阵形式：<br>$$\begin{bmatrix}V(s_1)\V(s_2)\\cdots\V(s_n)\end{bmatrix}&#x3D;\begin{bmatrix}r(s_1)\r(s_2)\\cdots\r(s_n)\end{bmatrix}+\gamma\begin{bmatrix}P(s_1|s_1)&amp;p(s_2|s_1)&amp;\ldots&amp;P(s_n|s_1)\P(s_1|s_2)&amp;P(s_2|s_2)&amp;\ldots&amp;P(s_n|s_2)\\cdots\P(s_1|s_n)&amp;P(s_2|s_n)&amp;\ldots&amp;P(s_n|s_n)\end{bmatrix}\begin{bmatrix}V(s_1)\V(s_2)\\ldots\V(s_n)\end{bmatrix}$$<br>求解：<br>$$\begin{aligned}<br>\mathcal{V}&amp; &#x3D;\mathcal{R}+\gamma\mathcal{PV} \<br>(I-\gamma\mathcal{P})\mathcal{V}&amp; &#x3D;\mathcal{R} \<br>\mathcal{V}&amp; &#x3D;(I-\gamma\mathcal{P})^{-1}\mathcal{R}<br>\end{aligned}$$<br>价值函数的解析解。</p><h1 id="3-马尔可夫决策过程"><a href="#3-马尔可夫决策过程" class="headerlink" title="3 马尔可夫决策过程"></a>3 马尔可夫决策过程</h1><p>将马尔可夫奖励(MRP)过程+A,即智能体的动作。<br>区别在于：非自发，有外界刺激改变。<br>状态转移矩阵变成三维。<br>策略：通常用$\pi$表示。<br>状态价值函数：$V^{\pi}(s)$表示：从s + $\pi$ –&gt; v<br>动作价值函数：$Q^{\pi}(s,a)$表示：从s + $\pi$ + a –&gt; v<br>他们的对应关系：<br>$V^\pi(s)&#x3D;\sum_{a\in A}\pi(a|s)Q^\pi(s,a)$<br>使用策略$\pi$时：<br>$Q^\pi(s,a)&#x3D;r(s,a)+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}|s,a)V^\pi(s^{\prime})$<br>贝尔曼期望方程<br>$$\begin{aligned}<br>V^{\pi}(s)&amp; &#x3D;\mathbb{E}<em>\pi[R_t+\gamma V^\pi(S</em>{t+1})|S_t&#x3D;s] \<br>&amp;&#x3D;\sum_{a\in A}\pi(a|s)\left(r(s,a)+\gamma\sum_{s^{\prime}\in S}p(s^{\prime}|s,a)V^\pi(s^{\prime})\right) \<br>Q^\pi(s,a)&amp; &#x3D;\mathbb{E}<em>\pi[R_t+\gamma Q^\pi(S</em>{t+1},A_{t+1})|S_t&#x3D;s,A_t&#x3D;a] \<br>&amp;&#x3D;r(s,a)+\gamma\sum_{s^{\prime}\in S}p(s^{\prime}|s,a)\sum_{a^{\prime}\in A}\pi(a^{\prime}|s^{\prime})Q^\pi(s^{\prime},a^{\prime})<br>\end{aligned}$$<br>计算MDP下策略$\pi$的状态价值函数——&gt;MDP+$\pi$&#x3D;MRP<br>把策略的动作选择进行边缘化，得到的是没有动作的MRP。<br>某一个状态：根据策略将所有动作的概率进行加权，得到的奖励就可看作MRP在该状态下的奖励：<br>$r’(s)&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)r(s,a)$<br>状态转移概率：<br>$P’(s’|s)&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)P(s’|s,a)$</p><h1 id="4-最优策略"><a href="#4-最优策略" class="headerlink" title="4 最优策略"></a>4 最优策略</h1><p>于是在有限状态和动作集合的 MDP 中，至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是<strong>最优策略</strong>（optimal policy）。<br>最优贝尔曼方程：<br>$$V^*(s)&#x3D;\max_{a\in\mathcal{A}}{r(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)V^*(s^{\prime})}\Q^*(s,a)&#x3D;r(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)\max_{a^{\prime}\in\mathcal{A}}Q^*(s^{\prime},a^{\prime})$$</p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>仿真</tag>
      
      <tag>强化学习</tag>
      
      <tag>补充说明</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>jottings-0</title>
    <link href="/2024/08/18/jottings-0/"/>
    <url>/2024/08/18/jottings-0/</url>
    
    <content type="html"><![CDATA[<p>最近有点迷茫，不知道在干啥。<br>好在背单词、学日语、学英语口语、健身坚持了下来；<br>小论文状态进去了些，点子也有了，还在落实；<br>技术好久没学了，下周要部署新技术；<br>明天起还是正常早起吧，晚上学是安静、就是有点废身体。<br>祝我好运~  🐈</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活</tag>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL-2 马尔可夫决策过程</title>
    <link href="/2024/08/18/RL-2/"/>
    <url>/2024/08/18/RL-2/</url>
    
    <content type="html"><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p><strong>马尔可夫决策过程</strong>（Markov decision process，MDP）是强化学习的重要概念。要学好强化学习，我们首先要掌握马尔可夫决策过程的基础知识。前两章所说的强化学习中的环境一般就是一个马尔可夫决策过程。与多臂老虎机问题不同，马尔可夫决策过程包含状态信息以及状态之间的转移机制。如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素。本章将从马尔可夫过程出发，一步一步地进行介绍，最后引出马尔可夫决策过程。</p><h1 id="2-马尔可夫过程"><a href="#2-马尔可夫过程" class="headerlink" title="2 马尔可夫过程"></a>2 马尔可夫过程</h1><h2 id="2-1随机过程"><a href="#2-1随机过程" class="headerlink" title="2.1随机过程"></a>2.1随机过程</h2><p><strong>随机过程</strong>（stochastic process）是概率论的“动力学”部分。概率论的研究对象是静态的随机现象，而随机过程的研究对象是随时间演变的随机现象（例如天气随时间的变化、城市交通随时间的变化）。在随机过程中，随机现象在某时刻的取值是一个向量随机变量，用$S_t$表示，所有可能的状态组成状态集合S。随机现象便是状态的变化过程。在某时刻t的状态$S_t$通常取决于t时刻之前的状态。我们将已知历史信息$(S_1,\dots,S_t)$时下一个时刻的状态为$S_{t+1}$的概率表示成$P(S_{t+1}|S_1,\ldots,S_t)$</p><h2 id="2-2马尔可夫性质"><a href="#2-2马尔可夫性质" class="headerlink" title="2.2马尔可夫性质"></a>2.2马尔可夫性质</h2><p>当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有<strong>马尔可夫性质</strong>（Markov property），用公式表示为$P(S_{t+1}|S_t)&#x3D;P(S_{t+1}|S_1,\ldots,S_t)$<br>也就是说，当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。因为虽然t+1时刻的状态只与t时刻的状态有关，但是t时刻的状态其实包含了t-1时刻的状态的信息，通过这种链式的关系，历史的信息被传递到了现在。马尔可夫性可以大大简化运算，因为只要当前状态可知，所有的历史信息都不再需要了，利用当前状态信息就可以决定未来。</p><h2 id="2-3马尔可夫过程"><a href="#2-3马尔可夫过程" class="headerlink" title="2.3马尔可夫过程"></a>2.3马尔可夫过程</h2><p><strong>马尔可夫过程</strong>（Markov process）指具有马尔可夫性质的随机过程，也被称为<strong>马尔可夫链</strong>（Markov chain）。我们通常用元组$\langle\mathcal{S},\mathcal{P}\rangle$描述一个马尔可夫过程，其中S是有限数量的状态集合，P是<strong>状态转移矩阵</strong>（state transition matrix）。假设一共n个状态，此时$\mathcal{S}&#x3D;{s_1,s_2,\ldots,s_n}$<br>状态转移矩阵P定义了所有状态对之间的状态转移概率，即：<br>$\mathcal{P}&#x3D;\begin{bmatrix}P(s_1|s_1)&amp;\cdots&amp;P(s_n|s_1)\\vdots&amp;\ddots&amp;\vdots\P(s_1|s_n)&amp;\cdots&amp;P(s_n|s_n)\end{bmatrix}$<br>矩阵P中第行第j列元素$P(s_j|s_i)&#x3D;P(S_{t+1}&#x3D;s_j|S_t&#x3D;s_i)$表示从状态$s_i$转移到状态$s_j$的概率，我们称为$P(s^{\prime}|s)$状态转移函数。从某个状态出发，到达其他状态的概率和必须为 1，即状态转移矩阵P的每一行的和为 1。<br>下图是一个具有是一个具有 6 个状态的马尔可夫过程的简单例子。其中每个绿色圆圈表示一个状态，每个状态都有一定概率（包括概率为 0）转移到其他状态，其中$s_6$通常被称为<strong>终止状态</strong>（terminal state），因为它不会再转移到其他状态，可以理解为它永远以概率 1 转移到自己。状态之间的虚线箭头表示状态的转移，箭头旁的数字表示该状态转移发生的概率。从每个状态出发转移到其他状态的概率总和为 1。例如，$s_1$有90%概率保持不变，有10%概率转移到$s_2$，而在$s_2$又有50%概率回到$s_3$，有50%概率转移到$s_3$。</p><p><img src="/image/RL-21.png"></p><p>转移矩阵如下：<br>$$\begin{gathered}\mathcal{P}&#x3D;\begin{bmatrix}0.9&amp;0.1&amp;0&amp;0&amp;0&amp;0\0.5&amp;0&amp;0.5&amp;0&amp;0&amp;0\0&amp;0&amp;0&amp;0.6&amp;0&amp;0.4\0&amp;0&amp;0&amp;0&amp;0.3&amp;0.7\0&amp;0.2&amp;0.3&amp;0.5&amp;0&amp;0\0&amp;0&amp;0&amp;0&amp;0&amp;1\end{bmatrix}\end{gathered}$$<br>其中第i行j列的值则代表从状态$s_i$转移到$s_j$的概率。<br>给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态<strong>序列</strong>（episode），这个步骤也被叫做<strong>采样</strong>（sampling）。例如，从$s_1$出发，可以生成序列<br>$s_1\to s_2\to s_3\to s_6$或<br>$s_1\to s_1\to s_2\to s_3\to s_4\to s_5\to s_3\to s_6$等</p><h1 id="3-马尔可夫奖励过程"><a href="#3-马尔可夫奖励过程" class="headerlink" title="3 马尔可夫奖励过程"></a>3 马尔可夫奖励过程</h1><p>在马尔可夫过程的基础上加入奖励函数r和折扣因子$\gamma$,就可以得到<strong>马尔可夫奖励过程</strong>（Markov reward process）。一个马尔可夫奖励过程由$\langle\mathcal{S},\mathcal{P},r,\gamma\rangle$构成，各个组成元素的含义如下所示。</p><ul><li>S是有限状态的集合。</li><li>P是状态转移矩阵。</li><li>r是奖励函数，某个状态的奖励 指转移到该状态时可以获得奖励的期望。</li><li>$\gamma$是折扣因子（discount factor），$\gamma$的取值范围为[0,1]。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的$\gamma$更关注长期的累计奖励，接近0的$\gamma$更考虑短期奖励。</li></ul><h2 id="3-1回报"><a href="#3-1回报" class="headerlink" title="3.1回报"></a>3.1回报</h2><p>在一个马尔可夫奖励过程中，从第t时刻状态$S_t$开始，直到终止状态时，所有奖励的衰减之和称为<strong>回报</strong>$G_t$（Return），公式如下：<br>$G_t&#x3D;R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\cdots&#x3D;\sum_{k&#x3D;0}^\infty\gamma^kR_{t+k}$<br>其中，表示在时刻获得的奖励。<br>如下图，构建一个马尔可夫奖励过程。<br><img src="/image/RL-22.png"><br>比如选取$s_1$为起始状态，设置$\gamma&#x3D;0.5$，采样到一条状态序列为$s_1\to s_2\to s_3\to s_6$，就可以计算$s_1$的回报$G_1$，得到$G_1&#x3D;-1+0.5\times(-2)+0.5^2\times(-2)&#x3D;-2.5$</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-comment"># -*- encoding: utf-8 -*-</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">@File    :   马尔科夫链.ipynb</span><br><span class="hljs-string">@Time    :   2024/08/17 01:54:00</span><br><span class="hljs-string">@Author  :   Neutrin</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># here put the import lib</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>np.random.seed(<span class="hljs-number">0</span>)<br><span class="hljs-comment"># 定义状态转移概率矩阵P</span><br>P = [<br>    [<span class="hljs-number">0.9</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],<br>    [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],<br>    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.4</span>],<br>    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.7</span>],<br>    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],<br>    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>],<br>]<br>P = np.array(P)<br><br>rewards = [-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>, -<span class="hljs-number">2</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]  <span class="hljs-comment"># 定义奖励函数</span><br>gamma = <span class="hljs-number">0.5</span>  <span class="hljs-comment"># 定义折扣因子</span><br><br><br><span class="hljs-comment"># 给定一条序列,计算从某个索引（起始状态）开始到序列最后（终止状态）得到的回报</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_return</span>(<span class="hljs-params">start_index, chain, gamma</span>):<br>    G = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(start_index, <span class="hljs-built_in">len</span>(chain))):<br>        G = gamma * G + rewards[chain[i] - <span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">return</span> G<br><br><br><span class="hljs-comment"># 一个状态序列,s1-s2-s3-s6</span><br>chain = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>]<br>start_index = <span class="hljs-number">0</span><br>G = compute_return(start_index, chain, gamma)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;根据本序列计算得到回报为：%s。&quot;</span> % G)<br></code></pre></td></tr></table></figure><h2 id="3-2价值函数"><a href="#3-2价值函数" class="headerlink" title="3.2价值函数"></a>3.2价值函数</h2><p>在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的<strong>价值</strong>（value）。所有状态的价值就组成了<strong>价值函数</strong>（value function），价值函数的输入为某个状态，输出为这个状态的价值。我们将价值函数写成<br>$V(s)&#x3D;\mathbb{E}[G_t|S_t&#x3D;s]$展开：<br>$$\begin{aligned}<br>V(s)&amp; &#x3D;\mathbb{E}[G_t|S_t&#x3D;s] \<br>&amp;&#x3D;\mathbb{E}[R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\ldots|S_t&#x3D;s] \<br>&amp;&#x3D;\mathbb{E}[R_t+\gamma(R_{t+1}+\gamma R_{t+2}+\ldots)|S_t&#x3D;s] \<br>&amp;&#x3D;\mathbb{E}[R_t+\gamma G_{t+1}|S_t&#x3D;s] \<br>&amp;&#x3D;\mathbb{E}[R_t+\gamma V(S_{t+1})|S_t&#x3D;s]<br>\end{aligned}$$<br>在上式的最后一个等号中，一方面，即时奖励的期望正是奖励函数的输出，即$\mathbb{E}[R_t|S_t&#x3D;s]&#x3D;r(s)$;另一方面，等式中剩余部分$\mathbb{E}[\gamma V(S_{t+1})|S_t&#x3D;s]$可以根据从状态s出发的转移概率得到，即：<br>$V(s)&#x3D;r(s)+\gamma\sum_{s^{\prime}\in S}p(s^{\prime}|s)V(s^{\prime})$<br>这就是贝尔曼方程，对每一个状态都成立。若一个马尔可夫奖励过程一共有n个状态，即$S&#x3D;{s_1,s_2,\dots,s_n}$,我们将所有状态的价值表示额成一个列向量$\mathcal{V}&#x3D;[V(s_1),V(s_2),\ldots,V(s_n)]^T$<br>同理，将奖励函数写成一个列向量$\mathcal{R}&#x3D;[r(s_1),r(s_2),\ldots,r(s_n)]^T$<br>于是我们可以将贝尔曼方程写成矩阵的形式：<br>$$\begin{bmatrix}V(s_1)\V(s_2)\\cdots\V(s_n)\end{bmatrix}&#x3D;\begin{bmatrix}r(s_1)\r(s_2)\\cdots\r(s_n)\end{bmatrix}+\gamma\begin{bmatrix}P(s_1|s_1)&amp;p(s_2|s_1)&amp;\ldots&amp;P(s_n|s_1)\P(s_1|s_2)&amp;P(s_2|s_2)&amp;\ldots&amp;P(s_n|s_2)\\cdots\P(s_1|s_n)&amp;P(s_2|s_n)&amp;\ldots&amp;P(s_n|s_n)\end{bmatrix}$$<br>求解矩阵运算得：<br>$\mathcal{V}&#x3D;\mathcal{R}+\gamma\mathcal{PV}$<br>以上解析解的计算复杂度是$O(n^3)$，其中是状态个数，因此这种方法只适用很小的马尔可夫奖励过程。求解较大规模的马尔可夫奖励过程中的价值函数时，可以使用<strong>动态规划</strong>（dynamic programming）算法、<strong>蒙特卡洛方法</strong>（Monte-Carlo method）和<strong>时序差分</strong>（temporal difference）.</p><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute</span>(<span class="hljs-params">P, rewards, gamma, states_num</span>):<br><br>    <span class="hljs-string">&#x27;&#x27;&#x27; 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 &#x27;&#x27;&#x27;</span><br>    rewards = np.array(rewards).reshape((-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment">#将rewards写成列向量形式</span><br>    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),<br><br>                   rewards)<br><br>    <span class="hljs-keyword">return</span> value<br><br>V = compute(P, rewards, gamma, <span class="hljs-number">6</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;MRP中每个状态价值分别为\n&quot;</span>, V)<br></code></pre></td></tr></table></figure><p>根据上述代码求解得到各个状态的价值V(s),具体如下：<br><img src="/image/RL-23.png"><br>我们现在用贝尔曼方程来进行简单的验证。例如，对于状态来说，当$\gamma$&#x3D;0.5时：<br>$\begin{aligned}&amp;V(s_4)&#x3D;r(s_4)+\gamma\sum_{s^{\prime}\in\mathcal{S}}P(s^{\prime}|s_4)V(s^{\prime})\&amp;10.54&#x3D;10+0.5\times(0.7\times0+0.3\times3.59)\end{aligned}$<br>可以发现左右两边的值几乎是相等的，说明我们求解得到的价值函数是满足状态为$s_4$时的贝尔曼方程。读者可以自行验证在其他状态时贝尔曼方程是否也成立。若贝尔曼方程对于所有状态都成立，就可以说明我们求解得到的价值函数是正确的。除了使用动态规划算法，马尔可夫奖励过程中的价值函数也可以通过蒙特卡洛方法估计得到，我们将在第5节中介绍该方法。</p><h1 id="4-马尔可夫决策过程"><a href="#4-马尔可夫决策过程" class="headerlink" title="4 马尔可夫决策过程"></a>4 马尔可夫决策过程</h1><p>前两节提到的马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程；而如果有一个外界的“刺激”来共同改变这个随机过程，就有了<strong>马尔可夫决策过程</strong>（Markov decision process，MDP）。我们将这个来自外界的刺激称为<strong>智能体</strong>（agent）的动作，在马尔可夫奖励过程（MRP）的基础上加入动作，就得到了马尔可夫决策过程（MDP）。马尔可夫决策过程由元组$\langle\mathcal{S},\mathcal{A},P,r,\gamma\rangle$构成，其中：</p><ul><li>$\mathcal{S}$是状态的集合；</li><li>$\mathcal{A}$是动作的集合；</li><li>$\mathcal{\gamma}$是折扣因子；</li><li>$\mathcal{r(s,a)}$是奖励函数，此时奖励可以同时取决于状态和动作，在奖励函数只取决于状态时，则退化为；</li><li>$P(s^{\prime}|s,a)$是状态转移函数，表示在状态s执行动作a之后到达状态$s^{\prime}$的概率。<br>我们发现 MDP 与 MRP 非常相像，主要区别为 MDP 中的状态转移函数和奖励函数都比 MRP 多了动作a作为自变量。注意，在上面 MDP 的定义中，我们不再使用类似 MRP 定义中的状态转移矩阵方式，而是直接表示成了状态转移函数。这样做一是因为此时状态转移与动作也有关，变成了一个三维数组，而不再是一个矩阵（二维数组）；二是因为状态转移函数更具有一般意义，例如，如果状态集合不是有限的，就无法用数组表示，但仍然可以用状态转移函数表示。我们在之后的课程学习中会遇到连续状态的 MDP 环境，那时状态集合都不是有限的。现在我们主要关注于离散状态的 MDP 环境，此时状态集合是有限的。<br>不同于马尔可夫奖励过程，在马尔可夫决策过程中，通常存在一个智能体来执行动作。例如，一艘小船在大海中随着水流自由飘荡的过程就是一个马尔可夫奖励过程，它如果凭借运气漂到了一个目的地，就能获得比较大的奖励；如果有个水手在控制着这条船往哪个方向前进，就可以主动选择前往目的地获得比较大的奖励。马尔可夫决策过程是一个与时间相关的不断进行的过程，在智能体和环境 MDP 之间存在一个不断交互的过程。<br>一般而言，它们之间的交互是如下图循环过程：智能体根据当前状态$S_t$选择动作$A_t$；对于状态$S_t$和动作$A_t$，MDP 根据奖励函数和状态转移函数得到$S_{t+1}$和$R_t$并反馈给智能体。智能体的目标是最大化得到的累计奖励。智能体根据当前状态从动作的集合A中选择一个动作的函数，被称为策略。<br><img src="/image/RL-24.png"></li></ul><h2 id="4-1策略"><a href="#4-1策略" class="headerlink" title="4.1策略"></a>4.1策略</h2><p>智能体的<strong>策略</strong>（Policy）通常用字母$\pi$表示。策略$\pi(a|s)&#x3D;P(A_t&#x3D;a|S_t&#x3D;s)$是一个函数，表示在输入状态情况下采取动作的概率。当一个策略是<strong>确定性策略</strong>（deterministic policy）时，它在每个状态时只输出一个确定性的动作，即只有该动作的概率为 1，其他动作的概率为 0；当一个策略是<strong>随机性策略</strong>（stochastic policy）时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。在 MDP 中，由于马尔可夫性质的存在，策略只需要与当前状态有关，不需要考虑历史状态。回顾一下在 MRP 中的价值函数，在 MDP 中也同样可以定义类似的价值函数。但此时的价值函数与策略有关，这意为着对于两个不同的策略来说，它们在同一个状态下的价值也很可能是不同的。这很好理解，因为不同的策略会采取不同的动作，从而之后会遇到不同的状态，以及获得不同的奖励，所以它们的累积奖励的期望也就不同，即状态价值不同。</p><h2 id="4-2状态价值函数"><a href="#4-2状态价值函数" class="headerlink" title="4.2状态价值函数"></a>4.2状态价值函数</h2><p>我们用$V^{\pi}(s)$表示在 MDP 中基于策略$\pi$的状态价值函数（state-value function），定义为从状态s出发遵循策略$\pi$能获得的期望回报，数学表达为：<br>$V^\pi(s)&#x3D;\mathbb{E}_\pi[G_t|S_t&#x3D;s]$</p><h2 id="4-3动作价值函数"><a href="#4-3动作价值函数" class="headerlink" title="4.3动作价值函数"></a>4.3动作价值函数</h2><p>不同于 MRP，在 MDP 中，由于动作的存在，我们额外定义一个<strong>动作价值函数</strong>（action-value function）。我们用$Q^\pi(s,a)$表示在 MDP 遵循策略$\pi$时，对当前状态s执行动作得到的期望回报：<br>$Q^\pi(s,a)&#x3D;\mathbb{E}<em>\pi[G_t|S_t&#x3D;s,A_t&#x3D;a]$<br>状态价值函数和动作价值函数之间的关系：在使用策略$\pi$中，状态s的价值等于在该状态下基于策略$\pi$采取所有动作的概率与相应的价值相乘再求和的结果：<br>$V^\pi(s)&#x3D;\sum</em>{a\in A}\pi(a|s)Q^\pi(s,a)$<br>使用策略$\pi$时，状态s下采取动作a的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积：<br>$Q^\pi(s,a)&#x3D;r(s,a)+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}|s,a)V^\pi(s^{\prime})$</p><h2 id="4-4贝尔曼期望方程"><a href="#4-4贝尔曼期望方程" class="headerlink" title="4.4贝尔曼期望方程"></a>4.4贝尔曼期望方程</h2><p>在贝尔曼方程中加上“期望”二字是为了与接下来的贝尔曼最优方程进行区分。我们通过简单推导就可以分别得到两个价值函数的<strong>贝尔曼期望方程</strong>（Bellman Expectation Equation）：<br>$$\begin{aligned}<br>V^{\pi}(s)&amp; &#x3D;\mathbb{E}<em>\pi[R_t+\gamma V^\pi(S</em>{t+1})|S_t&#x3D;s] \<br>&amp;&#x3D;\sum_{a\in A}\pi(a|s)\left(r(s,a)+\gamma\sum_{s^{\prime}\in S}p(s^{\prime}|s,a)V^\pi(s^{\prime})\right) \<br>Q^{\pi}(s,a)&amp; &#x3D;\mathbb{E}<em>\pi[R_t+\gamma Q^\pi(S</em>{t+1},A_{t+1})|S_t&#x3D;s,A_t&#x3D;a] \<br>&amp;&#x3D;r(s,a)+\gamma\sum_{s^{\prime}\in S}p(s^{\prime}|s,a)\sum_{a^{\prime}\in A}\pi(a^{\prime}|s^{\prime})Q^\pi(s^{\prime},a^{\prime})<br>\end{aligned}$$<br>价值函数和贝尔曼方程是强化学习非常重要的组成部分，之后的一些强化学习算法都是据此推导出来的，读者需要明确掌握！<br>下图是一个马尔可夫决策过程的简单例子，其中每个深色圆圈代表一个状态，一共有5个状态。黑色实线箭头代表可以采取的动作，浅色小圆圈代表动作，需要注意，并非在每个状态都能采取所有动作。<br><img src="/image/RL-25.png"></p><p>接下来我们编写代码来表示图 3-4 中的马尔可夫决策过程，并定义两个策略。第一个策略是一个完全随机策略，即在每个状态下，智能体会以同样的概率选取它可能采取的动作。</p><h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs makefile">S = [<span class="hljs-string">&quot;s1&quot;</span>, <span class="hljs-string">&quot;s2&quot;</span>, <span class="hljs-string">&quot;s3&quot;</span>, <span class="hljs-string">&quot;s4&quot;</span>, <span class="hljs-string">&quot;s5&quot;</span>]  <span class="hljs-comment"># 状态集合</span><br>A = [<span class="hljs-string">&quot;保持s1&quot;</span>, <span class="hljs-string">&quot;前往s1&quot;</span>, <span class="hljs-string">&quot;前往s2&quot;</span>, <span class="hljs-string">&quot;前往s3&quot;</span>, <span class="hljs-string">&quot;前往s4&quot;</span>, <span class="hljs-string">&quot;前往s5&quot;</span>, <span class="hljs-string">&quot;概率前往&quot;</span>]  <span class="hljs-comment"># 动作集合</span><br><span class="hljs-comment"># 状态转移函数</span><br>P = &#123;<br>    <span class="hljs-string">&quot;s1-保持s1-s1&quot;</span>: 1.0,<br>    <span class="hljs-string">&quot;s1-前往s2-s2&quot;</span>: 1.0,<br>    <span class="hljs-string">&quot;s2-前往s1-s1&quot;</span>: 1.0,<br>    <span class="hljs-string">&quot;s2-前往s3-s3&quot;</span>: 1.0,<br>    <span class="hljs-string">&quot;s3-前往s4-s4&quot;</span>: 1.0,<br>    <span class="hljs-string">&quot;s3-前往s5-s5&quot;</span>: 1.0,<br>    <span class="hljs-string">&quot;s4-前往s5-s5&quot;</span>: 1.0,<br>    <span class="hljs-string">&quot;s4-概率前往-s2&quot;</span>: 0.2,<br>    <span class="hljs-string">&quot;s4-概率前往-s3&quot;</span>: 0.4,<br>    <span class="hljs-string">&quot;s4-概率前往-s4&quot;</span>: 0.4,<br>&#125;<br><span class="hljs-comment"># 奖励函数</span><br>R = &#123;<br>    <span class="hljs-string">&quot;s1-保持s1&quot;</span>: -1,<br>    <span class="hljs-string">&quot;s1-前往s2&quot;</span>: 0,<br>    <span class="hljs-string">&quot;s2-前往s1&quot;</span>: -1,<br>    <span class="hljs-string">&quot;s2-前往s3&quot;</span>: -2,<br>    <span class="hljs-string">&quot;s3-前往s4&quot;</span>: -2,<br>    <span class="hljs-string">&quot;s3-前往s5&quot;</span>: 0,<br>    <span class="hljs-string">&quot;s4-前往s5&quot;</span>: 10,<br>    <span class="hljs-string">&quot;s4-概率前往&quot;</span>: 1,<br>&#125;<br>gamma = 0.5  <span class="hljs-comment"># 折扣因子</span><br>MDP = (S, A, P, R, gamma)<br><br><span class="hljs-comment"># 策略1,随机策略</span><br>Pi_1 = &#123;<br>    <span class="hljs-string">&quot;s1-保持s1&quot;</span>: 0.5,<br>    <span class="hljs-string">&quot;s1-前往s2&quot;</span>: 0.5,<br>    <span class="hljs-string">&quot;s2-前往s1&quot;</span>: 0.5,<br>    <span class="hljs-string">&quot;s2-前往s3&quot;</span>: 0.5,<br>    <span class="hljs-string">&quot;s3-前往s4&quot;</span>: 0.5,<br>    <span class="hljs-string">&quot;s3-前往s5&quot;</span>: 0.5,<br>    <span class="hljs-string">&quot;s4-前往s5&quot;</span>: 0.5,<br>    <span class="hljs-string">&quot;s4-概率前往&quot;</span>: 0.5,<br>&#125;<br><span class="hljs-comment"># 策略2</span><br>Pi_2 = &#123;<br>    <span class="hljs-string">&quot;s1-保持s1&quot;</span>: 0.6,<br>    <span class="hljs-string">&quot;s1-前往s2&quot;</span>: 0.4,<br>    <span class="hljs-string">&quot;s2-前往s1&quot;</span>: 0.3,<br>    <span class="hljs-string">&quot;s2-前往s3&quot;</span>: 0.7,<br>    <span class="hljs-string">&quot;s3-前往s4&quot;</span>: 0.5,<br>    <span class="hljs-string">&quot;s3-前往s5&quot;</span>: 0.5,<br>    <span class="hljs-string">&quot;s4-前往s5&quot;</span>: 0.1,<br>    <span class="hljs-string">&quot;s4-概率前往&quot;</span>: 0.9,<br>&#125;<br><br><br><span class="hljs-comment"># 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量</span><br>def join(str1, str2):<br>    return str1 + &#x27;-&#x27; + str2<br></code></pre></td></tr></table></figure><p>接下来我们想要计算该 MDP 下，一个策略$\pi$的状态价值函数。我们现在有的工具是 MRP 的解析解方法。于是，一个很自然的想法是：给定一个 MDP 和一个策略$\pi$，我们是否可以将其转化为一个 MRP？答案是肯定的。我们可以将策略的动作选择进行<strong>边缘化</strong>（marginalization)，就可以得到没有动作的 MRP 了。具体来说，对于某一个状态，我们根据策略所有动作的概率进行加权，得到的奖励和就可以认为是一个 MRP 在该状态下的奖励，即：<br>$r’(s)&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)r(s,a)$<br>同理，我们计算采取动作的概率与使s转移到的概率$s^{\prime}$的乘积，再将这些乘积相加，其和就是一个 MRP 的状态从s转移$s^{\prime}$至的概率：<br>$P’(s’|s)&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)P(s’|s,a)$<br>于是，我们构建得到了一个 MRP:$\langle\mathcal{S},P^{\prime},r^{\prime},\gamma\rangle$。根据价值函数的定义可以发现，转化前的 MDP 的状态价值函数和转化后的 MRP 的价值函数是一样的。于是我们可以用 MRP 中计算价值函数的解析解来计算这个 MDP 中该策略的状态价值函数。<br>我们接下来就编写代码来实现该方法，计算用随机策略（也就是代码中的<code>Pi_1</code>）时的状态价值函数。为了简单起见，我们直接给出转化后的 MRP 的状态转移矩阵和奖励函数，感兴趣的读者可以自行验证。</p><h3 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h3><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs prolog">gamma = <span class="hljs-number">0.5</span><br># 转化后的<span class="hljs-symbol">MRP</span>的状态转移矩阵<br><span class="hljs-symbol">P_from_mdp_to_mrp</span> = [<br>    [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],<br>    [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],<br>    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>],<br>    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.5</span>],<br>    [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>],<br>]<br><span class="hljs-symbol">P_from_mdp_to_mrp</span> = np.array(<span class="hljs-symbol">P_from_mdp_to_mrp</span>)<br><span class="hljs-symbol">R_from_mdp_to_mrp</span> = [<span class="hljs-number">-0.5</span>, <span class="hljs-number">-1.5</span>, <span class="hljs-number">-1.0</span>, <span class="hljs-number">5.5</span>, <span class="hljs-number">0</span>]<br><br><span class="hljs-symbol">V</span> = compute(<span class="hljs-symbol">P_from_mdp_to_mrp</span>, <span class="hljs-symbol">R_from_mdp_to_mrp</span>, gamma, <span class="hljs-number">5</span>)<br>print(<span class="hljs-string">&quot;MDP中每个状态价值分别为\n&quot;</span>, <span class="hljs-symbol">V</span>)<br></code></pre></td></tr></table></figure><p><img src="/image/RL-26.png"></p><p>知道了状态价值函数$V^{\pi}(S)$后，我们可以计算动作价值函数$Q^{\pi}(s,a)$。<br>这个 MRP 解析解的方法在状态动作集合比较大的时候不是很适用，那有没有其他的方法呢？下一节将介绍用蒙特卡洛方法来近似估计这个价值函数，用蒙特卡洛方法的好处在于我们不需要知道 MDP 的状态转移函数和奖励函数，它可以得到一个近似值，并且采样数越多越准确。</p><h1 id="5-蒙特卡洛方法"><a href="#5-蒙特卡洛方法" class="headerlink" title="5 蒙特卡洛方法"></a>5 蒙特卡洛方法</h1><p><strong>蒙特卡洛方法</strong>（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用蒙特卡洛方法时，我们通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。一个简单的例子是用蒙特卡洛方法来计算圆的面积。例如，在图 3-5 所示的正方形内部随机产生若干个点，细数落在圆中点的个数，圆的面积与正方形面积之比就等于圆中点的个数与正方形中点的个数之比。如果我们随机产生的点的个数越多，计算得到圆的面积就越接近于真实的圆的面积。<br><img src="/image/RL-27.png"><br>我们现在介绍如何用蒙特卡洛方法来估计一个策略在一个马尔可夫决策过程中的状态价值函数。回忆一下，一个状态的价值是它的期望回报，那么一个很直观的想法就是用策略在 MDP 上采样很多条序列，计算从这个状态出发的回报再求其期望就可以了，公式如下：<br>$V^\pi(s)&#x3D;\mathbb{E}<em>\pi[G_t|S_t&#x3D;s]\approx\frac{1}{N}\sum</em>{i&#x3D;1}^NG_i^{(i)}$<br>在一条序列中，可能没有出现过这个状态，可能只出现过一次这个状态，也可能出现过很多次这个状态。我们介绍的蒙特卡洛价值估计方法会在该状态每一次出现时计算它的回报。还有一种选择是一条序列只计算一次回报，也就是这条序列第一次出现该状态时计算后面的累积奖励，而后面再次出现该状态时，该状态就被忽略了。假设我们现在用策略$\pi$从状态s开始采样序列，据此来计算状态价值。我们为每一个状态维护一个计数器和总回报，计算状态价值的具体过程如下所示。<br>(1) 使用策略采样若干条序列：<br>$s_0^{(i)}\xrightarrow{a_0^{(i)}}r_0^{(i)},s_1^{(i)}\xrightarrow{a_1^{(i)}}r_1^{(i)},s_2^{(i)}\xrightarrow{a_2^{(i)}}\cdots\xrightarrow{a_{T-1}^{(i)}}r_{T-1}^{(i)},s_T^{(i)}$<br>(2) 对每一条序列中的每一时间步的状态进行以下操作：</p><ul><li>更新状态s的计数器$N(s)\leftarrow N(s)+1$</li><li>更新状态s的总回报$N(s)\leftarrow N(s)+G_t$<br>(3) 每一个状态的价值被估计为回报的平均值$V(s)&#x3D;M(s)&#x2F;N(s)$<br>根据大数定律，当$N(s)\to\infty$,有$V(s)\to V^\pi(s)$<br>计算回报的期望时，除了可以把所有的回报加起来除以次数，还有一种增量更新的方法。对于每个状态s和对应回报G，进行如下计算：<br>$$\begin{aligned}<br>&amp;N(s)\leftarrow N(s)+1 \<br>&amp;V(s)\leftarrow V(s)+\frac{1}{N(s)}(G-V(S))<br>\end{aligned}$$<br>接下来我们用代码定义一个采样函数。采样函数需要遵守状态转移矩阵和相应的策略，每次将<code>(s,a,r,s_next)</code>元组放入序列中，直到到达终止序列。然后我们通过该函数，用随机策略在下图的 MDP 中随机采样几条序列。</li></ul><h3 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sample</span>(<span class="hljs-params">MDP, Pi, timestep_max, number</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27; 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number &#x27;&#x27;&#x27;</span><br>    S, A, P, R, gamma = MDP<br>    episodes = []<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(number):<br>        episode = []<br>        timestep = <span class="hljs-number">0</span><br>        s = S[np.random.randint(<span class="hljs-number">4</span>)]  <span class="hljs-comment"># 随机选择一个除s5以外的状态s作为起点</span><br>        <span class="hljs-comment"># 当前状态为终止状态或者时间步太长时,一次采样结束</span><br>        <span class="hljs-keyword">while</span> s != <span class="hljs-string">&quot;s5&quot;</span> <span class="hljs-keyword">and</span> timestep &lt;= timestep_max:<br>            timestep += <span class="hljs-number">1</span><br>            rand, temp = np.random.rand(), <span class="hljs-number">0</span><br>            <span class="hljs-comment"># 在状态s下根据策略选择动作</span><br>            <span class="hljs-keyword">for</span> a_opt <span class="hljs-keyword">in</span> A:<br>                temp += Pi.get(join(s, a_opt), <span class="hljs-number">0</span>)<br>                <span class="hljs-keyword">if</span> temp &gt; rand:<br>                    a = a_opt<br>                    r = R.get(join(s, a), <span class="hljs-number">0</span>)<br>                    <span class="hljs-keyword">break</span><br>            rand, temp = np.random.rand(), <span class="hljs-number">0</span><br>            <span class="hljs-comment"># 根据状态转移概率得到下一个状态s_next</span><br>            <span class="hljs-keyword">for</span> s_opt <span class="hljs-keyword">in</span> S:<br>                temp += P.get(join(join(s, a), s_opt), <span class="hljs-number">0</span>)<br>                <span class="hljs-keyword">if</span> temp &gt; rand:<br>                    s_next = s_opt<br>                    <span class="hljs-keyword">break</span><br>            episode.append((s, a, r, s_next))  <span class="hljs-comment"># 把（s,a,r,s_next）元组放入序列中</span><br>            s = s_next  <span class="hljs-comment"># s_next变成当前状态,开始接下来的循环</span><br>        episodes.append(episode)<br>    <span class="hljs-keyword">return</span> episodes<br><br><br><span class="hljs-comment"># 采样5次,每个序列最长不超过20步</span><br>episodes = sample(MDP, Pi_1, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;第一条序列\n&#x27;</span>, episodes[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;第二条序列\n&#x27;</span>, episodes[<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;第五条序列\n&#x27;</span>, episodes[<span class="hljs-number">4</span>])<br><span class="hljs-comment"># 对所有采样序列计算所有状态的价值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">MC</span>(<span class="hljs-params">episodes, V, N, gamma</span>):<br>    <span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> episodes:<br>        G = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(episode) - <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):  <span class="hljs-comment">#一个序列从后往前计算</span><br>            (s, a, r, s_next) = episode[i]<br>            G = r + gamma * G<br>            N[s] = N[s] + <span class="hljs-number">1</span><br>            V[s] = V[s] + (G - V[s]) / N[s]<br><br><br>timestep_max = <span class="hljs-number">20</span><br><span class="hljs-comment"># 采样1000次,可以自行修改</span><br>episodes = sample(MDP, Pi_1, timestep_max, <span class="hljs-number">1000</span>)<br>gamma = <span class="hljs-number">0.5</span><br>V = &#123;<span class="hljs-string">&quot;s1&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;s2&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;s3&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;s4&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;s5&quot;</span>: <span class="hljs-number">0</span>&#125;<br>N = &#123;<span class="hljs-string">&quot;s1&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;s2&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;s3&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;s4&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;s5&quot;</span>: <span class="hljs-number">0</span>&#125;<br>MC(episodes, V, N, gamma)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;使用蒙特卡洛方法计算MDP的状态价值为\n&quot;</span>, V)<br></code></pre></td></tr></table></figure><p>可以看到用蒙特卡洛方法估计得到的状态价值和我们用 MRP 解析解得到的状态价值是很接近的。这得益于我们采样了比较多的序列，感兴趣的读者可以尝试修改采样次数，然后观察蒙特卡洛方法的结果。</p><h1 id="6-占用度量"><a href="#6-占用度量" class="headerlink" title="6 占用度量"></a>6 占用度量</h1><p>第四节提到，不同策略的价值函数是不一样的。这是因为对于同一个 MDP，不同策略会访问到的状态的概率分布是不同的。<br>我们需要理解不同策略会使智能体访问到不同概率分布的状态这个事实，这会影响到策略的价值函数。<br>首先我们定义 MDP 的初始状态分布为$\nu_0(s)$，在有些资料中，初始状态分布会被定义进 MDP 的组成元素中。我们用$P_t^{\pi}(s)$表示采取策略使得智能体在时刻状态为的概率，所以我们有$P_0^\pi(s)&#x3D;\nu_0(s)$，然后就可以定义一个策略的<strong>状态访问分布</strong>（state visitation distribution）：<br>$\nu^\pi(s)&#x3D;(1-\gamma)\sum_{t&#x3D;0}^\infty\gamma^tP_t^\pi(s)$<br>其中，$1-\gamma$是用来使得概率加和为 1 的归一化因子。状态访问概率表示一个策略和 MDP 交互会访问到的状态的分布。需要注意的是，理论上在计算该分布时需要交互到无穷步之后，但实际上智能体和 MDP 的交互在一个序列中是有限的。不过我们仍然可以用以上公式来表达状态访问概率的思想，状态访问概率有如下性质：<br>$\nu^\pi(s’)&#x3D;(1-\gamma)\nu_0(s’)+\gamma\int P(s’|s,a)\pi(a|s)\nu^\pi(s)dsda$<br>此外，我们还可以定义策略的<strong>占用度量</strong>（occupancy measure）：<br>$\rho^\pi(s,a)&#x3D;(1-\gamma)\sum_{t&#x3D;0}^\infty\gamma^tP_t^\pi(s)\pi(a|s)$<br>它表示动作状态(s,a)对被访问到的概率。二者之间存在如下关系：<br>$\rho^\pi(s,a)&#x3D;\nu^\pi(s)\pi(a|s)$<br>进一步得出如下两个定理。<strong>定理 1</strong>：智能体分别以策略$\pi_1$和$\pi_2$和同一个 MDP 交互得到的占用度量$\rho^{\pi_1}$和$\rho^{\pi_2}$满足<br>$\rho^{\pi_1}&#x3D;\rho^{\pi_2}\iff\pi_1&#x3D;\pi_2$<br><strong>定理 2</strong>：给定一合法占用度量$\rho$，可生成该占用度量的唯一策略是<br>$\pi_\rho&#x3D;\frac{\rho(s,a)}{\sum_{a’}\rho(s,a’)}$<br>注意：以上提到的“合法”占用度量是指存在一个策略使智能体与 MDP 交互产生的状态动作对被访问到的概率。</p><p>接下来我们编写代码来近似估计占用度量。这里我们采用近似估计，即设置一个较大的采样轨迹长度的最大值，然后采样很多次，用状态动作对出现的频率估计实际概率。</p><h3 id="代码-5"><a href="#代码-5" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">occupancy</span>(<span class="hljs-params">episodes, s, a, timestep_max, gamma</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27; 计算状态动作对（s,a）出现的频率,以此来估算策略的占用度量 &#x27;&#x27;&#x27;</span><br>    rho = <span class="hljs-number">0</span><br>    total_times = np.zeros(timestep_max)  <span class="hljs-comment"># 记录每个时间步t各被经历过几次</span><br>    occur_times = np.zeros(timestep_max)  <span class="hljs-comment"># 记录(s_t,a_t)=(s,a)的次数</span><br>    <span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> episodes:<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(episode)):<br>            (s_opt, a_opt, r, s_next) = episode[i]<br>            total_times[i] += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> s == s_opt <span class="hljs-keyword">and</span> a == a_opt:<br>                occur_times[i] += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(timestep_max)):<br>        <span class="hljs-keyword">if</span> total_times[i]:<br>            rho += gamma**i * occur_times[i] / total_times[i]<br>    <span class="hljs-keyword">return</span> (<span class="hljs-number">1</span> - gamma) * rho<br><br><br>gamma = <span class="hljs-number">0.5</span><br>timestep_max = <span class="hljs-number">1000</span><br><br>episodes_1 = sample(MDP, Pi_1, timestep_max, <span class="hljs-number">1000</span>)<br>episodes_2 = sample(MDP, Pi_2, timestep_max, <span class="hljs-number">1000</span>)<br>rho_1 = occupancy(episodes_1, <span class="hljs-string">&quot;s4&quot;</span>, <span class="hljs-string">&quot;概率前往&quot;</span>, timestep_max, gamma)<br>rho_2 = occupancy(episodes_2, <span class="hljs-string">&quot;s4&quot;</span>, <span class="hljs-string">&quot;概率前往&quot;</span>, timestep_max, gamma)<br><span class="hljs-built_in">print</span>(rho_1, rho_2)<br></code></pre></td></tr></table></figure><h1 id="7-最优策略"><a href="#7-最优策略" class="headerlink" title="7 最优策略"></a>7 最优策略</h1><p>强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。我们首先定义策略之间的偏序关系：当且仅当对于任意的状态s都有$V^\pi(s)\geq V^{\pi^{\prime}}(s)$,记$\pi&gt;\pi^{\prime}$，于是在有限状态和动作集合的 MDP 中，至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是<strong>最优策略</strong>（optimal policy）。最优策略可能有很多个，我们都将其表示为$\pi^{<em>}(s)$。<br>最优策略都有相同的状态价值函数，我们称之为<strong>最优状态价值函数</strong>，表示为：<br>$V^</em>(s)&#x3D;\max_\pi V^\pi(s),\quad\forall s\in\mathcal{S}$<br>同理，我们定义<strong>最优动作价值函数</strong>:<br>$Q^*(s,a)&#x3D;\max_\pi Q^\pi(s,a),\quad\forall s\in\mathcal{S},a\in\mathcal{A}$<br>为了使$Q^\pi(s,a)$最大，我们需要在当前的状态动作对(s,a)之后都执行最优策略。于是我们得到了最优状态价值函数和最优动作价值函数之间的关系：<br>$Q^*(s,a)&#x3D;r(s,a)+\gamma\sum_{s^\prime\in S}P(s^\prime|s,a)V^*(s^\prime)$<br>这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值：<br>$V^*(s)&#x3D;\max_{a\in\mathcal{A}}Q^*(s,a)$</p><h2 id="7-1贝尔曼最优方程"><a href="#7-1贝尔曼最优方程" class="headerlink" title="7.1贝尔曼最优方程"></a>7.1贝尔曼最优方程</h2><p>$$V^*(s)&#x3D;\max_{a\in\mathcal{A}}{r(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)V^*(s^{\prime})}\Q^*(s,a)&#x3D;r(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)\max_{a^{\prime}\in\mathcal{A}}Q^*(s^{\prime},a^{\prime})$$</p><h1 id="8总结"><a href="#8总结" class="headerlink" title="8总结"></a>8总结</h1><p>本章从零开始介绍了马尔可夫决策过程的基础概念知识，并讲解了如何通过求解贝尔曼方程得到状态价值的解析解以及如何用蒙特卡洛方法估计各个状态的价值。马尔可夫决策过程是强化学习中的基础概念，强化学习中的环境就是一个马尔可夫决策过程。我们接下来将要介绍的强化学习算法通常都是在求解马尔可夫决策过程中的最优策略。</p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>仿真</tag>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL-1 多臂老虎机</title>
    <link href="/2024/08/16/RL-1/"/>
    <url>/2024/08/16/RL-1/</url>
    
    <content type="html"><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p>我们在第 1 章中了解到，强化学习关注智能体和环境交互过程中的学习，这是一种<strong>试错型学习</strong>（trial-and-error learning）范式。在正式学习强化学习之前，我们需要先了解多臂老虎机问题，它可以被看作简化版的强化学习问题。与强化学习不同，多臂老虎机不存在状态信息，只有动作和奖励，算是最简单的“和环境交互中的学习”的一种形式。多臂老虎机中的<strong>探索与利用</strong>（exploration vs. exploitation）问题一直以来都是一个特别经典的问题，理解它能够帮助我们学习强化学习。</p><h1 id="2-问题介绍"><a href="#2-问题介绍" class="headerlink" title="2 问题介绍"></a>2 问题介绍</h1><h2 id="2-1定义"><a href="#2-1定义" class="headerlink" title="2.1定义"></a>2.1定义</h2><p>在多臂老虎机（multi-armed bandit，MAB）问题（见图 2-1）中，有一个拥有  根拉杆的老虎机，拉动每一根拉杆都对应一个关于奖励的概率分布 。我们每次拉动其中一根拉杆，就可以从该拉杆对应的奖励概率分布中获得一个奖励 。我们在各根拉杆的奖励概率分布未知的情况下，从头开始尝试，目标是在操作  次拉杆后获得尽可能高的累积奖励。由于奖励的概率分布是未知的，因此我们需要在“探索拉杆的获奖概率”和“根据经验选择获奖最多的拉杆”中进行权衡。“采用怎样的操作策略才能使获得的累积奖励最高”便是多臂老虎机问题。如果是你，会怎么做呢？<br><img src="/image/RL-11.png"></p><h2 id="2-2形式化描述"><a href="#2-2形式化描述" class="headerlink" title="2.2形式化描述"></a>2.2形式化描述</h2><p>躲避老虎机可以表示为一个元组$&lt;A,R&gt;$,其中：</p><ul><li>A为动作集合，其中一个动作表示拉动一个拉杆。若多臂老虎机一共有K根拉杆，那动作空间就是集合${a_1,\dots,a_k}$，我们用$a_t\in\mathcal{A}$表示任意一个动作；</li><li>R为奖励概率分布，拉动每一根拉杆的动作a都对应一个奖励概率分布$\mathcal{R}(r|a)$，不同拉杆的奖励分布通常是不同的。<br>假设每个时间步只能拉动一个拉杆，多臂老虎机的目标为最大化一段时间步T内累积的奖励:<br>$\max\sum_{t&#x3D;1}^{T}r_{t}, r_{t}\sim\mathcal{R}\left(\cdot|a_{t}\right)$<br>其中$a_t$表示在第时间步拉动某一拉杆的动作，$r_t$表示动作$a_t$获得的奖励。</li></ul><h2 id="2-3累积懊悔"><a href="#2-3累积懊悔" class="headerlink" title="2.3累积懊悔"></a>2.3累积懊悔</h2><p>对于每一个动作a，我们定义其期望奖励为$Q(a)&#x3D;\mathbb{E}<em>{r\sim\mathcal{R}(\cdot|a)} [r]$<br>于是，至少存在一根拉杆，它的期望奖励不小于拉动其他任意一根拉杆，我们将该最优期望奖励表示为$Q^*&#x3D;\max</em>{a\in\mathcal{A}}Q(a)$<br>为了更加直观、方便地观察任意一根拉杆的期望奖励离最优拉杆期望奖励的差距，我们引入<strong>懊悔</strong>（regret）概念。懊悔定义为拉动当前拉杆的动作a与最优拉杆的期望奖励差，即$R(a)&#x3D;Q^*-Q(a)$<br>累积懊悔即操作T次拉杆后累积的懊悔总量，对于一次完整的T步决策${a_1,a_2,\dots,a_T}$,积累懊悔为$\sigma_R&#x3D;\sum_{t&#x3D;1}^TR(a_t)$<br>MAB 问题的目标为最大化累积奖励，等价于最小化累积懊悔。</p><h2 id="2-4估计期望奖励"><a href="#2-4估计期望奖励" class="headerlink" title="2.4估计期望奖励"></a>2.4估计期望奖励</h2><p>为了知道拉动哪一根拉杆能获得更高的奖励，我们需要估计拉动这根拉杆的期望奖励。由于只拉动一次拉杆获得的奖励存在随机性，所以需要多次拉动一根拉杆，然后计算得到的多次奖励的期望，其算法流程如下所示。</p><ul><li>对于$\forall a\in\mathcal{A}$，初始化计数器$N(a)&#x3D;0$和期望奖励估值$\hat{Q}(a)&#x3D;0$</li><li><strong>for</strong> t &#x3D; 1-&gt;T do<ul><li>选取某根拉杆，该动作记为$a_t$</li><li>得到奖励$r_t$ </li><li>更新计数器$N(a_t)&#x3D;N(a_t)+1$: </li><li>更新期望奖励估值：$\hat{Q}(a_t)&#x3D;\hat{Q}(a_t)+\frac{1}{N(a_t)}\Big[r_t-\hat{Q}(a_t)\Big]$</li></ul></li><li><strong>end for</strong><br>以上 for 循环中的第四步如此更新估值，是因为这样可以进行增量式的期望更新，公式如下。<br>$$\begin{aligned}<br>Q_{k}&amp; &#x3D;\frac1k\sum_{i&#x3D;1}^kr_i \<br>&amp;&#x3D;\frac{1}{k}\left(r_k+\sum_{i&#x3D;1}^{k-1}r_i\right) \<br>&amp;&#x3D;\frac1k(r_k+(k-1)Q_{k-1}) \<br>&amp;&#x3D;\frac1k(r_k+kQ_{k-1}-Q_{k-1}) \<br>&amp;&#x3D;Q_{k-1}+\frac1k[r_k-Q_{k-1}]<br>\end{aligned}$$<br>式中，$Q_{k-1}$为k-1个奖励的期望值。<br>以下用python部署一个简单的多臂老虎机。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><br><span class="hljs-comment"># -*- encoding: utf-8 -*-</span><br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string"></span><br><span class="hljs-string">@File    :   多臂老虎机.ipynb</span><br><span class="hljs-string"></span><br><span class="hljs-string">@Time    :   2024/08/16 21:30:45</span><br><span class="hljs-string"></span><br><span class="hljs-string">@Author  :   Neutrin</span><br><span class="hljs-string"></span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># here put the import lib</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BernoulliBandit</span>:<br><br>    <span class="hljs-string">&quot;&quot;&quot; 伯努利多臂老虎机,输入K表示拉杆个数 &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, K</span>):<br><br>        <span class="hljs-variable language_">self</span>.probs = np.random.uniform(size=K)  <span class="hljs-comment"># 随机生成K个0～1的数,作为拉动每根拉杆的获奖</span><br>        <span class="hljs-comment"># 概率</span><br>        <span class="hljs-variable language_">self</span>.best_idx = np.argmax(<span class="hljs-variable language_">self</span>.probs)  <span class="hljs-comment"># 获奖概率最大的拉杆</span><br>        <span class="hljs-variable language_">self</span>.best_prob = <span class="hljs-variable language_">self</span>.probs[<span class="hljs-variable language_">self</span>.best_idx]  <span class="hljs-comment"># 最大的获奖概率</span><br>        <span class="hljs-variable language_">self</span>.K = K<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self, k</span>):<br><br>        <span class="hljs-comment"># 当玩家选择了k号拉杆后,根据拉动该老虎机的k号拉杆获得奖励的概率返回1（获奖）或0（未</span><br>        <span class="hljs-comment"># 获奖）</span><br>        <span class="hljs-keyword">if</span> np.random.rand() &lt; <span class="hljs-variable language_">self</span>.probs[k]:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br><br>np.random.seed(<span class="hljs-number">1</span>)  <span class="hljs-comment"># 设定随机种子,使实验具有可重复性</span><br>K = <span class="hljs-number">10</span><br>bandit_10_arm = BernoulliBandit(K)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;随机生成了一个%d臂伯努利老虎机&quot;</span> % K)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;获奖概率最大的拉杆为%d号,其获奖概率为%.4f&quot;</span> %<br>      (bandit_10_arm.best_idx, bandit_10_arm.best_prob))<br></code></pre></td></tr></table></figure>接下来我们用一个 Solver 基础类来实现上述的多臂老虎机的求解方案。根据前文的算法流程，我们需要实现下列函数功能：根据策略选择动作、根据动作获取奖励、更新期望奖励估值、更新累积懊悔和计数。在下面的 MAB 算法基本框架中，我们将<strong>根据策略选择动作</strong>、<strong>根据动作获取奖励</strong>和<strong>更新期望奖励估值</strong>放在 <code>run_one_step()</code> 函数中，由每个继承 Solver 类的策略具体实现。而<strong>更新累积懊悔和计数</strong>则直接放在主循环 <code>run()</code> 中。</li></ul><h1 id="3-探索与利用的平衡"><a href="#3-探索与利用的平衡" class="headerlink" title="3 探索与利用的平衡"></a>3 探索与利用的平衡</h1><p>在第2节的算法框架中，还没有一个策略告诉我们应该采取哪个动作，即拉动哪根拉杆，所以接下来我们将学习如何设计一个策略。例如，一个最简单的策略就是一直采取第一个动作，但这就非常依赖运气的好坏。如果运气绝佳，可能拉动的刚好是能获得最大期望奖励的拉杆，即最优拉杆；但如果运气很糟糕，获得的就有可能是最小的期望奖励。在多臂老虎机问题中，一个经典的问题就是探索与利用的平衡问题。<strong>探索</strong>（exploration）是指尝试拉动更多可能的拉杆，这根拉杆不一定会获得最大的奖励，但这种方案能够摸清楚所有拉杆的获奖情况。例如，对于一个 10 臂老虎机，我们要把所有的拉杆都拉动一下才知道哪根拉杆可能获得最大的奖励。<strong>利用</strong>（exploitation）是指拉动已知期望奖励最大的那根拉杆，由于已知的信息仅仅来自有限次的交互观测，所以当前的最优拉杆不一定是全局最优的。例如，对于一个 10 臂老虎机，我们只拉动过其中 3 根拉杆，接下来就一直拉动这 3 根拉杆中期望奖励最大的那根拉杆，但很有可能期望奖励最大的拉杆在剩下的 7 根当中，即使我们对 10 根拉杆各自都尝试了 20 次，发现 5 号拉杆的经验期望奖励是最高的，但仍然存在着微小的概率—另一根 6 号拉杆的真实期望奖励是比 5 号拉杆更高的。<br>于是在多臂老虎机问题中，设计策略时就需要平衡探索和利用的次数，使得累积奖励最大化。一个比较常用的思路是在开始时做比较多的探索，在对每根拉杆都有比较准确的估计后，再进行利用。目前已有一些比较经典的算法来解决这个问题，例如$\epsilon-$贪婪算法、上置信界算法和汤普森采样算法等，我们接下来将分别介绍这几种算法。</p><h1 id="4-epsilon-贪婪算法"><a href="#4-epsilon-贪婪算法" class="headerlink" title="4 $\epsilon-$贪婪算法"></a>4 $\epsilon-$贪婪算法</h1><p>完全贪婪算法即在每一时刻采取期望奖励估值最大的动作（拉动拉杆），这就是纯粹的利用，而没有探索，所以我们通常需要对完全贪婪算法进行一些修改，其中比较经典的一种方法为$\epsilon-$贪婪算法。$\epsilon-$贪婪在完全贪婪算法的基础上加上了噪声，每次以概率$1-\epsilon$选择以往经验中期望奖励估值最大的那根杆(利用),以概率$\epsilon$随机选择一根拉杆(探索)，公式如下：<br>$a_t&#x3D;\begin{cases}\arg\max_{a\in A}\hat{Q}(a),&amp;\text{采样概率:1-}\epsilon\\text{从 }\mathcal{A}\text{中随机选择},&amp;\text{采样概率:}\epsilon\end{cases}$<br>随着探索次数的不断增加，我们对各个动作的奖励估计得越来越准，此时我们就没必要继续花大力气进行探索。 所以$\epsilon-$贪婪算法在具体的实现中，我们可以令$\epsilon$随时间衰减，即探索的概率将会不断降低，但是不会在有限的步数内衰减至0，因为基于有限步数观测的完全贪婪算法仍然是一个局部信息的贪婪算法，永远距离最优解有一个固定的差距。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">EpsilonGreedy</span>(<span class="hljs-title class_ inherited__">Solver</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; epsilon贪婪算法,继承Solver类 &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, bandit, epsilon=<span class="hljs-number">0.01</span>, init_prob=<span class="hljs-number">1.0</span></span>):<br>        <span class="hljs-built_in">super</span>(EpsilonGreedy, <span class="hljs-variable language_">self</span>).__init__(bandit)<br>        <span class="hljs-variable language_">self</span>.epsilon = epsilon<br>        <span class="hljs-comment">#初始化拉动所有拉杆的期望奖励估值</span><br>        <span class="hljs-variable language_">self</span>.estimates = np.array([init_prob] * <span class="hljs-variable language_">self</span>.bandit.K)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run_one_step</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">if</span> np.random.random() &lt; <span class="hljs-variable language_">self</span>.epsilon:<br>            k = np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-variable language_">self</span>.bandit.K)  <span class="hljs-comment"># 随机选择一根拉杆</span><br>        <span class="hljs-keyword">else</span>:<br>            k = np.argmax(<span class="hljs-variable language_">self</span>.estimates)  <span class="hljs-comment"># 选择期望奖励估值最大的拉杆</span><br>        r = <span class="hljs-variable language_">self</span>.bandit.step(k)  <span class="hljs-comment"># 得到本次动作的奖励</span><br>        <span class="hljs-variable language_">self</span>.estimates[k] += <span class="hljs-number">1.</span> / (<span class="hljs-variable language_">self</span>.counts[k] + <span class="hljs-number">1</span>) * (r - <span class="hljs-variable language_">self</span>.estimates[k])<br>        <span class="hljs-keyword">return</span> k<br>        <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_results</span>(<span class="hljs-params">solvers, solver_names</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;生成累积懊悔随时间变化的图像。输入solvers是一个列表,列表中的每个元素是一种特定的策略。</span><br><span class="hljs-string">    而solver_names也是一个列表,存储每个策略的名称&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">for</span> idx, solver <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(solvers):<br>        time_list = <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(solver.regrets))<br>        plt.plot(time_list, solver.regrets, label=solver_names[idx])<br>    plt.xlabel(<span class="hljs-string">&#x27;Time steps&#x27;</span>)<br>    plt.ylabel(<span class="hljs-string">&#x27;Cumulative regrets&#x27;</span>)<br>    plt.title(<span class="hljs-string">&#x27;%d-armed bandit&#x27;</span> % solvers[<span class="hljs-number">0</span>].bandit.K)<br>    plt.legend()<br>    plt.show()<br><br>np.random.seed(<span class="hljs-number">1</span>)<br>epsilon_greedy_solver = EpsilonGreedy(bandit_10_arm, epsilon=<span class="hljs-number">0.01</span>)<br>epsilon_greedy_solver.run(<span class="hljs-number">5000</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;epsilon-贪婪算法的累积懊悔为：&#x27;</span>, epsilon_greedy_solver.regret)<br>plot_results([epsilon_greedy_solver], [<span class="hljs-string">&quot;EpsilonGreedy&quot;</span>])<br></code></pre></td></tr></table></figure><p>结果<br><img src="/image/RL-12.png"><br>在经历一段时间后，$\epsilon-$贪婪算法的累积懊悔几乎是线性增长的，接下来使用不同的参数值进行部署，</p><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">np</span>.random.seed(<span class="hljs-number">0</span>)<br><span class="hljs-attribute">epsilons</span> =<span class="hljs-meta"> [1e-4, 0.01, 0.1, 0.25, 0.5]</span><br><span class="hljs-attribute">epsilon_greedy_solver_list</span> =<span class="hljs-meta"> [</span><br><span class="hljs-meta">    EpsilonGreedy(bandit_10_arm, epsilon=e) for e in epsilons</span><br><span class="hljs-meta">]</span><br><span class="hljs-attribute">epsilon_greedy_solver_names</span> =<span class="hljs-meta"> [&quot;epsilon=&#123;&#125;&quot;.format(e) for e in epsilons]</span><br><span class="hljs-attribute">for</span> solver in epsilon_greedy_solver_list:<br>    <span class="hljs-attribute">solver</span>.run(<span class="hljs-number">5000</span>)<br><br><span class="hljs-attribute">plot_results</span>(epsilon_greedy_solver_list, epsilon_greedy_solver_names)<br></code></pre></td></tr></table></figure><p><img src="/image/RL-13.png"><br>无论取值多少，累积懊悔都是线性增长的，接下来部署随时间衰减的$\epsilon-$贪婪算法，采取具体的形式是反比例，<br>$\epsilon_t&#x3D;\frac{1}{t}$</p><h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecayingEpsilonGreedy</span>(<span class="hljs-title class_ inherited__">Solver</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; epsilon值随时间衰减的epsilon-贪婪算法,继承Solver类 &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, bandit, init_prob=<span class="hljs-number">1.0</span></span>):<br>        <span class="hljs-built_in">super</span>(DecayingEpsilonGreedy, <span class="hljs-variable language_">self</span>).__init__(bandit)<br>        <span class="hljs-variable language_">self</span>.estimates = np.array([init_prob] * <span class="hljs-variable language_">self</span>.bandit.K)<br>        <span class="hljs-variable language_">self</span>.total_count = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run_one_step</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.total_count += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> np.random.random() &lt; <span class="hljs-number">1</span> / <span class="hljs-variable language_">self</span>.total_count:  <span class="hljs-comment"># epsilon值随时间衰减</span><br>            k = np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-variable language_">self</span>.bandit.K)<br>        <span class="hljs-keyword">else</span>:<br>            k = np.argmax(<span class="hljs-variable language_">self</span>.estimates)<br><br>        r = <span class="hljs-variable language_">self</span>.bandit.step(k)<br>        <span class="hljs-variable language_">self</span>.estimates[k] += <span class="hljs-number">1.</span> / (<span class="hljs-variable language_">self</span>.counts[k] + <span class="hljs-number">1</span>) * (r - <span class="hljs-variable language_">self</span>.estimates[k])<br><br>        <span class="hljs-keyword">return</span> k<br><br><br>np.random.seed(<span class="hljs-number">1</span>)<br>decaying_epsilon_greedy_solver = DecayingEpsilonGreedy(bandit_10_arm)<br>decaying_epsilon_greedy_solver.run(<span class="hljs-number">5000</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;epsilon值衰减的贪婪算法的累积懊悔为：&#x27;</span>, decaying_epsilon_greedy_solver.regret)<br>plot_results([decaying_epsilon_greedy_solver], [<span class="hljs-string">&quot;DecayingEpsilonGreedy&quot;</span>])<br></code></pre></td></tr></table></figure><p><img src="/image/RL-14.png"><br>随时间做反比例衰减的$\epsilon-$贪婪算法明显优于固定值的$\epsilon-$贪婪算法</p><h1 id="5-上置信界算法"><a href="#5-上置信界算法" class="headerlink" title="5 上置信界算法"></a>5 上置信界算法</h1><p>设想这样一种情况：对于一台双臂老虎机，其中第一根拉杆只被拉动过一次，得到的奖励为0；第二根拉杆被拉动过很多次，我们对它的奖励分布已经有了大致的把握。这时你会怎么做？或许你会进一步尝试拉动第一根拉杆，从而更加确定其奖励分布。这种思路主要是基于不确定性，因为此时第一根拉杆只被拉动过一次，它的不确定性很高。一根拉杆的不确定性越大，它就越具有探索的价值，因为探索之后我们可能发现它的期望奖励很大。我们在此引入不确定性度量U(a)，它会随着一个动作被尝试次数的增加而减小。我们可以使用一种基于不确定性的策略来综合考虑现有的期望奖励估值和不确定性，其核心问题是如何估计不确定性。<br><strong>上置信界</strong>（upper confidence bound，UCB）算法是一种经典的基于不确定性的策略算法，它的思想用到了一个非常著名的数学原理：<strong>霍夫丁不等式</strong>（Hoeffding’s inequality）。在霍夫丁不等式中，令$X_1,\ldots,X_n$为n个独立同分布的随机变量，取值范围为[0，1]其经验期望为$\bar{x}<em>{n}&#x3D;\frac{1}{n}\sum</em>{j&#x3D;1}^{n}X_{j}$，则有：<br>$\mathbb{P}\left{\mathbb{E}\left[X\right]\geq\bar{x}_n+u\right}\leq e^{-2nu^2}$<br>现在我们将霍夫丁不等式运用于多臂老虎机问题中。<br>过于复杂，原理不做赘述，感兴趣的请自行搜索。</p><h3 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">UCB</span>(<span class="hljs-title class_ inherited__">Solver</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; UCB算法,继承Solver类 &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, bandit, coef, init_prob=<span class="hljs-number">1.0</span></span>):<br>        <span class="hljs-built_in">super</span>(UCB, <span class="hljs-variable language_">self</span>).__init__(bandit)<br>        <span class="hljs-variable language_">self</span>.total_count = <span class="hljs-number">0</span><br>        <span class="hljs-variable language_">self</span>.estimates = np.array([init_prob] * <span class="hljs-variable language_">self</span>.bandit.K)<br>        <span class="hljs-variable language_">self</span>.coef = coef<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run_one_step</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.total_count += <span class="hljs-number">1</span><br>        ucb = <span class="hljs-variable language_">self</span>.estimates + <span class="hljs-variable language_">self</span>.coef * np.sqrt(<br>            np.log(<span class="hljs-variable language_">self</span>.total_count) / (<span class="hljs-number">2</span> * (<span class="hljs-variable language_">self</span>.counts + <span class="hljs-number">1</span>)))  <span class="hljs-comment"># 计算上置信界</span><br>        k = np.argmax(ucb)  <span class="hljs-comment"># 选出上置信界最大的拉杆</span><br>        r = <span class="hljs-variable language_">self</span>.bandit.step(k)<br>        <span class="hljs-variable language_">self</span>.estimates[k] += <span class="hljs-number">1.</span> / (<span class="hljs-variable language_">self</span>.counts[k] + <span class="hljs-number">1</span>) * (r - <span class="hljs-variable language_">self</span>.estimates[k])<br>        <span class="hljs-keyword">return</span> k<br><br><br>np.random.seed(<span class="hljs-number">1</span>)<br>coef = <span class="hljs-number">1</span>  <span class="hljs-comment"># 控制不确定性比重的系数</span><br>UCB_solver = UCB(bandit_10_arm, coef)<br>UCB_solver.run(<span class="hljs-number">5000</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;上置信界算法的累积懊悔为：&#x27;</span>, UCB_solver.regret)<br>plot_results([UCB_solver], [<span class="hljs-string">&quot;UCB&quot;</span>])<br></code></pre></td></tr></table></figure><p><img src="/image/RL-15.png"></p><h1 id="6-汤普森采样法"><a href="#6-汤普森采样法" class="headerlink" title="6 汤普森采样法"></a>6 汤普森采样法</h1><p>MAB 中还有一种经典算法——<strong>汤普森采样</strong>（Thompson sampling），先假设拉动每根拉杆的奖励服从一个特定的概率分布，然后根据拉动每根拉杆的期望奖励来进行选择。但是由于计算所有拉杆的期望奖励的代价比较高，汤普森采样算法使用采样的方式，即根据当前每个动作a的奖励概率分布进行一轮采样，得到一组各根拉杆的奖励样本，再选择样本中奖励最大的动作。可以看出，汤普森采样是一种计算所有拉杆的最高奖励概率的蒙特卡洛采样方法。<br>了解了汤普森采样算法的基本思路后，我们需要解决另一个问题：怎样得到当前每个动作a的奖励概率分布并且在过程中进行更新？在实际情况中，我们通常用 Beta 分布对当前每个动作的奖励概率分布进行建模。具体来说，若某拉杆被选择了k次，其中$m_1$次奖励为1， $m_2$次奖励为0，则该拉杆的奖励服从参数为 的 Beta分布。下图是汤普森采样的一个示例。<br><img src="/image/RL-16.png"></p><h3 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ThompsonSampling</span>(<span class="hljs-title class_ inherited__">Solver</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; 汤普森采样算法,继承Solver类 &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, bandit</span>):<br>        <span class="hljs-built_in">super</span>(ThompsonSampling, <span class="hljs-variable language_">self</span>).__init__(bandit)<br>        <span class="hljs-variable language_">self</span>._a = np.ones(<span class="hljs-variable language_">self</span>.bandit.K)  <span class="hljs-comment"># 列表,表示每根拉杆奖励为1的次数</span><br>        <span class="hljs-variable language_">self</span>._b = np.ones(<span class="hljs-variable language_">self</span>.bandit.K)  <span class="hljs-comment"># 列表,表示每根拉杆奖励为0的次数</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run_one_step</span>(<span class="hljs-params">self</span>):<br>        samples = np.random.beta(<span class="hljs-variable language_">self</span>._a, <span class="hljs-variable language_">self</span>._b)  <span class="hljs-comment"># 按照Beta分布采样一组奖励样本</span><br>        k = np.argmax(samples)  <span class="hljs-comment"># 选出采样奖励最大的拉杆</span><br>        r = <span class="hljs-variable language_">self</span>.bandit.step(k)<br><br>        <span class="hljs-variable language_">self</span>._a[k] += r  <span class="hljs-comment"># 更新Beta分布的第一个参数</span><br>        <span class="hljs-variable language_">self</span>._b[k] += (<span class="hljs-number">1</span> - r)  <span class="hljs-comment"># 更新Beta分布的第二个参数</span><br>        <span class="hljs-keyword">return</span> k<br><br><br>np.random.seed(<span class="hljs-number">1</span>)<br>thompson_sampling_solver = ThompsonSampling(bandit_10_arm)<br>thompson_sampling_solver.run(<span class="hljs-number">5000</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;汤普森采样算法的累积懊悔为：&#x27;</span>, thompson_sampling_solver.regret)<br>plot_results([thompson_sampling_solver], [<span class="hljs-string">&quot;ThompsonSampling&quot;</span>])<br></code></pre></td></tr></table></figure><p><img src="/image/RL-17.png"><br>$\epsilon-$贪婪算法是线性增长的，其余都是次线性增长。</p><h1 id="7-总结与心得"><a href="#7-总结与心得" class="headerlink" title="7 总结与心得"></a>7 总结与心得</h1><p>本文着重与探索与利用两个环节，与环境进行交互，获得累积奖励与懊悔，这是强化学习是错法中的必备技术，多臂老虎机问题正是研究探索与利用的最佳环境。<br>$\epsilon-$贪婪算法等几种方法在多臂老虎机中非常常见，上置信界算法和汤普森采样方法均能保证对数的渐进最优累积懊悔。<br>多臂老虎机问题与强化学习的一大区别在于其与环境的交互并不会改变环境，即多臂老虎机的每次交互的结果和以往的动作无关，所以可看作<strong>无状态的强化学习</strong>（stateless reinforcement learning）。</p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>仿真</tag>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL-0 强化学习概述</title>
    <link href="/2024/08/16/RL-0/"/>
    <url>/2024/08/16/RL-0/</url>
    
    <content type="html"><![CDATA[<h1 id="1-什么是强化学习？"><a href="#1-什么是强化学习？" class="headerlink" title="1 什么是强化学习？"></a>1 什么是强化学习？</h1><p>广泛地讲，强化学习是机器通过与环境交互来实现目标的一种计算方法。机器和环境的一轮交互是指，机器在环境的一个状态下做一个动作决策，把这个动作作用到环境当中，这个环境发生相应的改变并且将相应的奖励反馈和下一轮状态传回机器。这种交互是迭代进行的，机器的目标是最大化在多轮交互过程中获得的累积奖励的期望。强化学习用智能体（agent）这个概念来表示做决策的机器。相比于有监督学习中的“模型”，强化学习中的“智能体”强调机器不但可以感知周围的环境信息，还可以通过做决策来直接改变这个环境，而不只是给出一些预测信号。<br>智能体和环境之间具体的交互方式如下图所示。在每一轮交互中，智能体感知到环境目前所处的状态，经过自身的计算给出本轮的动作，将其作用到环境中；环境得到智能体的动作后，产生相应的即时奖励信号并发生相应的状态转移。智能体则在下一轮交互中感知到新的环境状态，依次类推。</p><p><img src="/image/RL-01.png"></p><p>这里，智能体有3种关键要素，即感知、决策和奖励。</p><ul><li>感知。智能体在某种程度上感知环境的状态，从而知道自己所处的现状。例如，下围棋的智能体感知当前的棋盘情况；无人车感知周围道路的车辆、行人和红绿灯等情况；机器狗通过摄像头感知面前的图像，通过脚底的力学传感器来感知地面的摩擦功率和倾斜度等情况。</li><li>智能体根据当前的状态计算出达到目标需要采取的动作的过程叫作决策。例如，针对当前的棋盘决定下一颗落子的位置；针对当前的路况，无人车计算出方向盘的角度和刹车、油门的力度；针对当前收集到的视觉和力觉信号，机器狗给出4条腿的齿轮的角速度。策略是智能体最终体现出的智能形式，是不同智能体之间的核心区别。</li><li>奖励。环境根据状态和智能体采取的动作，产生一个标量信号作为奖励反馈。这个标量信号衡量智能体这一轮动作的好坏。例如，围棋博弈是否胜利；无人车是否安全、平稳且快速地行驶；机器狗是否在前进而没有摔倒。最大化累积奖励期望是智能体提升策略的目标，也是衡量智能体策略好坏的关键指标。<br>从以上分析可以看出，面向决策任务的强化学习和面向预测任务的有监督学习在形式上是有不少区别的。首先，决策任务往往涉及多轮交互，即序贯决策；而预测任务总是单轮的独立任务。如果决策也是单轮的，那么它可以转化为“判别最优动作”的预测任务。其次，因为决策任务是多轮的，智能体就需要在每轮做决策时考虑未来环境相应的改变，所以当前轮带来最大奖励反馈的动作，在长期来看并不一定是最优的。</li></ul><h1 id="2-强化学习的环境"><a href="#2-强化学习的环境" class="headerlink" title="2 强化学习的环境"></a>2 强化学习的环境</h1><p>强化学习的智能体是在和一个动态环境的交互中完成序贯决策的。我们说一个环境是动态的，意思就是它会随着某些因素的变化而不断演变，这在数学和物理中往往用随机过程来刻画。其实，生活中几乎所有的系统都在进行演变，例如一座城市的交通、一片湖中的生态、一场足球比赛、一个星系等。对于一个随机过程，其最关键的要素就是状态以及状态转移的条件概率分布。这就好比一个微粒在水中的布朗运动可以由它的起始位置以及下一刻的位置相对当前位置的条件概率分布来刻画。<br>如果在环境这样一个自身演变的随机过程中加入一个外来的干扰因素，即智能体的动作，那么环境的下一刻状态的概率分布将由当前状态和智能体的动作来共同决定，用最简单的数学公式表示则是<br>$下一状态~P(·|当前状态，智能体的动作)$<br>根据上式可知，智能体决策的动作作用到环境中，使得环境发生相应的状态改变，而智能体接下来则需要在新的状态下进一步给出决策。<br>由此我们看到，与面向决策任务的智能体进行交互的环境是一个动态的随机过程，其未来状态的分布由当前状态和智能体决策的动作来共同决定，并且每一轮状态转移都伴随着两方面的随机性：一是智能体决策的动作的随机性，二是环境基于当前状态和智能体动作来采样下一刻状态的随机性。通过对环境的动态随机过程的刻画，我们能清楚地感受到，在动态随机过程中学习和在一个固定的数据分布下学习是非常不同的。</p><h1 id="3-强化学习的目标"><a href="#3-强化学习的目标" class="headerlink" title="3 强化学习的目标"></a>3 强化学习的目标</h1><p>在上述动态环境下，智能体和环境每次进行交互时，环境会产生相应的奖励信号，其往往由实数标量来表示。这个奖励信号一般是诠释当前状态或动作的好坏的及时反馈信号，好比在玩游戏的过程中某一个操作获得的分数值。整个交互过程的每一轮获得的奖励信号可以进行累加，形成智能体的整体回报（return），好比一盘游戏最后的分数值。根据环境的动态性我们可以知道，即使环境和智能体策略不变，智能体的初始状态也不变，智能体和环境交互产生的结果也很可能是不同的，对应获得的回报也会不同。因此，在强化学习中，我们关注回报的期望，并将其定义为价值（value），这就是强化学习中智能体学习的优化目标。<br>价值的计算有些复杂，因为需要对交互过程中每一轮智能体采取动作的概率分布和环境相应的状态转移的概率分布做积分运算。强化学习和有监督学习的学习目标其实是一致的，即在某个数据分布下优化一个分数值的期望。不过，经过后面的分析我们会发现，强化学习和有监督学习的优化途径是不同的。</p><h1 id="4-强化学习中的数据"><a href="#4-强化学习中的数据" class="headerlink" title="4 强化学习中的数据"></a>4 强化学习中的数据</h1><p>接下来我们从数据层面谈谈有监督学习和强化学习的区别。<br>有监督学习的任务建立在从给定的数据分布中采样得到的训练数据集上，通过优化在训练数据集中设定的目标函数（如最小化预测误差）来找到模型的最优参数。这里，训练数据集背后的数据分布是完全不变的。<br>在强化学习中，数据是在智能体与环境交互的过程中得到的。如果智能体不采取某个决策动作，那么该动作对应的数据就永远无法被观测到，所以当前智能体的训练数据来自之前智能体的决策结果。因此，智能体的策略不同，与环境交互所产生的数据分布就不同，如下图所示。<br><img src="/image/RL-02.png"><br>具体而言，强化学习中有一个关于数据分布的概念，叫作占用度量（occupancy measure），其具体的数学定义和性质会在第3章讨论，在这里我们只做简要的陈述：归一化的占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的状态动作对（state-action pair)的概率分布。<br>占用度量有一个很重要的性质：给定两个策略及其与一个动态环境交互得到的两个占用度量，那么当且仅当这两个占用度量相同时，这两个策略相同。也就是说，如果一个智能体的策略有所改变，那么它和环境交互得到的占用度量也会相应改变。<br>根据占用度量这一重要的性质，我们可以领悟到强化学习本质的思维方式。</p><ul><li>强化学习的策略在训练中会不断更新，其对应的数据分布（即占用度量）也会相应地改变。因此，强化学习的一大难点就在于，智能体看到的数据分布是随着智能体的学习而不断发生改变的。</li><li>由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此寻找最优策略对应着寻找最优占用度量。</li></ul><h1 id="5-强化学习的独特性"><a href="#5-强化学习的独特性" class="headerlink" title="5 强化学习的独特性"></a>5 强化学习的独特性</h1><p>通过前面5节的讲解，读者现在应该已经对强化学习的基本数学概念有了一定的了解。这里我们回过头来再看看一般的有监督学习和强化学习的区别。<br>对于一般的有监督学习任务，我们的目标是找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数。在训练数据独立同分布的假设下，这个优化目标表示最小化模型在整个数据分布上的泛化误差（generalization error），用简要的公式可以概括为：<br>最优模型$&#x3D;\arg\min_\text{模型}\mathbb{E}_($特征，标签$)\sim$数据分布[损失函数(标签，模型(特征))]<br>相比之下，强化学习任务的最终优化目标是最大化智能体策略在和动态环境交互过程中的价值。根据第4节的分析，策略的价值可以等价转换成奖励函数在策略的占用度量上的期望，即：</p><p>$\text{最优策略}&#x3D;\arg\max_\text{策略}\mathbb{E}_{(\text{状态,动作})\sim\text{策略的占用度里}}[\text{奖励函数}(\text{状态,动作})]$</p><p>观察以上两个优化公式，我们可以回顾第3节，总结出两者的相似点和不同点。</p><ul><li>有监督学习和强化学习的优化目标相似，即都是在优化某个数据分布下的一个分数值的期望。</li><li>二者优化的途径是不同的，有监督学习直接通过优化模型对于数据特征的输出来优化目标，即修改目标函数而数据分布不变；强化学习则通过改变策略来调整智能体和环境交互数据的分布，进而优化目标，即修改数据分布而目标函数不变。<br>综上所述，一般有监督学习和强化学习的范式之间的区别为：</li><li>一般的有监督学习关注寻找一个模型，使其在给定数据分布下得到的损失函数的期望最小；</li><li>强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望。</li></ul>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>仿真</tag>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML-0 线性回归</title>
    <link href="/2024/08/16/ML-01/"/>
    <url>/2024/08/16/ML-01/</url>
    
    <content type="html"><![CDATA[<h1 id="1-基本形式"><a href="#1-基本形式" class="headerlink" title="1 基本形式"></a>1 基本形式</h1><p>线性模型尝试用一个多个属性的线性组合来进行预测：<br>$f(\boldsymbol{x})&#x3D;w_1x_1+w_2x_2+\ldots+w_dx_d+b$<br>写成：<br>$f(\boldsymbol{x})&#x3D;\boldsymbol{w}^\mathrm{T}\boldsymbol{x}+b$<br>模型的确定取决于$w,b$是否被确定.</p><h1 id="2-线性回归"><a href="#2-线性回归" class="headerlink" title="2 线性回归"></a>2 线性回归</h1><h2 id="2-1单输入"><a href="#2-1单输入" class="headerlink" title="2.1单输入"></a>2.1单输入</h2><p>线性回归要求预测值尽可能的接近真实值.<br>对于离散属性，若输入在时域具有先后关系，可以将其进行编码，转化为连续值：<br>将$n$个属性使用$n$维向量进行热编码，例：“高，中，低”可编码为“(100),(010),(001)”<br>线性回归期望获得如下：<br>$f(x_i)&#x3D;wx_i+b,  f(x_i)\simeq y_i$<br>正如第1节所说，确定$w,b$就能得到预测值；确定它们的关键在于度量预测值与真实值之间的差距.<br>在这里，我们尝试用均方误差(MSE)来对其进行度量，即：<br>$(w^*,b^*)&#x3D;\arg\min_{(w,b)}\sum_{i&#x3D;1}^m\left(f\left(x_i\right)-y_i\right)^2$<br>基于均方误差来进行模型求解的方法称为“最小二乘法”，它意味着找到一条直线，使样本到直线上的欧式距离之和最小.<br>求解出最小值的过程称为模型的最小二乘“参数估计”.对上式求偏导：<br>$\frac{\partial E_{(w,b)}}{\partial w}&#x3D;2\left(w\sum_{i&#x3D;1}^mx_i^2-\sum_{i&#x3D;1}^m\left(y_i-b\right)x_i\right)$<br>$\frac{\partial E_{(w,b)}}{\partial b}&#x3D;2\left(mb-\sum_{i&#x3D;1}^m\left(y_i-wx_i\right)\right)$<br>令偏导为0可得$w,b$最优解的闭式解：<br>$w&#x3D;\frac{\sum_{i&#x3D;1}^my_i(x_i-\bar{x})}{\sum_{i&#x3D;1}^mx_i^2-\frac1m\left(\sum_{i&#x3D;1}^mx_i\right)^2}$<br>$b&#x3D;\frac1m\sum_{i&#x3D;1}^m(y_i-wx_i)$</p><h2 id="2-2多输入"><a href="#2-2多输入" class="headerlink" title="2.2多输入"></a>2.2多输入</h2><p>更一般的情形是，输入样本由多个属性.<br>此时期望获得：<br>$f(x_i)&#x3D;w^{T}x_i+b,  f(x_i)\simeq y_i$<br>称多元线性回归.<br>同样的，利用最小二乘法对参数进行估计.<br>将x扩充为</p><p>$\mathbf{X}&#x3D;\begin{pmatrix}x_{11}&amp;x_{12}&amp;\ldots&amp;x_{1d}&amp;1\x_{21}&amp;x_{22}&amp;\ldots&amp;x_{2d}&amp;1\\vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\vdots\x_{m1}&amp;x_{m2}&amp;\ldots&amp;x_{md}&amp;1\end{pmatrix}&#x3D;\begin{pmatrix}\boldsymbol{x}_1^\mathrm{T}&amp;1\\boldsymbol{x}_2^\mathrm{T}&amp;1\\vdots&amp;\vdots\\boldsymbol{x}_m^\mathrm{T}&amp;1\end{pmatrix}$</p><p>将输入写成向量形式，则对于$w$,有：<br>$\hat{\boldsymbol{w}}^{*}&#x3D;\arg\min_{\hat{\boldsymbol{w}}}\left(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{w}}\right)^{\mathrm{T}}\left(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{w}}\right)$<br>求偏导得：<br>$\frac{\partial E_{\hat{\boldsymbol{w}}}}{\partial\hat{\boldsymbol{w}}}&#x3D;2 \mathbf{X}^{\mathrm{T}}\left(\mathbf{X}\hat{\boldsymbol{w}}-\boldsymbol{y}\right)$<br>令上式为0即可得到.</p><p>线性函数虽然简单，但变化丰富，我们可以让预测函数与一个可微调函数$g(·)$相乘，使其可以拟合更多类型的数值：<br>$y&#x3D;g^{-1}(w^Tx+b)$<br>上述模型称广义线性模型，</p><h1 id="3-对数几率回归"><a href="#3-对数几率回归" class="headerlink" title="3 对数几率回归"></a>3 对数几率回归</h1><p>对于二分类任务，将得到的预测值转为0&#x2F;1值，最理想的是“单位阶跃函数”，但是它不连续，不能作为可微调函数.作为替代，使用：<br>$y&#x3D;\frac{1}{1+e^{-z}}$<br>若令z等于$(w^Tx+b)$,再进行变化，得：<br>$ln\frac{y}{1-y}&#x3D;(w^Tx+b)$<br>y和1-y分别是正例和反例的可能性.<br>实际上是使用线性回归模型去逼近真实标记的对数几率.</p><h1 id="4-代码"><a href="#4-代码" class="headerlink" title="4 代码"></a>4 代码</h1><p>(基于sklearn库)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><br><span class="hljs-comment"># -*- encoding: utf-8 -*-</span><br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string"></span><br><span class="hljs-string">@File    :   linear regression.ipynb</span><br><span class="hljs-string"></span><br><span class="hljs-string">@Time    :   2024/08/15 15:59:55</span><br><span class="hljs-string"></span><br><span class="hljs-string">@Author  :   Neutrin</span><br><span class="hljs-string"></span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment">#this is a simple of linear regression models</span><br><span class="hljs-comment"># here put the import lib</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix, ConfusionMatrixDisplay<br><br><span class="hljs-comment">#读取数据</span><br>data = pd.read_csv(<span class="hljs-string">&#x27;watermelon3_0_Ch.csv&#x27;</span>)<br><br><span class="hljs-comment">#热编码</span><br>data = pd.get_dummies(data)<br><span class="hljs-built_in">print</span>(data.shape)<br><br><span class="hljs-comment">#划分数据集</span><br><br>x_train, x_test, y_train, y_test = train_test_split(data.iloc[:,<span class="hljs-number">0</span>:<span class="hljs-number">20</span>],data.iloc[:,<span class="hljs-number">21</span>],test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment">#建立模型</span><br>model = LogisticRegression(max_iter=<span class="hljs-number">1000</span>)<br>model.fit(x_train, y_train)<br><br><span class="hljs-comment">#预测</span><br>y_pred = model.predict(x_test)<br><br><span class="hljs-comment">#评估</span><br>score = model.score(x_test, y_test)<br><span class="hljs-built_in">print</span>(score)<br>cm = confusion_matrix(y_test, y_pred)<br><br><span class="hljs-comment"># 归一化混淆矩阵</span><br><br>cm_normalized = cm.astype(<span class="hljs-string">&#x27;float&#x27;</span>) / cm.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>)[:, np.newaxis]<br><br><span class="hljs-comment"># 绘制归一化混淆矩阵</span><br><br>disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=model.classes_)<br><br>disp.plot(cmap=plt.cm.Blues)<br><span class="hljs-comment"># 显示图像</span><br>plt.title(<span class="hljs-string">&#x27;Normalized Confusion Matrix&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>结果如图所示：<br><img src="/image/LR.png"></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>仿真</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
