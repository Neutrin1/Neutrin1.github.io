<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>[朴素贝叶斯法]</title>
    <link href="/2025/03/26/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"/>
    <url>/2025/03/26/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="朴素贝叶斯法：简单而强大的概率分类器"><a href="#朴素贝叶斯法：简单而强大的概率分类器" class="headerlink" title="朴素贝叶斯法：简单而强大的概率分类器"></a>朴素贝叶斯法：简单而强大的概率分类器</h1><p>朴素贝叶斯是机器学习中一种基于概率的分类方法，它像一位经验丰富的法官，通过已有的证据来推断最可能的结论。这种方法简单高效，特别适合处理文本分类等任务。</p><h2 id="1-基本思想-用日常例子理解"><a href="#1-基本思想-用日常例子理解" class="headerlink" title="1. 基本思想 - 用日常例子理解"></a>1. 基本思想 - 用日常例子理解</h2><p>朴素贝叶斯的核心是<strong>贝叶斯定理</strong>，它告诉我们如何根据已知信息更新我们的信念。</p><p>想象一下：</p><ul><li>你收到一封邮件，想知道它是否是垃圾邮件</li><li>你观察到这封邮件中有”免费”、”中奖”等词语</li></ul><p>朴素贝叶斯会问：</p><ol><li>一般来说，垃圾邮件占所有邮件的比例是多少？（先验概率）</li><li>如果是垃圾邮件，出现这些词的可能性有多大？（似然概率）</li><li>综合以上信息，这封邮件是垃圾邮件的可能性有多大？（后验概率）</li></ol><p>用公式表示：</p><p>$$P(垃圾邮件|观察到的词) &#x3D; \frac{P(观察到的词|垃圾邮件)P(垃圾邮件)}{P(观察到的词)}$$</p><p>“朴素”在哪里？朴素贝叶斯假设所有特征（如邮件中的每个词）都相互独立。这就像假设”免费”这个词出现的概率不受”中奖”这个词是否出现的影响。</p><h2 id="2-朴素贝叶斯分类器-决策过程"><a href="#2-朴素贝叶斯分类器-决策过程" class="headerlink" title="2. 朴素贝叶斯分类器 - 决策过程"></a>2. 朴素贝叶斯分类器 - 决策过程</h2><p>分类时，朴素贝叶斯会计算”这封邮件是垃圾邮件的概率”和”这封邮件是正常邮件的概率”，然后选择概率较大的类别。</p><p>$$\hat{y} &#x3D; \arg\max_{y} P(y) \prod_{i&#x3D;1}^{n} P(x_i|y)$$</p><p>举例：判断一封含有”免费”和”朋友”的邮件</p><p>计算是垃圾邮件的概率：</p><ul><li>垃圾邮件的先验概率：30%</li><li>“免费”出现在垃圾邮件中的概率：80%</li><li>“朋友”出现在垃圾邮件中的概率：20%</li><li>结果：30% × 80% × 20% &#x3D; 4.8%</li></ul><p>计算是正常邮件的概率：</p><ul><li>正常邮件的先验概率：70% </li><li>“免费”出现在正常邮件中的概率：10%</li><li>“朋友”出现在正常邮件中的概率：60%</li><li>结果：70% × 10% × 60% &#x3D; 4.2%</li></ul><p>结论：这封邮件更可能是垃圾邮件（4.8% &gt; 4.2%）</p><h2 id="3-常见的朴素贝叶斯模型-适应不同数据类型"><a href="#3-常见的朴素贝叶斯模型-适应不同数据类型" class="headerlink" title="3. 常见的朴素贝叶斯模型 - 适应不同数据类型"></a>3. 常见的朴素贝叶斯模型 - 适应不同数据类型</h2><h3 id="3-1-高斯朴素贝叶斯"><a href="#3-1-高斯朴素贝叶斯" class="headerlink" title="3.1 高斯朴素贝叶斯"></a>3.1 高斯朴素贝叶斯</h3><p>适用于连续数值特征，比如身高、体重、温度等。</p><p>例子：根据身高、体重预测性别</p><ul><li>假设男性身高服从均值为175cm、方差为36的正态分布</li><li>如果观察到一个人身高180cm，可以计算这个身高在男性分布中的概率</li></ul><p>$$P(身高&#x3D;180|男性) &#x3D; \frac{1}{\sqrt{2\pi\cdot 36}} \exp\left(-\frac{(180 - 175)^2}{2\cdot 36}\right)$$</p><h3 id="3-2-多项式朴素贝叶斯"><a href="#3-2-多项式朴素贝叶斯" class="headerlink" title="3.2 多项式朴素贝叶斯"></a>3.2 多项式朴素贝叶斯</h3><p>适用于计数型特征，特别是文本分类。</p><p>例子：根据单词出现次数分类新闻</p><ul><li>体育新闻中”比赛”、”得分”、”球员”等词出现频率高</li><li>政治新闻中”政策”、”国家”、”经济”等词出现频率高</li></ul><h3 id="3-3-伯努利朴素贝叶斯"><a href="#3-3-伯努利朴素贝叶斯" class="headerlink" title="3.3 伯努利朴素贝叶斯"></a>3.3 伯努利朴素贝叶斯</h3><p>适用于二元特征（有&#x2F;无）的情况。</p><p>例子：判断电影类型</p><ul><li>是否包含爆炸场景？(是&#x2F;否)</li><li>是否有浪漫情节？(是&#x2F;否)</li><li>是否出现外星生物？(是&#x2F;否)</li></ul><h2 id="4-算法优缺点-实际应用中的考量"><a href="#4-算法优缺点-实际应用中的考量" class="headerlink" title="4. 算法优缺点 - 实际应用中的考量"></a>4. 算法优缺点 - 实际应用中的考量</h2><p><strong>优点：</strong></p><ul><li><strong>速度快</strong>：训练和预测都很快，就像快速浏览简历而不是深入阅读</li><li><strong>小样本也能用</strong>：即使只有少量数据也能工作得不错</li><li><strong>容易理解</strong>：模型逻辑清晰，像是基于几个简单规则的决策</li><li><strong>处理高维数据</strong>：即使有成千上万个特征（如文章中的词汇量）也能高效处理</li></ul><p><strong>缺点：</strong></p><ul><li><strong>独立性假设过强</strong>：特征间往往有联系，比如”纽约”和”曼哈顿”同时出现的概率比独立计算的要高</li><li><strong>对极端值敏感</strong>：一个非常罕见的特征可能过度影响结果</li><li><strong>零概率问题</strong>：如果某个词在训练数据的某类别中从未出现过，会导致整个概率变为零</li></ul><h2 id="5-实际应用-生活中的例子"><a href="#5-实际应用-生活中的例子" class="headerlink" title="5. 实际应用 - 生活中的例子"></a>5. 实际应用 - 生活中的例子</h2><ul><li><strong>垃圾邮件过滤</strong>：Gmail基于邮件内容判断是否为垃圾邮件</li><li><strong>新闻分类</strong>：自动将新闻分到体育、政治、科技等类别</li><li><strong>情感分析</strong>：判断产品评论是正面还是负面</li><li><strong>疾病诊断</strong>：根据各种症状判断最可能的疾病</li><li><strong>推荐系统</strong>：简单的内容推荐，如”喜欢这本书的人也喜欢…”</li></ul><h2 id="6-代码示例-文本分类实战"><a href="#6-代码示例-文本分类实战" class="headerlink" title="6. 代码示例 - 文本分类实战"></a>6. 代码示例 - 文本分类实战</h2><p>下面是使用Python实现的一个新闻分类例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_20newsgroups<br><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> CountVectorizer<br><span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> MultinomialNB<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score, classification_report<br><br><span class="hljs-comment"># 加载数据（四个不同主题的新闻组）</span><br>categories = [<span class="hljs-string">&#x27;alt.atheism&#x27;</span>, <span class="hljs-string">&#x27;soc.religion.christian&#x27;</span>, <span class="hljs-string">&#x27;comp.graphics&#x27;</span>, <span class="hljs-string">&#x27;sci.med&#x27;</span>]<br>data = fetch_20newsgroups(subset=<span class="hljs-string">&#x27;all&#x27;</span>, categories=categories, shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 特征提取：将文本转为词频向量（词袋模型）</span><br>vectorizer = CountVectorizer(stop_words=<span class="hljs-string">&#x27;english&#x27;</span>, max_features=<span class="hljs-number">1000</span>)  <span class="hljs-comment"># 忽略常见英文虚词，只保留1000个最常见词</span><br>X = vectorizer.fit_transform(data.data)  <span class="hljs-comment"># 转换为词频矩阵</span><br>y = data.target  <span class="hljs-comment"># 类别标签</span><br><br><span class="hljs-comment"># 划分训练集和测试集（80%训练，20%测试）</span><br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># 训练朴素贝叶斯模型</span><br>clf = MultinomialNB(alpha=<span class="hljs-number">1.0</span>)  <span class="hljs-comment"># alpha是平滑参数，避免零概率问题</span><br>clf.fit(X_train, y_train)<br><br><span class="hljs-comment"># 预测并评估</span><br>y_pred = clf.predict(X_test)<br>accuracy = accuracy_score(y_test, y_pred)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;准确率: <span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(classification_report(y_test, y_pred, target_names=data.target_names))<br><br><span class="hljs-comment"># 演示预测</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_category</span>(<span class="hljs-params">text</span>):<br>    text_vector = vectorizer.transform([text])<br>    category = data.target_names[clf.predict(text_vector)[<span class="hljs-number">0</span>]]<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;预测类别: <span class="hljs-subst">&#123;category&#125;</span>&quot;</span><br><br><span class="hljs-comment"># 测试几个例子</span><br><span class="hljs-built_in">print</span>(predict_category(<span class="hljs-string">&quot;God created the universe and loves everyone.&quot;</span>))<br><span class="hljs-built_in">print</span>(predict_category(<span class="hljs-string">&quot;The new graphics card can render 3D images quickly.&quot;</span>))<br><span class="hljs-built_in">print</span>(predict_category(<span class="hljs-string">&quot;The patient shows symptoms of high fever and cough.&quot;</span>))<br></code></pre></td></tr></table></figure><p>通过这个简单的朴素贝叶斯模型，我们就能够相当准确地对新闻文本进行分类，展示了这种简单算法的强大之处。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K邻近法(k-nearest neighbor,k-NN)</title>
    <link href="/2025/03/25/K%E9%82%BB%E8%BF%91%E6%B3%95/"/>
    <url>/2025/03/25/K%E9%82%BB%E8%BF%91%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h1><p>kNN是一种基本分类与回归方法，这里讨论分类问题的KNN方法。kNN假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其 $k$ 个最近邻的训练实例的类别，通过多数表决等方式进行预测。kNN利用训练数据集对特征向量空间进行划分，并作为其分类的”模型”。三个基本要素是k值的选择、距离度量以及分类决策规则。</p><h1 id="2-k邻近算法-简洁版"><a href="#2-k邻近算法-简洁版" class="headerlink" title="2 k邻近算法(简洁版)"></a>2 k邻近算法(简洁版)</h1><h2 id="2-1算法"><a href="#2-1算法" class="headerlink" title="2.1算法"></a>2.1算法</h2><p>(1)根据给定的距离度量，在训练集T中找出与 $x_i$ 最近邻的 $k$ 个点，涵盖这 $k$ 个点的邻域记作 $N_k(x)$.</p><p>(2)在 $N_k(x)$中根据分类决策规则(如多数表决)决定 $x$ 的类别 $y$：</p><p>$$y &#x3D; \arg \max_{c_j} \sum_{x_i \in N_k(x)} I(y_i &#x3D; c_j), \quad i &#x3D; 1, 2, \dots, N; \quad j &#x3D; 1, 2, \dots, K$$</p><p>其中，$I$ 为指示函数。<br>k近邻法的特殊情况是$k$ &#x3D;1的情形，称为最近邻算法。<br>没有显式的学习过程。</p><h2 id="2-2模型"><a href="#2-2模型" class="headerlink" title="2.2模型"></a>2.2模型</h2><p>当训练集、距离度量、$k$ 值及分类决策规则确定后，对于一个新的输入实例，它所属的类唯一地确定。特征空间中，对于每个训练点 $x_i$，距离该点比其他点更近的所有点组成一个区域，叫做单元；每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。最邻近法将实例 $x_i$ 的类 $y_i$ 作为其单元中所有点的类标记。<br><img src="/image/knn1.png"></p><h3 id="a-距离度量"><a href="#a-距离度量" class="headerlink" title="a)距离度量"></a>a)距离度量</h3><p>特征空间中两个实例点的距离是两个实例点相似程度的反映。kNN模型的特征空间一般是n维实数向量空间$R^n$。使用的距离是欧氏距离，也可以是其它距离。</p><h3 id="b-k值的选择"><a href="#b-k值的选择" class="headerlink" title="b)k值的选择"></a>b)k值的选择</h3><p>k值的选择会对k近邻法的结果产生重大影响。</p><p>如果选择较小的 $k$ 值，就相当于用较小的邻域中的训练实例进行预测，”学习”的近似误差（approximation error）会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是”学习”的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说， $k$ 值的减小就意味着整体模型变得复杂，容易发生过拟合。</p><p>如果选择较大的 $k$ 值，就相当于用较大邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误。k值的增大就意味着整体的模型变得简单。</p><p>如果 $k&#x3D;N$，那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。<br>在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。</p><h3 id="c-分类决策规则"><a href="#c-分类决策规则" class="headerlink" title="c)分类决策规则"></a>c)分类决策规则</h3><p>$k$ 近邻法中的分类决策规则往往是多数表决，即由输入实例的 $k$ 个邻近的训练实例中的多数类决定输入实例的类。</p><p>多数表决规则（majority voting rule）有如下解释：如果分类的损失函数为 0-1 损失函数，分类函数为</p><p>$$f : \mathbb{R}^n \to {c_1, c_2, \cdots, c_K}$$</p><p>那么误分类的概率是</p><p>$$P(Y \neq f(X)) &#x3D; 1 - P(Y &#x3D; f(X))$$</p><p>对给定的实例 $x \in \mathcal{X}$，其最近邻的 $k$ 个训练实例点构成集合 $N_k(x)$。如果涵盖 $N_k(x)$ 的区域的类别是 $c_j$，那么误分类率是</p><p>$$\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \neq c_j) &#x3D; 1 - \frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i &#x3D; c_j)$$</p><p>要使误分类率最小即经验风险最小，就要使<br>$$\sum_{x_i \in N_k(x)} I(y_i &#x3D; c_j)$$<br>最大，所以多数表决规则等价于经验风险最小化。</p><h1 id="3-kNN的实现：kd树"><a href="#3-kNN的实现：kd树" class="headerlink" title="3 kNN的实现：kd树"></a>3 kNN的实现：kd树</h1><h2 id="3-1-kd树介绍"><a href="#3-1-kd树介绍" class="headerlink" title="3.1 kd树介绍"></a>3.1 kd树介绍</h2><p>kd树(k-dimensional tree)是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分，其每个结点对应于k维空间划分中的一个超矩形区域。</p><p>kd树的构造目的是实现高效的最近邻搜索，特别是在k维空间中的数据点很多时，直接计算距离的方法效率低下。</p><h2 id="3-2-kd树的构造"><a href="#3-2-kd树的构造" class="headerlink" title="3.2 kd树的构造"></a>3.2 kd树的构造</h2><p>kd树的构造过程是一个递归的过程：</p><ol><li>在k维空间中选择一个坐标轴j进行切分（通常按轮流选择的规则）</li><li>在选定的坐标轴上，选择切分点，将数据集划分为两部分</li><li>对划分的两部分分别递归构建左右子树</li></ol><p>常用的切分方式是选择当前维度上的中位数作为切分点，这样可以保证构造出的kd树是平衡的。</p><p>构造算法的数学描述如下：</p><p>$$\text{BuildKdTree}(P, depth):$$</p><p>$$<br>\begin{cases}<br>\text{如果 } P \text{ 为空，返回 } \text{null}\<br>\text{axis} &#x3D; \text{depth} \bmod k\<br>\text{按 } \text{axis} \text{ 维度对 } P \text{ 中的点排序}\<br>\text{median} &#x3D; \text{在 } \text{axis} \text{ 维度上的中位数点}\<br>\text{将 } \text{median} \text{ 作为根节点}\<br>\text{root.left} &#x3D; \text{BuildKdTree}(P_{\text{left}}, \text{depth}+1)\<br>\text{root.right} &#x3D; \text{BuildKdTree}(P_{\text{right}}, \text{depth}+1)\<br>\text{返回 } \text{root}<br>\end{cases}<br>$$</p><p>其中，$P_{\text{left}}$ 是 $\text{axis}$ 维度上小于 $\text{median}$ 的点集，$P_{\text{right}}$ 是大于 $\text{median}$ 的点集。</p><h2 id="3-3-kd树的最近邻搜索"><a href="#3-3-kd树的最近邻搜索" class="headerlink" title="3.3 kd树的最近邻搜索"></a>3.3 kd树的最近邻搜索</h2><p>kd树最核心的操作是搜索给定点的最近邻。算法步骤如下：</p><ol><li>从根节点开始，递归地向下遍历树</li><li>在每个节点，将目标点与当前节点比较</li><li>根据比较结果选择一个子树优先遍历</li><li>回溯时检查另一子树是否可能包含更近的点</li><li>如果可能，遍历另一子树</li></ol><p>最近邻搜索的数学表达为：</p><p>$$\text{NearestNeighbor}(\text{node}, \text{target}, \text{best}):$$</p><p>$$<br>\begin{cases}<br>\text{如果 } \text{node} \text{ 为空，返回 } \text{best}\<br>d &#x3D; \text{Distance}(\text{target}, \text{node.point})\<br>\text{如果 } d &lt; \text{Distance}(\text{target}, \text{best}) \text{ 则 } \text{best} &#x3D; \text{node.point}\<br>\text{axis} &#x3D; \text{node} \text{ 的划分维度}\<br>\text{如果 } \text{target}[\text{axis}] &lt; \text{node.point}[\text{axis}]\<br>\quad \text{first} &#x3D; \text{node.left}\<br>\quad \text{second} &#x3D; \text{node.right}\<br>\text{否则}\<br>\quad \text{first} &#x3D; \text{node.right}\<br>\quad \text{second} &#x3D; \text{node.left}\<br>\text{best} &#x3D; \text{NearestNeighbor}(\text{first}, \text{target}, \text{best})\<br>\text{如果 } |\text{target}[\text{axis}] - \text{node.point}[\text{axis}]| &lt; \text{Distance}(\text{target}, \text{best})\<br>\quad \text{best} &#x3D; \text{NearestNeighbor}(\text{second}, \text{target}, \text{best})\<br>\text{返回 } \text{best}<br>\end{cases}<br>$$</p><h2 id="3-4-kd树的复杂度分析"><a href="#3-4-kd树的复杂度分析" class="headerlink" title="3.4 kd树的复杂度分析"></a>3.4 kd树的复杂度分析</h2><ul><li>构建kd树的时间复杂度：$O(n\log n)$，其中$n$是数据点的数量</li><li>平衡kd树的空间复杂度：$O(n)$</li><li>最近邻搜索的平均时间复杂度：$O(\log n)$</li><li>最近邻搜索的最坏情况时间复杂度：$O(n)$（当数据分布极不均匀时）</li></ul><p>kd树在低维空间（通常 $k&lt;20$ ）中表现良好，但在高维空间中由于”维度灾难”问题，其效率会急剧下降。在高维情况下，可能需要考虑其他数据结构如哈希或近似最近邻算法。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>感知机(Perceptron)</title>
    <link href="/2025/03/25/Perceptron/"/>
    <url>/2025/03/25/Perceptron/</url>
    
    <content type="html"><![CDATA[<h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h1><p>感知机是一种用于二分类的线性分类模型，输入为实例的特征向量，输出为实例的类别，取$+1$和$-1$二值。感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于判别模型。感知机的目的是训练出将输入数据进行二分类的超平面；因此，可以导入基于误分类的损失函数，并利用梯度下降算法对损失函数进行极小化，以此得到感知机模型。</p><h1 id="2-感知机-简洁版"><a href="#2-感知机-简洁版" class="headerlink" title="2 感知机(简洁版)"></a>2 感知机(简洁版)</h1><h2 id="2-1模型"><a href="#2-1模型" class="headerlink" title="2.1模型"></a>2.1模型</h2><p>定义由输入空间到输出空间的函数：</p><p>$$f(x)&#x3D;sign(wx+b)$$</p><p>称感知机。其中，权重$w$和偏置$b$是感知机的参数，$sign$是符号函数。感知机是一种线性分类模型，属于判别模型。<br><img src="/image/Perceptron1.png"><br>由上可知，对于感知机，输出值无非不过两种情况：</p><p>$$wx+b&gt;0&#x2F;wx+b&lt;0$$</p><p>而我们要找的超平面为</p><p>$$wx+b&#x3D;0$$</p><p>注意，数据集必须要线性可分。</p><h2 id="2-2学习策略"><a href="#2-2学习策略" class="headerlink" title="2.2学习策略"></a>2.2学习策略</h2><h3 id="a-损失函数"><a href="#a-损失函数" class="headerlink" title="a)损失函数"></a>a)损失函数</h3><p>$$-\frac{1}{|w|} \sum_{x_i \in M} y_i (w \cdot x_i + b)$$</p><p>是所有误分类点到超平面$S$的总距离，其中</p><p>$$-\frac{1}{|w|}$$</p><p>为$w$的$L_2$范数，若不考虑，则损失函数的定义为：</p><p>$$L(w, b) &#x3D; -\sum_{x_i \in M} y_i (w \cdot x_i + b)$$</p><p>这是关于$w$,$b$的连续可导函数。</p><h3 id="b-学习算法"><a href="#b-学习算法" class="headerlink" title="b)学习算法"></a>b)学习算法</h3><p>获得感知机模型的过程本质上就是极小化损失函数的过程：</p><p>$$\min_{w, b} L(w, b) &#x3D; - \sum_{x_i \in M} y_i (w \cdot x_i + b)$$</p><p>假设误分类点集合是固定的，那么损失函数$L(w,b)$的梯度由：<br>$$\nabla_w L(w, b) &#x3D; - \sum_{x_i \in M} y_i x_i$$</p><p>$$\nabla_b L(w, b) &#x3D; - \sum_{x_i \in M} y_i$$<br>给出。</p><p>随机选取一个误分类点$(x_i,y_i)$，对$w$,$b$进行更新：</p><p>$$w \leftarrow w + \eta y_i x_i$$</p><p>$$b \leftarrow b + \eta y_i$$<br>其中$\eta$是学习率。</p><h1 id="3实现代码"><a href="#3实现代码" class="headerlink" title="3实现代码"></a>3实现代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-comment"># -*- encoding: utf-8 -*-</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">@File    :   感知机.ipynb</span><br><span class="hljs-string">@Time    :   2025/03/24 21:29:44</span><br><span class="hljs-string">@Author  :   Neutrin </span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># here put the import lib</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>np.random.seed(<span class="hljs-number">42</span>)  <br><br><span class="hljs-comment"># 生成样本点 100个负样本 100个正样本</span><br>n_samples_neg = <span class="hljs-number">100</span><br>X_neg = np.random.normal(loc=[-<span class="hljs-number">2</span>, -<span class="hljs-number">2</span>], scale=<span class="hljs-number">1.0</span>, size=(n_samples_neg, <span class="hljs-number">2</span>))<br>y_neg = np.zeros(n_samples_neg)<br><br>n_samples_pos = <span class="hljs-number">100</span><br>X_pos = np.random.normal(loc=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], scale=<span class="hljs-number">1.0</span>, size=(n_samples_pos, <span class="hljs-number">2</span>))<br>y_pos = np.ones(n_samples_pos)<br><br><span class="hljs-comment"># 合并样本点</span><br>X = np.vstack((X_neg, X_pos))<br>y = np.hstack((y_neg, y_pos))<br><br><span class="hljs-comment"># 可视化</span><br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>))<br>plt.scatter(X_neg[:, <span class="hljs-number">0</span>], X_neg[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;blue&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">&#x27;Class 0&#x27;</span>)<br>plt.scatter(X_pos[:, <span class="hljs-number">0</span>], X_pos[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;red&#x27;</span>, marker=<span class="hljs-string">&#x27;x&#x27;</span>, label=<span class="hljs-string">&#x27;Class 1&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Perceptron Training Data&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Feature 1&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Feature 2&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Dataset shape: X: <span class="hljs-subst">&#123;X.shape&#125;</span>, y: <span class="hljs-subst">&#123;y.shape&#125;</span>&quot;</span>)<br><span class="hljs-comment"># 感知机用于二分类问题，输出为+1或-1</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">out</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-keyword">return</span> np.sign(np.dot(x, w) + b)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Perceptron</span>: <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, learning_rate=<span class="hljs-number">0.01</span>, max_iterations=<span class="hljs-number">1000</span></span>):  <span class="hljs-comment"># 学习率和最大迭代次数</span><br>        <span class="hljs-variable language_">self</span>.learning_rate = learning_rate<br>        <span class="hljs-variable language_">self</span>.max_iterations = max_iterations<br>        <span class="hljs-variable language_">self</span>.w = <span class="hljs-literal">None</span><br>        <span class="hljs-variable language_">self</span>.b = <span class="hljs-literal">None</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fit</span>(<span class="hljs-params">self, X, y</span>):<br>        <span class="hljs-comment"># 初始化权重和偏置</span><br>        n_samples, n_features = X.shape  <span class="hljs-comment"># 样本数和特征数</span><br>        <span class="hljs-variable language_">self</span>.w = np.zeros(n_features)<br>        <span class="hljs-variable language_">self</span>.b = <span class="hljs-number">0</span><br>        <br>        <span class="hljs-comment"># 将标签0转为-1 (感知机标签通常为-1和+1)</span><br>        y_transformed = np.where(y == <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># where函数，满足条件输出1，不满足输出0</span><br>        <br>        <span class="hljs-comment"># 训练模型</span><br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.max_iterations):<br>            misclassified = <span class="hljs-number">0</span>               <span class="hljs-comment"># 错误分类的样本数</span><br>            <span class="hljs-keyword">for</span> idx, x_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(X):   <br>                y_i = y_transformed[idx]<br>                <span class="hljs-comment"># 如果预测错误，更新权重</span><br>                <span class="hljs-keyword">if</span> y_i * (np.dot(x_i, <span class="hljs-variable language_">self</span>.w) + <span class="hljs-variable language_">self</span>.b) &lt;= <span class="hljs-number">0</span>:<br>                    <span class="hljs-variable language_">self</span>.w += <span class="hljs-variable language_">self</span>.learning_rate * y_i * x_i<br>                    <span class="hljs-variable language_">self</span>.b += <span class="hljs-variable language_">self</span>.learning_rate * y_i<br>                    misclassified += <span class="hljs-number">1</span><br>            <span class="hljs-comment"># 如果没有错误分类，提前结束训练</span><br>            <span class="hljs-keyword">if</span> misclassified == <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">break</span><br>                <br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> np.where(np.sign(np.dot(X, <span class="hljs-variable language_">self</span>.w) + <span class="hljs-variable language_">self</span>.b) == -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">score</span>(<span class="hljs-params">self, X, y</span>):<br>        y_pred = <span class="hljs-variable language_">self</span>.predict(X)<br>        <span class="hljs-keyword">return</span> np.mean(y_pred == y)<br>    <br>    <span class="hljs-comment"># 获取用于绘制决策边界的参数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decision_boundary</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.w, <span class="hljs-variable language_">self</span>.b<br><br><span class="hljs-comment"># 创建并训练感知机模型</span><br>perceptron = Perceptron(learning_rate=<span class="hljs-number">0.01</span>, max_iterations=<span class="hljs-number">1000</span>)<br>perceptron.fit(X, y)<br><br><span class="hljs-comment"># 评估模型</span><br>accuracy = perceptron.score(X, y)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Accuracy: <span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 绘制决策边界</span><br>w, b = perceptron.decision_boundary()<br>x_min, x_max = X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br>y_min, y_max = X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br>xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="hljs-number">0.1</span>),<br>                     np.arange(y_min, y_max, <span class="hljs-number">0.1</span>))<br>Z = np.sign(np.dot(np.c_[xx.ravel(), yy.ravel()], w) + b).reshape(xx.shape)<br><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br>plt.contourf(xx, yy, Z, alpha=<span class="hljs-number">0.3</span>, cmap=plt.cm.coolwarm)<br>plt.scatter(X_neg[:, <span class="hljs-number">0</span>], X_neg[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;blue&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">&#x27;Class 0&#x27;</span>)<br>plt.scatter(X_pos[:, <span class="hljs-number">0</span>], X_pos[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;red&#x27;</span>, marker=<span class="hljs-string">&#x27;x&#x27;</span>, label=<span class="hljs-string">&#x27;Class 1&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Perceptron Decision Boundary&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Feature 1&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Feature 2&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
