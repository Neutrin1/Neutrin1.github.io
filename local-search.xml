<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>增肌第二周复盘</title>
    <link href="/2025/04/28/%E5%A2%9E%E8%82%8C%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%A4%8D%E7%9B%98/"/>
    <url>/2025/04/28/%E5%A2%9E%E8%82%8C%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%A4%8D%E7%9B%98/</url>
    
    <content type="html"><![CDATA[<p>增肌第二周，力量有明显提升，数据有大的波动。<br>周六商务喝酒去了，没有练。<br>下周保持。</p><table><thead><tr><th>日期</th><th>数值</th><th>训练部位</th><th>训练内容</th><th>饮食</th><th>数值</th></tr></thead><tbody><tr><td>4月21日</td><td>149.1</td><td>练胸、练腹</td><td>练乒乓球1.5h；（宿舍练）10KG哑铃推胸10组，早起俯卧撑30，俯卧撑100，40KG臂力器4组，卷腹200</td><td>鸡胸肉+香蕉</td><td>17.9</td></tr><tr><td>4月22日</td><td>148.5</td><td>练背、练腹</td><td>练乒乓球1.5h；（宿舍练）10KG哑铃反手划船4组、正手划船4组、俯身抱拉4组、单边划船4组，卷腹150</td><td>鸡胸肉+香蕉</td><td>17.1</td></tr><tr><td>4月23日</td><td>146.9</td><td>练肩、练腹</td><td>（宿舍练）10KG哑铃推8组，7.5KG哑铃提拉8组，5KG侧平举4组，卷腹200</td><td>鸡胸肉+香蕉</td><td>16.6</td></tr><tr><td>4月24日</td><td>147.4</td><td>练手臂、练腹</td><td>练乒乓球1h，（宿舍练）5、7.5KG哑单臂铃弯举4组，7.5KG哑铃弯举4组，7.5KG哑铃正手弯举4组，7.5KG哑铃锤式弯举4组</td><td>鸡胸肉+香蕉</td><td>16.8</td></tr><tr><td>4月25日</td><td>147.6</td><td>练胸、练腹</td><td>（健身房练）35KG，40KG斜推上胸2+2，35KG，40KG斜推下胸2+2，蝴蝶机45KG夹胸4组，俯卧撑100，臂力器40KG 4组</td><td>鸡胸肉+香蕉</td><td>16.9</td></tr><tr><td>4月26日</td><td>149.9</td><td>无</td><td>无</td><td>无</td><td>17.6</td></tr><tr><td>4月27号</td><td>149.7</td><td>练肩、练腹</td><td>练乒乓球1h，蝴蝶机40KG推肩4组，15KG杠铃提拉4组，10KG哑铃推肩8组，俯卧撑50，卷腹200</td><td>鸡胸肉+香蕉</td><td>17.5</td></tr></tbody></table><p><img src="/image/weight3.png"></p><p>增肌第二周，力量有明显提升，数据有大的波动。<br>周六商务喝酒去了，没有练。<br>下周保持。</p>]]></content>
    
    
    <categories>
      
      <category>生活</category>
      
    </categories>
    
    
    <tags>
      
      <tag>健身</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023.2~2025.3体重复盘</title>
    <link href="/2025/04/21/2023-2-2025-3%E4%BD%93%E9%87%8D%E5%A4%8D%E7%9B%98/"/>
    <url>/2025/04/21/2023-2-2025-3%E4%BD%93%E9%87%8D%E5%A4%8D%E7%9B%98/</url>
    
    <content type="html"><![CDATA[<p>记录及复盘一下2023.2~2025.3之间的体重数据。<br><img src="/image/weight1.png"></p>]]></content>
    
    
    <categories>
      
      <category>生活</category>
      
    </categories>
    
    
    <tags>
      
      <tag>健身、减脂</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>增肌第一周复盘</title>
    <link href="/2025/04/21/%E5%A2%9E%E8%82%8C%E7%AC%AC%E4%B8%80%E5%91%A8%E5%A4%8D%E7%9B%98/"/>
    <url>/2025/04/21/%E5%A2%9E%E8%82%8C%E7%AC%AC%E4%B8%80%E5%91%A8%E5%A4%8D%E7%9B%98/</url>
    
    <content type="html"><![CDATA[<p>2025年第16周健身记录，增肌第一周。<br>由于这周处于轻感冒的亚健康状态，练得没有很猛。</p><table><thead><tr><th>日期</th><th>体重（单位&#x2F;斤）</th><th>健身记录</th><th>健身项目</th><th>练后餐</th></tr></thead><tbody><tr><td>4.14</td><td>149.1</td><td>练胸、练腹（康复训练）</td><td>(宿舍练)俯卧撑50，7.5KG哑铃弯举4组，10KG哑铃弯举4组，仰卧起坐100，卷腹100</td><td>肉干+香蕉</td></tr><tr><td>4.15</td><td>148.9</td><td>练背、练腹（康复训练）</td><td>练乒乓球1h；（健身房练）45KG高位下拉4组，45KG划船4组，坐姿划船20KG4组，杠铃俯身划船30KG4组，仰卧起坐200，卷腹200</td><td>肉干+香蕉</td></tr><tr><td>4.16</td><td>149.1</td><td>练肩、练腹（康复训练）</td><td>（宿舍练）哑铃坐姿推肩7.5KG8组，哑铃侧平举5KG8组，仰卧起坐100，卷腹150</td><td>无</td></tr><tr><td>4.17</td><td>147.3</td><td>练手臂、练腹（康复训练）</td><td>（宿舍练）7.5KG哑铃弯举4组，7.5KG哑铃锤式弯举4组，哑铃正手弯举4组，5KG哑铃单臂弯举4组</td><td>鸡胸肉+香蕉</td></tr><tr><td>4.18</td><td>148.1</td><td>练胸、练腹（康复训练）</td><td>（宿舍练）7.5KG哑铃斜推8组，40KG臂力器4组，俯卧撑50，卷腹120</td><td>鸡胸肉+香蕉</td></tr><tr><td>4.19</td><td>148.6</td><td>练背、练腹</td><td>练乒乓球1h；（宿舍练）9.5KG哑铃俯身划船4组，9.5KG哑铃俯身反向划船4组，7.5KG哑铃上斜划船4组，5KG俯身哑铃侧平举4组</td><td>鸡胸肉+香蕉</td></tr><tr><td>4.20</td><td>149.1</td><td>练肩、练腹</td><td>练乒乓球1.5h；（宿舍练）7.5KG哑铃推肩4组，9.5KG哑铃推肩4组，9.5KG哑铃阿诺德推举4组，5KG哑铃侧平举4组，7.5KG哑铃提拉4组</td><td>鸡胸肉+香蕉</td></tr></tbody></table><p><img src="/image/weight2.png"></p>]]></content>
    
    
    <categories>
      
      <category>生活</category>
      
    </categories>
    
    
    <tags>
      
      <tag>健身</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EfficientNet网络</title>
    <link href="/2025/04/07/EfficientNet%E7%BD%91%E7%BB%9C/"/>
    <url>/2025/04/07/EfficientNet%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="EfficientNet网络"><a href="#EfficientNet网络" class="headerlink" title="EfficientNet网络"></a>EfficientNet网络</h1><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>EfficientNet是由Google研究团队于2019年提出的一种高效卷积神经网络架构。它通过系统地扩展网络宽度、深度和分辨率，在保持较低参数量和计算成本的同时，实现了卓越的性能表现。EfficientNet系列模型在ImageNet等多个图像分类基准测试中刷新了当时的最高精度记录。</p><h2 id="2-核心创新点"><a href="#2-核心创新点" class="headerlink" title="2. 核心创新点"></a>2. 核心创新点</h2><h3 id="2-1-复合缩放方法（Compound-Scaling）"><a href="#2-1-复合缩放方法（Compound-Scaling）" class="headerlink" title="2.1 复合缩放方法（Compound Scaling）"></a>2.1 复合缩放方法（Compound Scaling）</h3><p>EfficientNet的关键创新是提出了复合缩放方法，即同时按照一定比例缩放网络的三个维度：</p><ul><li>深度（Depth）：增加网络层数</li><li>宽度（Width）：增加特征通道数</li><li>分辨率（Resolution）：增加输入图像大小</li></ul><p>不同于传统方法只关注单一维度的缩放，EfficientNet通过数学公式建立了三个维度之间的平衡关系：</p><p>$$d &#x3D; \alpha^\phi, w &#x3D; \beta^\phi, r &#x3D; \gamma^\phi$$</p><p>其中$\phi$是复合系数，而$\alpha, \beta, \gamma$是通过网格搜索确定的常数。</p><h3 id="2-2-基础网络：EfficientNet-B0"><a href="#2-2-基础网络：EfficientNet-B0" class="headerlink" title="2.2 基础网络：EfficientNet-B0"></a>2.2 基础网络：EfficientNet-B0</h3><p>EfficientNet系列的基础模型B0是通过神经架构搜索（NAS）得到的，其核心构建模块是MBConv（Mobile Inverted Bottleneck Convolution）块，这一设计源自MobileNetV2。</p><h2 id="3-网络结构"><a href="#3-网络结构" class="headerlink" title="3. 网络结构"></a>3. 网络结构</h2><p>EfficientNet的基本架构由以下组件构成：</p><ol><li><strong>stem层</strong>：初始卷积层</li><li><strong>多个MBConv块</strong>：主体特征提取部分</li><li><strong>全局池化层</strong>：特征汇聚</li><li><strong>全连接层</strong>：分类头部</li></ol><h3 id="MBConv块结构"><a href="#MBConv块结构" class="headerlink" title="MBConv块结构"></a>MBConv块结构</h3><p><img src="/image/EfficientNet.png"><br>每个MBConv块包含：</p><ul><li>扩张点卷积（1×1卷积）</li><li>深度可分离卷积（Depthwise Separable Convolution）</li><li>压缩点卷积（1×1卷积）</li><li>残差连接（当输入输出尺寸匹配时）</li><li>SE注意力模块（Squeeze-and-Excitation）</li></ul><h2 id="4-EfficientNet系列模型"><a href="#4-EfficientNet系列模型" class="headerlink" title="4. EfficientNet系列模型"></a>4. EfficientNet系列模型</h2><p>从B0到B7系列模型通过应用不同的复合缩放系数$\phi$得到：</p><table><thead><tr><th>模型</th><th>参数量</th><th>Top-1准确率 (%)</th><th>FLOPS</th><th>缩放系数</th></tr></thead><tbody><tr><td>B0</td><td>5.3M</td><td>77.1</td><td>0.39B</td><td>1.0</td></tr><tr><td>B1</td><td>7.8M</td><td>79.1</td><td>0.70B</td><td>1.1</td></tr><tr><td>B2</td><td>9.2M</td><td>80.1</td><td>1.0B</td><td>1.2</td></tr><tr><td>B3</td><td>12M</td><td>81.6</td><td>1.8B</td><td>1.4</td></tr><tr><td>B4</td><td>19M</td><td>82.9</td><td>4.2B</td><td>1.8</td></tr><tr><td>B5</td><td>30M</td><td>83.6</td><td>9.9B</td><td>2.2</td></tr><tr><td>B6</td><td>43M</td><td>84.0</td><td>19B</td><td>2.6</td></tr><tr><td>B7</td><td>66M</td><td>84.3</td><td>37B</td><td>3.1</td></tr></tbody></table><h2 id="5-性能与优势"><a href="#5-性能与优势" class="headerlink" title="5. 性能与优势"></a>5. 性能与优势</h2><ol><li><strong>高效率</strong>：在相同精度下，EfficientNet比其他CNN架构使用更少的参数和计算资源</li><li><strong>可扩展性</strong>：提供了从轻量级到大规模模型的一整套解决方案</li><li><strong>迁移学习能力</strong>：在多个下游任务中表现优异</li></ol><h2 id="6-应用场景"><a href="#6-应用场景" class="headerlink" title="6. 应用场景"></a>6. 应用场景</h2><ul><li>移动设备上的图像分类</li><li>目标检测（如EfficientDet）</li><li>语义分割</li><li>迁移学习基础模型</li></ul><h2 id="7-代码示例"><a href="#7-代码示例" class="headerlink" title="7. 代码示例"></a>7. 代码示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torchvision.models <span class="hljs-keyword">import</span> efficientnet_b0<br><br><span class="hljs-comment"># 加载预训练的EfficientNet-B0模型</span><br>model = efficientnet_b0(pretrained=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 修改分类头以适应自定义类别数</span><br>num_classes = <span class="hljs-number">10</span><br>model.classifier[<span class="hljs-number">1</span>] = nn.Linear(model.classifier[<span class="hljs-number">1</span>].in_features, num_classes)<br><br><span class="hljs-comment"># 模型推理</span><br>x = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)<br><span class="hljs-keyword">with</span> torch.no_grad():<br>    output = model(x)<br><span class="hljs-built_in">print</span>(output.shape)  <span class="hljs-comment"># torch.Size([1, 10])</span><br></code></pre></td></tr></table></figure><h2 id="8-参考文献"><a href="#8-参考文献" class="headerlink" title="8. 参考文献"></a>8. 参考文献</h2><ol><li>Tan, M., &amp; Le, Q. (2019). EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. In International Conference on Machine Learning (ICML).</li><li>Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., &amp; Chen, L. C. (2018). MobileNetV2: Inverted Residuals and Linear Bottlenecks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</li></ol>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>U-net网络</title>
    <link href="/2025/03/28/U-net%E7%BD%91%E7%BB%9C/"/>
    <url>/2025/03/28/U-net%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="U-net网络：医学图像分割的里程碑"><a href="#U-net网络：医学图像分割的里程碑" class="headerlink" title="U-net网络：医学图像分割的里程碑"></a>U-net网络：医学图像分割的里程碑</h1><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>U-net是一种卷积神经网络架构，最早由Olaf Ronneberger、Philipp Fischer和Thomas Brox在2015年提出，用于生物医学图像分割。该网络因其U形结构而得名，已成为医学图像分割领域的基准模型。U-net在训练数据有限的情况下也能获得精确的分割结果，这使其在医学影像分析中特别有价值。</p><h2 id="2-网络结构"><a href="#2-网络结构" class="headerlink" title="2. 网络结构"></a>2. 网络结构</h2><p>U-net的架构由两部分组成：收缩路径（编码器）和扩展路径（解码器），整体结构呈”U”形。<br><img src="/image/Unet.png"></p><h3 id="2-1-收缩路径（下采样）"><a href="#2-1-收缩路径（下采样）" class="headerlink" title="2.1 收缩路径（下采样）"></a>2.1 收缩路径（下采样）</h3><p>收缩路径遵循典型卷积网络的架构，从左到右依次为：</p><ul><li><strong>初始层</strong>：输入图像首先经过两次3×3卷积操作，每次卷积后接批归一化和ReLU激活函数</li><li><strong>下采样块</strong>：共4个下采样块，每个包含：<ul><li>2×2最大池化（步长为2）进行下采样，将特征图尺寸减半</li><li>两次3×3卷积（无填充），每次卷积后接批归一化和ReLU</li><li>每次下采样后特征通道数翻倍（64→128→256→512→1024）</li></ul></li><li><strong>底部</strong>：位于网络最深处，包含两个3×3卷积层和ReLU激活函数，特征通道数为1024</li></ul><h3 id="2-2-扩展路径（上采样）"><a href="#2-2-扩展路径（上采样）" class="headerlink" title="2.2 扩展路径（上采样）"></a>2.2 扩展路径（上采样）</h3><p>扩展路径包含4个上采样块，每个块包含：</p><ul><li><strong>上采样操作</strong>：使用2×2转置卷积（步长为2）进行上采样，将特征图尺寸扩大一倍</li><li><strong>特征融合</strong>：将上采样的特征与收缩路径对应层的特征图进行拼接（跳跃连接）<ul><li>这些跳跃连接对于恢复分割过程中丢失的空间信息至关重要</li></ul></li><li><strong>特征处理</strong>：拼接后进行两次3×3卷积操作，每次后接批归一化和ReLU</li><li><strong>特征减半</strong>：每次上采样后特征通道数减半（1024→512→256→128→64）</li></ul><h3 id="2-3-最终输出层"><a href="#2-3-最终输出层" class="headerlink" title="2.3 最终输出层"></a>2.3 最终输出层</h3><ul><li>网络最后使用1×1卷积层将特征图映射到所需的类别数量（C个通道）</li><li>对于二分类任务，通常使用sigmoid激活函数</li><li>对于多分类任务，使用softmax激活函数</li><li>输出特征图尺寸与输入图像相同，实现像素级的精确分割</li></ul><h3 id="2-4-尺寸变化说明"><a href="#2-4-尺寸变化说明" class="headerlink" title="2.4 尺寸变化说明"></a>2.4 尺寸变化说明</h3><p>假设输入图像为572×572：</p><ol><li>每次3×3卷积（无填充）会使特征图尺寸减少4个像素</li><li>四次下采样后，特征图尺寸变为28×28</li><li>上采样过程中恢复尺寸，最终输出为388×388</li><li>这种设计使网络能够对边界区域进行更精确的分割</li></ol><h2 id="3-U-net的创新点"><a href="#3-U-net的创新点" class="headerlink" title="3. U-net的创新点"></a>3. U-net的创新点</h2><p>U-net相比传统CNN有几个关键创新：</p><ol><li><strong>对称的U形结构</strong>：使网络能够同时捕获上下文和精确定位</li><li><strong>跳跃连接</strong>：将编码器的特征直接传递给解码器，保留空间信息</li><li><strong>无全连接层</strong>：保留空间信息，可用于任意尺寸的输入</li><li><strong>数据增强策略</strong>：通过弹性形变等技术有效扩充训练样本</li></ol><h2 id="4-应用场景"><a href="#4-应用场景" class="headerlink" title="4. 应用场景"></a>4. 应用场景</h2><p>U-net在医学图像领域有广泛应用：</p><ul><li>细胞分割</li><li>器官及病变区域分割</li><li>血管分割</li><li>肿瘤检测</li><li>眼底图像分析</li></ul><h2 id="5-U-net变种"><a href="#5-U-net变种" class="headerlink" title="5. U-net变种"></a>5. U-net变种</h2><p>随着技术发展，U-net架构衍生出多个变种：</p><ul><li><strong>3D U-net</strong>：扩展到三维数据处理</li><li><strong>V-net</strong>：用于3D医学图像分割，使用残差连接</li><li><strong>Attention U-net</strong>：引入注意力机制改善特征选择</li><li><strong>TransUnet</strong>：结合Transformer和U-net的优势</li></ul><h2 id="6-代码实现"><a href="#6-代码实现" class="headerlink" title="6. 代码实现"></a>6. 代码实现</h2><p>以下是使用PyTorch实现U-net的简化版本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DoubleConv</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, out_channels</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.double_conv = nn.Sequential(<br>            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(out_channels),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(out_channels, out_channels, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(out_channels),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.double_conv(x)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">UNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_channels, n_classes</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 下采样部分</span><br>        <span class="hljs-variable language_">self</span>.inc = DoubleConv(n_channels, <span class="hljs-number">64</span>)<br>        <span class="hljs-variable language_">self</span>.down1 = nn.Sequential(nn.MaxPool2d(<span class="hljs-number">2</span>), DoubleConv(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>))<br>        <span class="hljs-variable language_">self</span>.down2 = nn.Sequential(nn.MaxPool2d(<span class="hljs-number">2</span>), DoubleConv(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>))<br>        <span class="hljs-variable language_">self</span>.down3 = nn.Sequential(nn.MaxPool2d(<span class="hljs-number">2</span>), DoubleConv(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>))<br>        <span class="hljs-variable language_">self</span>.down4 = nn.Sequential(nn.MaxPool2d(<span class="hljs-number">2</span>), DoubleConv(<span class="hljs-number">512</span>, <span class="hljs-number">1024</span>))<br>        <br>        <span class="hljs-comment"># 上采样部分</span><br>        <span class="hljs-variable language_">self</span>.up1 = nn.ConvTranspose2d(<span class="hljs-number">1024</span>, <span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.conv1 = DoubleConv(<span class="hljs-number">1024</span>, <span class="hljs-number">512</span>)<br>        <span class="hljs-variable language_">self</span>.up2 = nn.ConvTranspose2d(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = DoubleConv(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>)<br>        <span class="hljs-variable language_">self</span>.up3 = nn.ConvTranspose2d(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.conv3 = DoubleConv(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>)<br>        <span class="hljs-variable language_">self</span>.up4 = nn.ConvTranspose2d(<span class="hljs-number">128</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.conv4 = DoubleConv(<span class="hljs-number">128</span>, <span class="hljs-number">64</span>)<br>        <span class="hljs-variable language_">self</span>.outc = nn.Conv2d(<span class="hljs-number">64</span>, n_classes, kernel_size=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 编码器路径</span><br>        x1 = <span class="hljs-variable language_">self</span>.inc(x)<br>        x2 = <span class="hljs-variable language_">self</span>.down1(x1)<br>        x3 = <span class="hljs-variable language_">self</span>.down2(x2)<br>        x4 = <span class="hljs-variable language_">self</span>.down3(x3)<br>        x5 = <span class="hljs-variable language_">self</span>.down4(x4)<br>        <br>        <span class="hljs-comment"># 解码器路径</span><br>        x = <span class="hljs-variable language_">self</span>.up1(x5)<br>        x = <span class="hljs-variable language_">self</span>.conv1(torch.cat([x4, x], dim=<span class="hljs-number">1</span>))<br>        x = <span class="hljs-variable language_">self</span>.up2(x)<br>        x = <span class="hljs-variable language_">self</span>.conv2(torch.cat([x3, x], dim=<span class="hljs-number">1</span>))<br>        x = <span class="hljs-variable language_">self</span>.up3(x)<br>        x = <span class="hljs-variable language_">self</span>.conv3(torch.cat([x2, x], dim=<span class="hljs-number">1</span>))<br>        x = <span class="hljs-variable language_">self</span>.up4(x)<br>        x = <span class="hljs-variable language_">self</span>.conv4(torch.cat([x1, x], dim=<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.outc(x)<br></code></pre></td></tr></table></figure><h2 id="7-训练技巧"><a href="#7-训练技巧" class="headerlink" title="7. 训练技巧"></a>7. 训练技巧</h2><p>成功训练U-net网络的几个关键点：</p><ol><li><strong>数据增强</strong>：旋转、缩放、弹性变形等</li><li><strong>权重初始化</strong>：使用适当的初始化方法（如He初始化）</li><li><strong>损失函数选择</strong>：常用Dice损失或交叉熵与Dice损失的组合</li><li><strong>学习率策略</strong>：通常采用衰减学习率</li><li><strong>批量归一化</strong>：帮助加速训练和提高稳定性</li></ol><h2 id="8-优缺点分析"><a href="#8-优缺点分析" class="headerlink" title="8. 优缺点分析"></a>8. 优缺点分析</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>在小数据集上表现优异</li><li>结构简单，训练效率高</li><li>可处理任意大小的输入</li><li>分割精度高，特别适合医学图像</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>对全局上下文信息获取有限</li><li>池化操作可能导致细节信息损失</li><li>在类别不平衡数据上需要特殊处理</li><li>原始结构计算量较大</li></ul><h2 id="9-结论"><a href="#9-结论" class="headerlink" title="9. 结论"></a>9. 结论</h2><p>U-net凭借其简洁而强大的设计，已成为医学图像分割任务的基础架构。虽然已有许多改进版本，但其核心思想—结合多尺度特征和保留空间信息的能力—仍然是现代图像分割网络的基石。随着深度学习技术的不断发展，U-net及其变种将继续在医学图像分析领域发挥重要作用。</p><h2 id="10-参考资料"><a href="#10-参考资料" class="headerlink" title="10. 参考资料"></a>10. 参考资料</h2><ol><li>Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation. MICCAI 2015.</li><li>Zhou Z, et al. UNet++: A nested U-Net architecture for medical image segmentation. DLMIA 2018.</li><li>Chen L C, et al. DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. IEEE TPAMI 2017.</li><li>Isensee F, et al. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature Methods 2021.</li></ol>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>迁移学习</title>
    <link href="/2025/03/27/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    <url>/2025/03/27/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="1-迁移学习详解"><a href="#1-迁移学习详解" class="headerlink" title="1. 迁移学习详解"></a>1. 迁移学习详解</h1><p>迁移学习是一种机器学习方法，它利用在一个任务上训练好的模型作为另一个相关任务的起点。这种方法特别适用于深度学习领域，因为深度神经网络需要大量数据和计算资源才能从头训练。</p><h1 id="2-迁移学习的原理"><a href="#2-迁移学习的原理" class="headerlink" title="2. 迁移学习的原理"></a>2. 迁移学习的原理</h1><p>迁移学习的核心思想是知识迁移。在深度学习模型中：</p><ul><li><strong>底层特征</strong>：卷积神经网络的前几层通常检测基础视觉特征，如边缘、颜色、纹理和简单形状，这些特征对大多数视觉任务都是通用的</li><li><strong>中层特征</strong>：中间层捕获更复杂的特征组合，如部件和物体的一部分</li><li><strong>高层特征</strong>：靠近输出的层包含特定于原始任务的高度抽象特征</li></ul><p>通过迁移学习，我们保留通用的知识（低层和中层特征），同时调整特定于任务的组件（高层特征和分类器），从而实现知识的有效迁移。</p><h2 id="2-1-迁移学习的主要步骤详解"><a href="#2-1-迁移学习的主要步骤详解" class="headerlink" title="2.1 迁移学习的主要步骤详解"></a>2.1 迁移学习的主要步骤详解</h2><ol><li><p><strong>选择合适的预训练模型</strong>：</p><ul><li>考虑源任务与目标任务的相似性</li><li>评估模型的规模和复杂性</li><li>常用预训练模型：ResNet、VGG、EfficientNet、MobileNet等</li></ul></li><li><p><strong>特征提取与模型修改</strong>：</p><ul><li>移除预训练模型的任务特定层（通常是最后的全连接层）</li><li>根据新任务添加适当的层（如针对不同类别数的新分类头）</li><li>保留预训练模型的权重作为初始化</li></ul></li><li><p><strong>模型层参数冻结策略</strong>：</p><ul><li>完全冻结：仅训练新添加的层（适用于小数据集或非常相似的任务）</li><li>部分冻结：保留前几层，微调后几层（平衡通用特征与特定特征）</li><li>逐层解冻：从顶层开始，逐步解冻并训练更多层（渐进式微调）</li></ul></li><li><p><strong>学习率设置与训练</strong>：</p><ul><li>对新添加的层使用较大学习率</li><li>对微调的预训练层使用较小学习率</li><li>采用学习率调度策略（如余弦退火）优化训练过程</li></ul></li><li><p><strong>正则化与防止过拟合</strong>：</p><ul><li>应用数据增强增加训练样本多样性</li><li>使用Dropout或权重衰减等正则化技术</li><li>采用早停策略避免过拟合</li></ul></li></ol><h2 id="2-2-迁移学习的应用场景"><a href="#2-2-迁移学习的应用场景" class="headerlink" title="2.2 迁移学习的应用场景"></a>2.2 迁移学习的应用场景</h2><p>迁移学习在多个领域展现出极高的实用价值：</p><ul><li><strong>医学影像分析</strong>：利用在自然图像上预训练的模型识别X光、CT或MRI中的病变</li><li><strong>自然语言处理</strong>：通过BERT、GPT等预训练语言模型解决特定文本任务</li><li><strong>计算机视觉</strong>：从通用物体识别转向特定领域识别（如工业缺陷检测）</li><li><strong>音频处理</strong>：语音识别、音乐分类等音频任务</li></ul><h2 id="2-3-迁移学习案例：皮肤病变分类"><a href="#2-3-迁移学习案例：皮肤病变分类" class="headerlink" title="2.3 迁移学习案例：皮肤病变分类"></a>2.3 迁移学习案例：皮肤病变分类</h2><p>假设我们要构建一个皮肤病变分类系统，以区分良性和恶性肿瘤，但只有数百张医学图像。通过迁移学习，我们可以利用在ImageNet上预训练的ResNet50，快速构建一个准确的诊断模型。</p><p>这种方法不仅大大减少了所需的训练数据量，还显著提高了模型性能。在实际医疗应用中，这可能意味着更准确的早期诊断和更好的患者预后。</p><h1 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3. 代码实现"></a>3. 代码实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models, transforms<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torchvision.datasets <span class="hljs-keyword">import</span> ImageFolder<br><br><span class="hljs-comment"># 1. 加载预训练的ResNet模型</span><br>model = models.resnet50(pretrained=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 2. 冻结所有卷积层参数</span><br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.parameters():<br>    param.requires_grad = <span class="hljs-literal">False</span><br>    <br><span class="hljs-comment"># 3. 替换最后的全连接层</span><br>num_classes = <span class="hljs-number">2</span>  <span class="hljs-comment"># 猫和狗两个类别</span><br>model.fc = nn.Linear(model.fc.in_features, num_classes)<br><br><span class="hljs-comment"># 4. 定义数据转换</span><br>data_transforms = transforms.Compose([<br>    transforms.Resize((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)),<br>    transforms.ToTensor(),<br>    transforms.Normalize([<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], [<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>])<br>])<br><br><span class="hljs-comment"># 5. 加载数据集</span><br>train_dataset = ImageFolder(<span class="hljs-string">&#x27;path/to/train_data&#x27;</span>, transform=data_transforms)<br>train_loader = DataLoader(train_dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 6. 定义损失函数和优化器</span><br>criterion = nn.CrossEntropyLoss()<br>optimizer = optim.Adam(model.fc.parameters(), lr=<span class="hljs-number">0.001</span>)<br><br><span class="hljs-comment"># 7. 训练模型</span><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>model.to(device)<br><br>num_epochs = <span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    model.train()<br>    running_loss = <span class="hljs-number">0.0</span><br>    <br>    <span class="hljs-keyword">for</span> inputs, labels <span class="hljs-keyword">in</span> train_loader:<br>        inputs, labels = inputs.to(device), labels.to(device)<br>        <br>        optimizer.zero_grad()<br>        outputs = model(inputs)<br>        loss = criterion(outputs, labels)<br>        loss.backward()<br>        optimizer.step()<br>        <br>        running_loss += loss.item() * inputs.size(<span class="hljs-number">0</span>)<br>    <br>    epoch_loss = running_loss / <span class="hljs-built_in">len</span>(train_dataset)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Epoch <span class="hljs-subst">&#123;epoch+<span class="hljs-number">1</span>&#125;</span>/<span class="hljs-subst">&#123;num_epochs&#125;</span>, Loss: <span class="hljs-subst">&#123;epoch_loss:<span class="hljs-number">.4</span>f&#125;</span>&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练完成!&quot;</span>)<br></code></pre></td></tr></table></figure><h1 id="4-迁移学习的优缺点"><a href="#4-迁移学习的优缺点" class="headerlink" title="4. 迁移学习的优缺点"></a>4. 迁移学习的优缺点</h1><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>减少训练时间和计算资源</li><li>提高小数据集上的模型性能</li><li>避免过拟合问题</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>负迁移问题（如果源任务与目标任务相差太大）</li><li>可能需要微调以适应特定任务</li><li>预训练模型的选择对性能影响较大</li></ul><h1 id="5-高级迁移学习技巧"><a href="#5-高级迁移学习技巧" class="headerlink" title="5. 高级迁移学习技巧"></a>5. 高级迁移学习技巧</h1><ol><li><strong>特征提取</strong>：仅用预训练模型作为特征提取器，冻结所有层</li><li><strong>微调</strong>：解冻部分高层，在新数据上训练</li><li><strong>逐层解冻</strong>：从顶层开始，逐步解冻更多层进行训练</li></ol><p>通过合理应用迁移学习，我们可以在有限资源和数据的情况下，构建出性能良好的深度学习模型。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>卷积神经网络</title>
    <link href="/2025/03/27/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2025/03/27/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络-CNN"><a href="#卷积神经网络-CNN" class="headerlink" title="卷积神经网络 (CNN)"></a>卷积神经网络 (CNN)</h1><p>卷积神经网络(Convolutional Neural Network, CNN)是一类特殊的深度神经网络，专为处理具有网格结构的数据而设计，特别是图像数据。自从2012年AlexNet在ImageNet竞赛中取得突破性成功以来，CNN已成为计算机视觉领域的主导技术。</p><h2 id="1-CNN的基本原理"><a href="#1-CNN的基本原理" class="headerlink" title="1. CNN的基本原理"></a>1. CNN的基本原理</h2><p>卷积神经网络的核心思想是通过卷积操作自动学习空间层次特征。与传统的多层感知机(MLP)不同，CNN具有以下特点：</p><ul><li><strong>局部连接</strong>：每个神经元只与输入数据的一个局部区域连接</li><li><strong>权值共享</strong>：同一特征图中的神经元共享相同的权重</li><li><strong>空间下采样</strong>：通过池化操作减少数据维度并提高鲁棒性</li></ul><p>这些特性使CNN在保留空间信息的同时大幅减少了参数数量，提高了计算效率和泛化能力。</p><h2 id="2-CNN的基本组成部分"><a href="#2-CNN的基本组成部分" class="headerlink" title="2. CNN的基本组成部分"></a>2. CNN的基本组成部分</h2><h3 id="2-1-卷积层-Convolutional-Layer"><a href="#2-1-卷积层-Convolutional-Layer" class="headerlink" title="2.1 卷积层(Convolutional Layer)"></a>2.1 卷积层(Convolutional Layer)</h3><p>卷积层是CNN最重要的组成部分，主要通过卷积操作提取输入数据的特征。</p><p>卷积操作过程如下：</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs mathematica">输入<span class="hljs-operator">:</span> 一个尺寸为 <span class="hljs-variable">H</span>×<span class="hljs-variable">W</span>×<span class="hljs-built_in">C</span> 的数据<span class="hljs-punctuation">(</span>高×宽×通道数<span class="hljs-punctuation">)</span><br>卷积核<span class="hljs-operator">:</span> <span class="hljs-built_in">K</span>个尺寸为 <span class="hljs-variable">Kh</span>×<span class="hljs-variable">Kw</span>×<span class="hljs-built_in">C</span> 的过滤器<br>输出<span class="hljs-operator">:</span> 一个尺寸为 <span class="hljs-variable">H</span><span class="hljs-operator">&#x27;</span>×<span class="hljs-variable">W</span><span class="hljs-operator">&#x27;</span>×<span class="hljs-built_in">K</span> 的特征图<br></code></pre></td></tr></table></figure><p>卷积过程示意：</p><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs tap">输入矩阵(5×5):        卷积核(3×3):<br>┌───┬───┬───┬───┬───┐  ┌───┬───┬───┐<br>│<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 0 </span>│<span class="hljs-number"> 0 </span>│  │<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 0 </span>│<span class="hljs-number"> 1 </span>│<br>├───┼───┼───┼───┼───┤  ├───┼───┼───┤<br>│<span class="hljs-number"> 0 </span>│<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 0 </span>│  │<span class="hljs-number"> 0 </span>│<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 0 </span>│<br>├───┼───┼───┼───┼───┤  ├───┼───┼───┤<br>│<span class="hljs-number"> 0 </span>│<span class="hljs-number"> 0 </span>│<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 1 </span>│  │<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 0 </span>│<span class="hljs-number"> 1 </span>│<br>├───┼───┼───┼───┼───┤  └───┴───┴───┘<br>│<span class="hljs-number"> 0 </span>│<span class="hljs-number"> 0 </span>│<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 0 </span>│  <br>├───┼───┼───┼───┼───┤  输出矩阵(3×3):<br>│<span class="hljs-number"> 0 </span>│<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 1 </span>│<span class="hljs-number"> 0 </span>│<span class="hljs-number"> 0 </span>│  ┌───┬───┬───┐<br>└───┴───┴───┴───┴───┘  │<span class="hljs-number"> 4 </span>│<span class="hljs-number"> 3 </span>│<span class="hljs-number"> 4 </span>│<br>                       ├───┼───┼───┤<br>                       │<span class="hljs-number"> 2 </span>│<span class="hljs-number"> 4 </span>│<span class="hljs-number"> 3 </span>│<br>                       ├───┼───┼───┤<br>                       │<span class="hljs-number"> 2 </span>│<span class="hljs-number"> 3 </span>│<span class="hljs-number"> 3 </span>│<br>                       └───┴───┴───┘    <br></code></pre></td></tr></table></figure><p>卷积过程计算示例(左上角):</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">1</span>×<span class="hljs-number">1</span> + <span class="hljs-number">1</span>×<span class="hljs-number">0</span> + <span class="hljs-number">1</span>×<span class="hljs-number">1</span> +<br><span class="hljs-attribute">0</span>×<span class="hljs-number">0</span> + <span class="hljs-number">1</span>×<span class="hljs-number">1</span> + <span class="hljs-number">1</span>×<span class="hljs-number">0</span> +<br><span class="hljs-attribute">0</span>×<span class="hljs-number">1</span> + <span class="hljs-number">0</span>×<span class="hljs-number">0</span> + <span class="hljs-number">1</span>×<span class="hljs-number">1</span> = <span class="hljs-number">4</span><br></code></pre></td></tr></table></figure><p>卷积核在原始图像上滑动，每个位置执行”乘积求和”操作。卷积过程自动提取特征，如边缘、纹理和形状。不同的卷积核可以检测不同的特征模式，通过学习这些卷积核的权重，CNN能够适应各种视觉识别任务。</p><p>卷积的数学表示：</p><p>$$F(i,j) &#x3D; \sum_{m&#x3D;0}^{k_h-1} \sum_{n&#x3D;0}^{k_w-1} I(i+m, j+n) \cdot K(m,n)$$</p><p>其中$F$是输出特征图，$I$是输入图像，$K$是卷积核。</p><h3 id="2-2-激活函数层"><a href="#2-2-激活函数层" class="headerlink" title="2.2 激活函数层"></a>2.2 激活函数层</h3><p>卷积层之后通常跟随一个非线性激活函数，常用的有：</p><ul><li><strong>ReLU (Rectified Linear Unit)</strong>: $f(x) &#x3D; \max(0, x)$</li><li><strong>Leaky ReLU</strong>: $f(x) &#x3D; \max(0.01x, x)$</li><li><strong>ELU (Exponential Linear Unit)</strong>: $f(x) &#x3D; \begin{cases} x, &amp; \text{if } x &gt; 0 \ \alpha(e^x - 1), &amp; \text{if } x \leq 0 \end{cases}$</li></ul><p>激活函数的引入为网络带来非线性，提高了模型的表达能力。</p><h3 id="2-3-池化层-Pooling-Layer"><a href="#2-3-池化层-Pooling-Layer" class="headerlink" title="2.3 池化层(Pooling Layer)"></a>2.3 池化层(Pooling Layer)</h3><p>池化层的主要功能是减少特征图的空间尺寸，降低计算复杂度，同时提高一定的位移不变性。常见的池化操作有：</p><ul><li><strong>最大池化(Max Pooling)</strong>: 取窗口内的最大值</li><li><strong>平均池化(Average Pooling)</strong>: 计算窗口内像素的平均值</li></ul><h3 id="2-4-全连接层-Fully-Connected-Layer"><a href="#2-4-全连接层-Fully-Connected-Layer" class="headerlink" title="2.4 全连接层(Fully Connected Layer)"></a>2.4 全连接层(Fully Connected Layer)</h3><p>在经过多个卷积和池化层后，特征图被展平为一维向量，然后通过全连接层进行分类或回归。这与传统神经网络的操作类似，每个神经元与前一层的所有神经元相连。</p><h2 id="3-经典CNN架构"><a href="#3-经典CNN架构" class="headerlink" title="3. 经典CNN架构"></a>3. 经典CNN架构</h2><h3 id="3-1-LeNet-5"><a href="#3-1-LeNet-5" class="headerlink" title="3.1 LeNet-5"></a>3.1 LeNet-5</h3><p>由Yann LeCun在1998年提出，是最早的CNN架构之一，用于手写数字识别。</p><p>结构：</p><ul><li>输入: 32×32 灰度图像</li><li>两个卷积层和池化层的组合</li><li>三个全连接层</li></ul><h3 id="3-2-AlexNet"><a href="#3-2-AlexNet" class="headerlink" title="3.2 AlexNet"></a>3.2 AlexNet</h3><p>2012年ImageNet竞赛冠军，标志着深度学习时代的到来。</p><p>主要创新：</p><ul><li>使用ReLU激活函数</li><li>使用Dropout防止过拟合</li><li>使用数据增强技术</li><li>在GPU上进行训练</li></ul><h3 id="3-3-VGG"><a href="#3-3-VGG" class="headerlink" title="3.3 VGG"></a>3.3 VGG</h3><p>由牛津大学提出，以简洁的网络结构著称。</p><p>特点：</p><ul><li>使用3×3的小卷积核</li><li>网络深度达到16-19层</li><li>结构规整，便于理解与修改</li></ul><h3 id="3-4-GoogLeNet-Inception"><a href="#3-4-GoogLeNet-Inception" class="headerlink" title="3.4 GoogLeNet(Inception)"></a>3.4 GoogLeNet(Inception)</h3><p>谷歌提出的网络架构，引入了Inception模块。</p><p>特点：</p><ul><li>使用1×1卷积减少计算量</li><li>并行使用多种尺寸的卷积核</li><li>中间层添加辅助分类器</li></ul><h3 id="3-5-ResNet"><a href="#3-5-ResNet" class="headerlink" title="3.5 ResNet"></a>3.5 ResNet</h3><p>微软研究院提出的残差网络，通过残差连接解决了深层网络的梯度消失问题。</p><p>残差块结构：</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">x ---&gt; Conv ---&gt; BN ---&gt; ReLU ---&gt; Conv ---&gt; BN ---&gt;+---&gt; ReLU ---&gt; output<br><span class="hljs-section">|                                                    ^</span><br><span class="hljs-section">+----------------------------------------------------+</span><br></code></pre></td></tr></table></figure><h2 id="4-CNN的简单实现"><a href="#4-CNN的简单实现" class="headerlink" title="4. CNN的简单实现"></a>4. CNN的简单实现</h2><p>下面是一个使用PyTorch实现的简单卷积神经网络示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, transforms<br><br><span class="hljs-comment"># 定义CNN模型</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleCNN</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(SimpleCNN, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-comment"># 第一个卷积层，输入通道1，输出通道32，卷积核大小3x3</span><br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 第二个卷积层，输入通道32，输出通道64，卷积核大小3x3</span><br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 池化层</span><br>        <span class="hljs-variable language_">self</span>.pool = nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 全连接层</span><br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">7</span> * <span class="hljs-number">7</span>, <span class="hljs-number">128</span>)<br>        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">128</span>, <span class="hljs-number">10</span>)<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(<span class="hljs-number">0.25</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 第一个卷积层 + 激活函数 + 池化</span><br>        x = F.relu(<span class="hljs-variable language_">self</span>.conv1(x))<br>        x = <span class="hljs-variable language_">self</span>.pool(x)<br>        <span class="hljs-comment"># 第二个卷积层 + 激活函数 + 池化</span><br>        x = F.relu(<span class="hljs-variable language_">self</span>.conv2(x))<br>        x = <span class="hljs-variable language_">self</span>.pool(x)<br>        <span class="hljs-comment"># 展平</span><br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">64</span> * <span class="hljs-number">7</span> * <span class="hljs-number">7</span>)<br>        <span class="hljs-comment"># 全连接层</span><br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br>        x = <span class="hljs-variable language_">self</span>.dropout(x)<br>        x = <span class="hljs-variable language_">self</span>.fc2(x)<br>        <span class="hljs-keyword">return</span> F.log_softmax(x, dim=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 训练函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_model</span>(<span class="hljs-params">model, device, train_loader, optimizer, epoch</span>):<br>    model.train()<br>    <span class="hljs-keyword">for</span> batch_idx, (data, target) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        data, target = data.to(device), target.to(device)<br>        optimizer.zero_grad()<br>        output = model(data)<br>        loss = F.nll_loss(output, target)<br>        loss.backward()<br>        optimizer.step()<br>        <span class="hljs-keyword">if</span> batch_idx % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Train Epoch: <span class="hljs-subst">&#123;epoch&#125;</span> [<span class="hljs-subst">&#123;batch_idx * <span class="hljs-built_in">len</span>(data)&#125;</span>/<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(train_loader.dataset)&#125;</span>]\tLoss: <span class="hljs-subst">&#123;loss.item():<span class="hljs-number">.6</span>f&#125;</span>&#x27;</span>)<br><br><span class="hljs-comment"># 测试函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_model</span>(<span class="hljs-params">model, device, test_loader</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    test_loss = <span class="hljs-number">0</span><br>    correct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> test_loader:<br>            data, target = data.to(device), target.to(device)<br>            output = model(data)<br>            test_loss += F.nll_loss(output, target, reduction=<span class="hljs-string">&#x27;sum&#x27;</span>).item()<br>            pred = output.argmax(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>            correct += pred.eq(target.view_as(pred)).<span class="hljs-built_in">sum</span>().item()<br>    <br>    test_loss /= <span class="hljs-built_in">len</span>(test_loader.dataset)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;\nTest set: Average loss: <span class="hljs-subst">&#123;test_loss:<span class="hljs-number">.4</span>f&#125;</span>, Accuracy: <span class="hljs-subst">&#123;correct&#125;</span>/<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(test_loader.dataset)&#125;</span> (<span class="hljs-subst">&#123;<span class="hljs-number">100.</span> * correct / <span class="hljs-built_in">len</span>(test_loader.dataset):<span class="hljs-number">.2</span>f&#125;</span>%)\n&#x27;</span>)<br><br><span class="hljs-comment"># 主函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-comment"># 检查是否有可用的GPU</span><br>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>    <br>    <span class="hljs-comment"># 准备数据</span><br>    transform = transforms.Compose([<br>        transforms.ToTensor(),<br>        transforms.Normalize((<span class="hljs-number">0.1307</span>,), (<span class="hljs-number">0.3081</span>,))<br>    ])<br>    <br>    <span class="hljs-comment"># 加载MNIST数据集</span><br>    train_dataset = datasets.MNIST(<span class="hljs-string">&#x27;../data&#x27;</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>, transform=transform)<br>    test_dataset = datasets.MNIST(<span class="hljs-string">&#x27;../data&#x27;</span>, train=<span class="hljs-literal">False</span>, transform=transform)<br>    <br>    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br>    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=<span class="hljs-number">1000</span>)<br>    <br>    <span class="hljs-comment"># 创建模型</span><br>    model = SimpleCNN().to(device)<br>    optimizer = optim.Adam(model.parameters(), lr=<span class="hljs-number">0.001</span>)<br>    <br>    <span class="hljs-comment"># 训练和测试模型</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>):<br>        train_model(model, device, train_loader, optimizer, epoch)<br>        test_model(model, device, test_loader)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main()<br></code></pre></td></tr></table></figure><p>这个简单的CNN实现可用于MNIST手写数字识别，包含了两个卷积层、两个池化层和两个全连接层。模型结构简洁，易于理解，是入门CNN的理想示例。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多层感知机</title>
    <link href="/2025/03/27/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <url>/2025/03/27/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="1-多层感知机-MLP-简介"><a href="#1-多层感知机-MLP-简介" class="headerlink" title="1. 多层感知机(MLP)简介"></a>1. 多层感知机(MLP)简介</h1><p>多层感知机是一种前馈式人工神经网络，是深度学习领域中最基础但功能强大的模型之一。与单层感知机相比，多层感知机能够学习并解决非线性问题，这使其成为现代神经网络架构的基石。</p><h2 id="1-1-原理与结构"><a href="#1-1-原理与结构" class="headerlink" title="1.1 原理与结构"></a>1.1 原理与结构</h2><p>多层感知机由三类层组成：</p><ul><li><strong>输入层</strong>：接收外部数据，每个节点对应一个输入特征</li><li><strong>隐藏层</strong>：一个或多个中间处理层，执行非线性变换，提取特征表示</li><li><strong>输出层</strong>：产生最终预测结果，节点数通常与任务类别数量相关</li></ul><p>每层由多个神经元组成，相邻层的神经元之间全连接。数学上，MLP可表示为：</p><p>$$h^{(1)} &#x3D; \sigma(W^{(1)}x + b^{(1)})$$<br>$$h^{(2)} &#x3D; \sigma(W^{(2)}h^{(1)} + b^{(2)})$$<br>$$\ldots$$<br>$$\hat{y} &#x3D; \sigma(W^{(L)}h^{(L-1)} + b^{(L)})$$</p><p>其中，$\sigma$是激活函数(如ReLU、Sigmoid等)，$W$是权重矩阵，$b$是偏置向量。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>激活函数引入非线性变换，常用的激活函数包括：</p><ul><li><strong>ReLU</strong>: $f(x) &#x3D; \max(0, x)$，计算高效，有效缓解梯度消失问题</li><li><strong>Sigmoid</strong>: $f(x) &#x3D; \frac{1}{1 + e^{-x}}$，输出范围(0,1)，适合二分类问题</li><li><strong>Tanh</strong>: $f(x) &#x3D; \frac{e^x - e^{-x}}{e^x + e^{-x}}$，输出范围(-1,1)，中心对称特性好</li></ul><h2 id="1-2-训练过程"><a href="#1-2-训练过程" class="headerlink" title="1.2 训练过程"></a>1.2 训练过程</h2><p>MLP通过反向传播算法训练：</p><ol><li><strong>前向传播</strong>：输入数据通过网络，计算预测输出</li><li><strong>计算损失</strong>：根据预测值和实际值计算损失函数值（如交叉熵损失、均方误差等）</li><li><strong>反向传播</strong>：计算损失函数对各参数的梯度，使用链式法则传递误差</li><li><strong>参数更新</strong>：使用优化算法（如梯度下降法、Adam等）更新参数</li></ol><h3 id="a-常用损失函数"><a href="#a-常用损失函数" class="headerlink" title="a)常用损失函数"></a>a)常用损失函数</h3><ul><li>**均方误差(MSE)**：回归问题常用，$L &#x3D; \frac{1}{n}\sum_{i&#x3D;1}^{n}(y_i - \hat{y}_i)^2$</li><li><strong>交叉熵损失</strong>：分类问题常用，$L &#x3D; -\sum_{i&#x3D;1}^{n} y_i\log(\hat{y}_i)$</li></ul><h3 id="b-优化算法"><a href="#b-优化算法" class="headerlink" title="b)优化算法"></a>b)优化算法</h3><ul><li>**随机梯度下降(SGD)**：每次使用一小批数据更新参数</li><li><strong>Adam</strong>：结合动量和自适应学习率的优化方法</li><li><strong>RMSprop</strong>：自适应调整不同参数的学习率</li></ul><h2 id="1-3-应用场景"><a href="#1-3-应用场景" class="headerlink" title="1.3 应用场景"></a>1.3 应用场景</h2><p>MLP广泛应用于：</p><ul><li><strong>图像识别</strong>：手写数字识别、简单物体分类</li><li><strong>语音识别</strong>：语音特征提取与模式匹配</li><li><strong>自然语言处理</strong>：文本分类、情感分析</li><li><strong>金融预测</strong>：股票价</li></ul><h2 id="实现示例"><a href="#实现示例" class="headerlink" title="实现示例"></a>实现示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MLP</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size</span>):<br>        <span class="hljs-built_in">super</span>(MLP, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.layer1 = nn.Linear(input_size, hidden_size)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.layer2 = nn.Linear(hidden_size, output_size)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        out = <span class="hljs-variable language_">self</span>.layer1(x)<br>        out = <span class="hljs-variable language_">self</span>.relu(out)<br>        out = <span class="hljs-variable language_">self</span>.layer2(out)<br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>朴素贝叶斯法</title>
    <link href="/2025/03/26/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"/>
    <url>/2025/03/26/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="朴素贝叶斯法：简单而强大的概率分类器"><a href="#朴素贝叶斯法：简单而强大的概率分类器" class="headerlink" title="朴素贝叶斯法：简单而强大的概率分类器"></a>朴素贝叶斯法：简单而强大的概率分类器</h1><p>朴素贝叶斯是机器学习中一种基于概率的分类方法，它像一位经验丰富的法官，通过已有的证据来推断最可能的结论。这种方法简单高效，特别适合处理文本分类等任务。</p><h2 id="1-基本思想"><a href="#1-基本思想" class="headerlink" title="1. 基本思想"></a>1. 基本思想</h2><p>朴素贝叶斯的核心是<strong>贝叶斯定理</strong>，它告诉我们如何根据已知信息更新我们的信念。</p><p>想象一下：</p><ul><li>你收到一封邮件，想知道它是否是垃圾邮件</li><li>你观察到这封邮件中有”免费”、”中奖”等词语</li></ul><p>朴素贝叶斯会问：</p><ol><li>一般来说，垃圾邮件占所有邮件的比例是多少？（先验概率）</li><li>如果是垃圾邮件，出现这些词的可能性有多大？（似然概率）</li><li>综合以上信息，这封邮件是垃圾邮件的可能性有多大？（后验概率）</li></ol><p>用公式表示：</p><p>$$P(垃圾邮件|观察到的词) &#x3D; \frac{P(观察到的词|垃圾邮件)P(垃圾邮件)}{P(观察到的词)}$$</p><p>“朴素”在哪里？朴素贝叶斯假设所有特征（如邮件中的每个词）都相互独立。这就像假设”免费”这个词出现的概率不受”中奖”这个词是否出现的影响。</p><h2 id="2-朴素贝叶斯分类器"><a href="#2-朴素贝叶斯分类器" class="headerlink" title="2. 朴素贝叶斯分类器"></a>2. 朴素贝叶斯分类器</h2><p>分类时，朴素贝叶斯会计算”这封邮件是垃圾邮件的概率”和”这封邮件是正常邮件的概率”，然后选择概率较大的类别。</p><p>$$\hat{y} &#x3D; \arg\max_{y} P(y) \prod_{i&#x3D;1}^{n} P(x_i|y)$$</p><p>举例：判断一封含有”免费”和”朋友”的邮件</p><p>计算是垃圾邮件的概率：</p><ul><li>垃圾邮件的先验概率：30%</li><li>“免费”出现在垃圾邮件中的概率：80%</li><li>“朋友”出现在垃圾邮件中的概率：20%</li><li>结果：30% × 80% × 20% &#x3D; 4.8%</li></ul><p>计算是正常邮件的概率：</p><ul><li>正常邮件的先验概率：70% </li><li>“免费”出现在正常邮件中的概率：10%</li><li>“朋友”出现在正常邮件中的概率：60%</li><li>结果：70% × 10% × 60% &#x3D; 4.2%</li></ul><p>结论：这封邮件更可能是垃圾邮件（4.8% &gt; 4.2%）</p><h2 id="3-常见的朴素贝叶斯模型"><a href="#3-常见的朴素贝叶斯模型" class="headerlink" title="3. 常见的朴素贝叶斯模型"></a>3. 常见的朴素贝叶斯模型</h2><h3 id="3-1-高斯朴素贝叶斯"><a href="#3-1-高斯朴素贝叶斯" class="headerlink" title="3.1 高斯朴素贝叶斯"></a>3.1 高斯朴素贝叶斯</h3><p>适用于连续数值特征，比如身高、体重、温度等。</p><p>例子：根据身高、体重预测性别</p><ul><li>假设男性身高服从均值为175cm、方差为36的正态分布</li><li>如果观察到一个人身高180cm，可以计算这个身高在男性分布中的概率</li></ul><p>$$P(身高&#x3D;180|男性) &#x3D; \frac{1}{\sqrt{2\pi\cdot 36}} \exp\left(-\frac{(180 - 175)^2}{2\cdot 36}\right)$$</p><h3 id="3-2-多项式朴素贝叶斯"><a href="#3-2-多项式朴素贝叶斯" class="headerlink" title="3.2 多项式朴素贝叶斯"></a>3.2 多项式朴素贝叶斯</h3><p>适用于计数型特征，特别是文本分类。</p><p>例子：根据单词出现次数分类新闻</p><ul><li>体育新闻中”比赛”、”得分”、”球员”等词出现频率高</li><li>政治新闻中”政策”、”国家”、”经济”等词出现频率高</li></ul><h3 id="3-3-伯努利朴素贝叶斯"><a href="#3-3-伯努利朴素贝叶斯" class="headerlink" title="3.3 伯努利朴素贝叶斯"></a>3.3 伯努利朴素贝叶斯</h3><p>适用于二元特征（有&#x2F;无）的情况。</p><p>例子：判断电影类型</p><ul><li>是否包含爆炸场景？(是&#x2F;否)</li><li>是否有浪漫情节？(是&#x2F;否)</li><li>是否出现外星生物？(是&#x2F;否)</li></ul><h2 id="4-算法优缺点"><a href="#4-算法优缺点" class="headerlink" title="4. 算法优缺点"></a>4. 算法优缺点</h2><p><strong>优点：</strong></p><ul><li><strong>速度快</strong>：训练和预测都很快，就像快速浏览简历而不是深入阅读</li><li><strong>小样本也能用</strong>：即使只有少量数据也能工作得不错</li><li><strong>容易理解</strong>：模型逻辑清晰，像是基于几个简单规则的决策</li><li><strong>处理高维数据</strong>：即使有成千上万个特征（如文章中的词汇量）也能高效处理</li></ul><p><strong>缺点：</strong></p><ul><li><strong>独立性假设过强</strong>：特征间往往有联系，比如”纽约”和”曼哈顿”同时出现的概率比独立计算的要高</li><li><strong>对极端值敏感</strong>：一个非常罕见的特征可能过度影响结果</li><li><strong>零概率问题</strong>：如果某个词在训练数据的某类别中从未出现过，会导致整个概率变为零</li></ul><h2 id="5-实际应用"><a href="#5-实际应用" class="headerlink" title="5. 实际应用"></a>5. 实际应用</h2><ul><li><strong>垃圾邮件过滤</strong>：Gmail基于邮件内容判断是否为垃圾邮件</li><li><strong>新闻分类</strong>：自动将新闻分到体育、政治、科技等类别</li><li><strong>情感分析</strong>：判断产品评论是正面还是负面</li><li><strong>疾病诊断</strong>：根据各种症状判断最可能的疾病</li><li><strong>推荐系统</strong>：简单的内容推荐，如”喜欢这本书的人也喜欢…”</li></ul><h2 id="6-代码示例"><a href="#6-代码示例" class="headerlink" title="6. 代码示例"></a>6. 代码示例</h2><p>下面是使用Python实现的一个新闻分类例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_20newsgroups<br><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> CountVectorizer<br><span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> MultinomialNB<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score, classification_report<br><br><span class="hljs-comment"># 加载数据（四个不同主题的新闻组）</span><br>categories = [<span class="hljs-string">&#x27;alt.atheism&#x27;</span>, <span class="hljs-string">&#x27;soc.religion.christian&#x27;</span>, <span class="hljs-string">&#x27;comp.graphics&#x27;</span>, <span class="hljs-string">&#x27;sci.med&#x27;</span>]<br>data = fetch_20newsgroups(subset=<span class="hljs-string">&#x27;all&#x27;</span>, categories=categories, shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 特征提取：将文本转为词频向量（词袋模型）</span><br>vectorizer = CountVectorizer(stop_words=<span class="hljs-string">&#x27;english&#x27;</span>, max_features=<span class="hljs-number">1000</span>)  <span class="hljs-comment"># 忽略常见英文虚词，只保留1000个最常见词</span><br>X = vectorizer.fit_transform(data.data)  <span class="hljs-comment"># 转换为词频矩阵</span><br>y = data.target  <span class="hljs-comment"># 类别标签</span><br><br><span class="hljs-comment"># 划分训练集和测试集（80%训练，20%测试）</span><br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># 训练朴素贝叶斯模型</span><br>clf = MultinomialNB(alpha=<span class="hljs-number">1.0</span>)  <span class="hljs-comment"># alpha是平滑参数，避免零概率问题</span><br>clf.fit(X_train, y_train)<br><br><span class="hljs-comment"># 预测并评估</span><br>y_pred = clf.predict(X_test)<br>accuracy = accuracy_score(y_test, y_pred)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;准确率: <span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(classification_report(y_test, y_pred, target_names=data.target_names))<br><br><span class="hljs-comment"># 演示预测</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_category</span>(<span class="hljs-params">text</span>):<br>    text_vector = vectorizer.transform([text])<br>    category = data.target_names[clf.predict(text_vector)[<span class="hljs-number">0</span>]]<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;预测类别: <span class="hljs-subst">&#123;category&#125;</span>&quot;</span><br><br><span class="hljs-comment"># 测试几个例子</span><br><span class="hljs-built_in">print</span>(predict_category(<span class="hljs-string">&quot;God created the universe and loves everyone.&quot;</span>))<br><span class="hljs-built_in">print</span>(predict_category(<span class="hljs-string">&quot;The new graphics card can render 3D images quickly.&quot;</span>))<br><span class="hljs-built_in">print</span>(predict_category(<span class="hljs-string">&quot;The patient shows symptoms of high fever and cough.&quot;</span>))<br></code></pre></td></tr></table></figure><p>通过这个简单的朴素贝叶斯模型，我们就能够相当准确地对新闻文本进行分类，展示了这种简单算法的强大之处。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K邻近法</title>
    <link href="/2025/03/25/K%E9%82%BB%E8%BF%91%E6%B3%95/"/>
    <url>/2025/03/25/K%E9%82%BB%E8%BF%91%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><p>kNN是一种基本分类与回归方法，这里讨论分类问题的KNN方法。kNN假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其 $k$ 个最近邻的训练实例的类别，通过多数表决等方式进行预测。kNN利用训练数据集对特征向量空间进行划分，并作为其分类的”模型”。三个基本要素是k值的选择、距离度量以及分类决策规则。</p><h1 id="2-k邻近算法-简洁版"><a href="#2-k邻近算法-简洁版" class="headerlink" title="2. k邻近算法(简洁版)"></a>2. k邻近算法(简洁版)</h1><h2 id="2-1算法"><a href="#2-1算法" class="headerlink" title="2.1算法"></a>2.1算法</h2><p>(1)根据给定的距离度量，在训练集T中找出与 $x_i$ 最近邻的 $k$ 个点，涵盖这 $k$ 个点的邻域记作 $N_k(x)$.</p><p>(2)在 $N_k(x)$中根据分类决策规则(如多数表决)决定 $x$ 的类别 $y$：</p><p>$$y &#x3D; \arg \max_{c_j} \sum_{x_i \in N_k(x)} I(y_i &#x3D; c_j), \quad i &#x3D; 1, 2, \dots, N; \quad j &#x3D; 1, 2, \dots, K$$</p><p>其中，$I$ 为指示函数。<br>k近邻法的特殊情况是$k$ &#x3D;1的情形，称为最近邻算法。<br>没有显式的学习过程。</p><h2 id="2-2模型"><a href="#2-2模型" class="headerlink" title="2.2模型"></a>2.2模型</h2><p>当训练集、距离度量、$k$ 值及分类决策规则确定后，对于一个新的输入实例，它所属的类唯一地确定。特征空间中，对于每个训练点 $x_i$，距离该点比其他点更近的所有点组成一个区域，叫做单元；每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。最邻近法将实例 $x_i$ 的类 $y_i$ 作为其单元中所有点的类标记。<br><img src="/image/knn1.png"></p><h3 id="a-距离度量"><a href="#a-距离度量" class="headerlink" title="a)距离度量"></a>a)距离度量</h3><p>特征空间中两个实例点的距离是两个实例点相似程度的反映。kNN模型的特征空间一般是n维实数向量空间$R^n$。使用的距离是欧氏距离，也可以是其它距离。</p><h3 id="b-k值的选择"><a href="#b-k值的选择" class="headerlink" title="b)k值的选择"></a>b)k值的选择</h3><p>k值的选择会对k近邻法的结果产生重大影响。</p><p>如果选择较小的 $k$ 值，就相当于用较小的邻域中的训练实例进行预测，”学习”的近似误差（approximation error）会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是”学习”的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说， $k$ 值的减小就意味着整体模型变得复杂，容易发生过拟合。</p><p>如果选择较大的 $k$ 值，就相当于用较大邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误。k值的增大就意味着整体的模型变得简单。</p><p>如果 $k&#x3D;N$，那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。<br>在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。</p><h3 id="c-分类决策规则"><a href="#c-分类决策规则" class="headerlink" title="c)分类决策规则"></a>c)分类决策规则</h3><p>$k$ 近邻法中的分类决策规则往往是多数表决，即由输入实例的 $k$ 个邻近的训练实例中的多数类决定输入实例的类。</p><p>多数表决规则（majority voting rule）有如下解释：如果分类的损失函数为 0-1 损失函数，分类函数为</p><p>$$f : \mathbb{R}^n \to {c_1, c_2, \cdots, c_K}$$</p><p>那么误分类的概率是</p><p>$$P(Y \neq f(X)) &#x3D; 1 - P(Y &#x3D; f(X))$$</p><p>对给定的实例 $x \in \mathcal{X}$，其最近邻的 $k$ 个训练实例点构成集合 $N_k(x)$。如果涵盖 $N_k(x)$ 的区域的类别是 $c_j$，那么误分类率是</p><p>$$\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \neq c_j) &#x3D; 1 - \frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i &#x3D; c_j)$$</p><p>要使误分类率最小即经验风险最小，就要使<br>$$\sum_{x_i \in N_k(x)} I(y_i &#x3D; c_j)$$<br>最大，所以多数表决规则等价于经验风险最小化。</p><h1 id="3-kNN的实现：kd树"><a href="#3-kNN的实现：kd树" class="headerlink" title="3. kNN的实现：kd树"></a>3. kNN的实现：kd树</h1><h2 id="3-1-kd树介绍"><a href="#3-1-kd树介绍" class="headerlink" title="3.1 kd树介绍"></a>3.1 kd树介绍</h2><p>kd树(k-dimensional tree)是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分，其每个结点对应于k维空间划分中的一个超矩形区域。</p><p>kd树的构造目的是实现高效的最近邻搜索，特别是在k维空间中的数据点很多时，直接计算距离的方法效率低下。</p><h2 id="3-2-kd树的构造"><a href="#3-2-kd树的构造" class="headerlink" title="3.2 kd树的构造"></a>3.2 kd树的构造</h2><p>kd树的构造过程是一个递归的过程：</p><ol><li>在k维空间中选择一个坐标轴j进行切分（通常按轮流选择的规则）</li><li>在选定的坐标轴上，选择切分点，将数据集划分为两部分</li><li>对划分的两部分分别递归构建左右子树</li></ol><p>常用的切分方式是选择当前维度上的中位数作为切分点，这样可以保证构造出的kd树是平衡的。</p><p>构造算法的数学描述如下：</p><p>$$\text{BuildKdTree}(P, depth):$$</p><p>$$<br>\begin{cases}<br>\text{如果 } P \text{ 为空，返回 } \text{null}\<br>\text{axis} &#x3D; \text{depth} \bmod k\<br>\text{按 } \text{axis} \text{ 维度对 } P \text{ 中的点排序}\<br>\text{median} &#x3D; \text{在 } \text{axis} \text{ 维度上的中位数点}\<br>\text{将 } \text{median} \text{ 作为根节点}\<br>\text{root.left} &#x3D; \text{BuildKdTree}(P_{\text{left}}, \text{depth}+1)\<br>\text{root.right} &#x3D; \text{BuildKdTree}(P_{\text{right}}, \text{depth}+1)\<br>\text{返回 } \text{root}<br>\end{cases}<br>$$</p><p>其中，$P_{\text{left}}$ 是 $\text{axis}$ 维度上小于 $\text{median}$ 的点集，$P_{\text{right}}$ 是大于 $\text{median}$ 的点集。</p><h2 id="3-3-kd树的最近邻搜索"><a href="#3-3-kd树的最近邻搜索" class="headerlink" title="3.3 kd树的最近邻搜索"></a>3.3 kd树的最近邻搜索</h2><p>kd树最核心的操作是搜索给定点的最近邻。算法步骤如下：</p><ol><li>从根节点开始，递归地向下遍历树</li><li>在每个节点，将目标点与当前节点比较</li><li>根据比较结果选择一个子树优先遍历</li><li>回溯时检查另一子树是否可能包含更近的点</li><li>如果可能，遍历另一子树</li></ol><p>最近邻搜索的数学表达为：</p><p>$$\text{NearestNeighbor}(\text{node}, \text{target}, \text{best}):$$</p><p>$$<br>\begin{cases}<br>\text{如果 } \text{node} \text{ 为空，返回 } \text{best}\<br>d &#x3D; \text{Distance}(\text{target}, \text{node.point})\<br>\text{如果 } d &lt; \text{Distance}(\text{target}, \text{best}) \text{ 则 } \text{best} &#x3D; \text{node.point}\<br>\text{axis} &#x3D; \text{node} \text{ 的划分维度}\<br>\text{如果 } \text{target}[\text{axis}] &lt; \text{node.point}[\text{axis}]\<br>\quad \text{first} &#x3D; \text{node.left}\<br>\quad \text{second} &#x3D; \text{node.right}\<br>\text{否则}\<br>\quad \text{first} &#x3D; \text{node.right}\<br>\quad \text{second} &#x3D; \text{node.left}\<br>\text{best} &#x3D; \text{NearestNeighbor}(\text{first}, \text{target}, \text{best})\<br>\text{如果 } |\text{target}[\text{axis}] - \text{node.point}[\text{axis}]| &lt; \text{Distance}(\text{target}, \text{best})\<br>\quad \text{best} &#x3D; \text{NearestNeighbor}(\text{second}, \text{target}, \text{best})\<br>\text{返回 } \text{best}<br>\end{cases}<br>$$</p><h2 id="3-4-kd树的复杂度分析"><a href="#3-4-kd树的复杂度分析" class="headerlink" title="3.4 kd树的复杂度分析"></a>3.4 kd树的复杂度分析</h2><ul><li>构建kd树的时间复杂度：$O(n\log n)$，其中$n$是数据点的数量</li><li>平衡kd树的空间复杂度：$O(n)$</li><li>最近邻搜索的平均时间复杂度：$O(\log n)$</li><li>最近邻搜索的最坏情况时间复杂度：$O(n)$（当数据分布极不均匀时）</li></ul><p>kd树在低维空间（通常 $k&lt;20$ ）中表现良好，但在高维空间中由于”维度灾难”问题，其效率会急剧下降。在高维情况下，可能需要考虑其他数据结构如哈希或近似最近邻算法。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>感知机</title>
    <link href="/2025/03/25/%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <url>/2025/03/25/%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><p>感知机是一种用于二分类的线性分类模型，输入为实例的特征向量，输出为实例的类别，取$+1$和$-1$二值。感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于判别模型。感知机的目的是训练出将输入数据进行二分类的超平面；因此，可以导入基于误分类的损失函数，并利用梯度下降算法对损失函数进行极小化，以此得到感知机模型。</p><h1 id="2-感知机-简洁版"><a href="#2-感知机-简洁版" class="headerlink" title="2. 感知机(简洁版)"></a>2. 感知机(简洁版)</h1><h2 id="2-1模型"><a href="#2-1模型" class="headerlink" title="2.1模型"></a>2.1模型</h2><p>定义由输入空间到输出空间的函数：</p><p>$$f(x)&#x3D;sign(wx+b)$$</p><p>称感知机。其中，权重$w$和偏置$b$是感知机的参数，$sign$是符号函数。感知机是一种线性分类模型，属于判别模型。<br><img src="/image/Perceptron1.png"><br>由上可知，对于感知机，输出值无非不过两种情况：</p><p>$$wx+b&gt;0&#x2F;wx+b&lt;0$$</p><p>而我们要找的超平面为</p><p>$$wx+b&#x3D;0$$</p><p>注意，数据集必须要线性可分。</p><h2 id="2-2学习策略"><a href="#2-2学习策略" class="headerlink" title="2.2学习策略"></a>2.2学习策略</h2><h3 id="a-损失函数"><a href="#a-损失函数" class="headerlink" title="a)损失函数"></a>a)损失函数</h3><p>$$-\frac{1}{|w|} \sum_{x_i \in M} y_i (w \cdot x_i + b)$$</p><p>是所有误分类点到超平面$S$的总距离，其中</p><p>$$-\frac{1}{|w|}$$</p><p>为$w$的$L_2$范数，若不考虑，则损失函数的定义为：</p><p>$$L(w, b) &#x3D; -\sum_{x_i \in M} y_i (w \cdot x_i + b)$$</p><p>这是关于$w$,$b$的连续可导函数。</p><h3 id="b-学习算法"><a href="#b-学习算法" class="headerlink" title="b)学习算法"></a>b)学习算法</h3><p>获得感知机模型的过程本质上就是极小化损失函数的过程：</p><p>$$\min_{w, b} L(w, b) &#x3D; - \sum_{x_i \in M} y_i (w \cdot x_i + b)$$</p><p>假设误分类点集合是固定的，那么损失函数$L(w,b)$的梯度由：<br>$$\nabla_w L(w, b) &#x3D; - \sum_{x_i \in M} y_i x_i$$</p><p>$$\nabla_b L(w, b) &#x3D; - \sum_{x_i \in M} y_i$$<br>给出。</p><p>随机选取一个误分类点$(x_i,y_i)$，对$w$,$b$进行更新：</p><p>$$w \leftarrow w + \eta y_i x_i$$</p><p>$$b \leftarrow b + \eta y_i$$<br>其中$\eta$是学习率。</p><h1 id="3-实现代码"><a href="#3-实现代码" class="headerlink" title="3. 实现代码"></a>3. 实现代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-comment"># -*- encoding: utf-8 -*-</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">@File    :   感知机.ipynb</span><br><span class="hljs-string">@Time    :   2025/03/24 21:29:44</span><br><span class="hljs-string">@Author  :   Neutrin </span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># here put the import lib</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>np.random.seed(<span class="hljs-number">42</span>)  <br><br><span class="hljs-comment"># 生成样本点 100个负样本 100个正样本</span><br>n_samples_neg = <span class="hljs-number">100</span><br>X_neg = np.random.normal(loc=[-<span class="hljs-number">2</span>, -<span class="hljs-number">2</span>], scale=<span class="hljs-number">1.0</span>, size=(n_samples_neg, <span class="hljs-number">2</span>))<br>y_neg = np.zeros(n_samples_neg)<br><br>n_samples_pos = <span class="hljs-number">100</span><br>X_pos = np.random.normal(loc=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], scale=<span class="hljs-number">1.0</span>, size=(n_samples_pos, <span class="hljs-number">2</span>))<br>y_pos = np.ones(n_samples_pos)<br><br><span class="hljs-comment"># 合并样本点</span><br>X = np.vstack((X_neg, X_pos))<br>y = np.hstack((y_neg, y_pos))<br><br><span class="hljs-comment"># 可视化</span><br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>))<br>plt.scatter(X_neg[:, <span class="hljs-number">0</span>], X_neg[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;blue&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">&#x27;Class 0&#x27;</span>)<br>plt.scatter(X_pos[:, <span class="hljs-number">0</span>], X_pos[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;red&#x27;</span>, marker=<span class="hljs-string">&#x27;x&#x27;</span>, label=<span class="hljs-string">&#x27;Class 1&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Perceptron Training Data&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Feature 1&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Feature 2&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Dataset shape: X: <span class="hljs-subst">&#123;X.shape&#125;</span>, y: <span class="hljs-subst">&#123;y.shape&#125;</span>&quot;</span>)<br><span class="hljs-comment"># 感知机用于二分类问题，输出为+1或-1</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">out</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-keyword">return</span> np.sign(np.dot(x, w) + b)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Perceptron</span>: <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, learning_rate=<span class="hljs-number">0.01</span>, max_iterations=<span class="hljs-number">1000</span></span>):  <span class="hljs-comment"># 学习率和最大迭代次数</span><br>        <span class="hljs-variable language_">self</span>.learning_rate = learning_rate<br>        <span class="hljs-variable language_">self</span>.max_iterations = max_iterations<br>        <span class="hljs-variable language_">self</span>.w = <span class="hljs-literal">None</span><br>        <span class="hljs-variable language_">self</span>.b = <span class="hljs-literal">None</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fit</span>(<span class="hljs-params">self, X, y</span>):<br>        <span class="hljs-comment"># 初始化权重和偏置</span><br>        n_samples, n_features = X.shape  <span class="hljs-comment"># 样本数和特征数</span><br>        <span class="hljs-variable language_">self</span>.w = np.zeros(n_features)<br>        <span class="hljs-variable language_">self</span>.b = <span class="hljs-number">0</span><br>        <br>        <span class="hljs-comment"># 将标签0转为-1 (感知机标签通常为-1和+1)</span><br>        y_transformed = np.where(y == <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># where函数，满足条件输出1，不满足输出0</span><br>        <br>        <span class="hljs-comment"># 训练模型</span><br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.max_iterations):<br>            misclassified = <span class="hljs-number">0</span>               <span class="hljs-comment"># 错误分类的样本数</span><br>            <span class="hljs-keyword">for</span> idx, x_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(X):   <br>                y_i = y_transformed[idx]<br>                <span class="hljs-comment"># 如果预测错误，更新权重</span><br>                <span class="hljs-keyword">if</span> y_i * (np.dot(x_i, <span class="hljs-variable language_">self</span>.w) + <span class="hljs-variable language_">self</span>.b) &lt;= <span class="hljs-number">0</span>:<br>                    <span class="hljs-variable language_">self</span>.w += <span class="hljs-variable language_">self</span>.learning_rate * y_i * x_i<br>                    <span class="hljs-variable language_">self</span>.b += <span class="hljs-variable language_">self</span>.learning_rate * y_i<br>                    misclassified += <span class="hljs-number">1</span><br>            <span class="hljs-comment"># 如果没有错误分类，提前结束训练</span><br>            <span class="hljs-keyword">if</span> misclassified == <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">break</span><br>                <br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> np.where(np.sign(np.dot(X, <span class="hljs-variable language_">self</span>.w) + <span class="hljs-variable language_">self</span>.b) == -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">score</span>(<span class="hljs-params">self, X, y</span>):<br>        y_pred = <span class="hljs-variable language_">self</span>.predict(X)<br>        <span class="hljs-keyword">return</span> np.mean(y_pred == y)<br>    <br>    <span class="hljs-comment"># 获取用于绘制决策边界的参数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decision_boundary</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.w, <span class="hljs-variable language_">self</span>.b<br><br><span class="hljs-comment"># 创建并训练感知机模型</span><br>perceptron = Perceptron(learning_rate=<span class="hljs-number">0.01</span>, max_iterations=<span class="hljs-number">1000</span>)<br>perceptron.fit(X, y)<br><br><span class="hljs-comment"># 评估模型</span><br>accuracy = perceptron.score(X, y)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Accuracy: <span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 绘制决策边界</span><br>w, b = perceptron.decision_boundary()<br>x_min, x_max = X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br>y_min, y_max = X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br>xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="hljs-number">0.1</span>),<br>                     np.arange(y_min, y_max, <span class="hljs-number">0.1</span>))<br>Z = np.sign(np.dot(np.c_[xx.ravel(), yy.ravel()], w) + b).reshape(xx.shape)<br><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br>plt.contourf(xx, yy, Z, alpha=<span class="hljs-number">0.3</span>, cmap=plt.cm.coolwarm)<br>plt.scatter(X_neg[:, <span class="hljs-number">0</span>], X_neg[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;blue&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">&#x27;Class 0&#x27;</span>)<br>plt.scatter(X_pos[:, <span class="hljs-number">0</span>], X_pos[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;red&#x27;</span>, marker=<span class="hljs-string">&#x27;x&#x27;</span>, label=<span class="hljs-string">&#x27;Class 1&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Perceptron Decision Boundary&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Feature 1&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Feature 2&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
